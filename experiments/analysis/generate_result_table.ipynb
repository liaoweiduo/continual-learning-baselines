{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Generate results table"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from plotly.subplots import make_subplots\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objs as go\n",
    "\n",
    "def get_accs(exp_name, mode='sys', project_name='CGQA', name='Top1_Acc_Stream/eval_phase/test_stream', index='', test_n_way=None, model=None, frz_prefix='naive'):\n",
    "\n",
    "    if mode in ['continual', 'forgetting']:\n",
    "        if not os.path.exists(os.path.join('../../../avalanche-experiments', project_name, exp_name, f'results-{exp_name}{index}.npy')):\n",
    "            return np.array([0])\n",
    "        results = np.load(os.path.join('../../../avalanche-experiments', project_name, exp_name, f'results-{exp_name}{index}.npy'), allow_pickle=True)\n",
    "\n",
    "    else:\n",
    "        if test_n_way is not None and model is not None:\n",
    "            file_name = f'results-{exp_name}-{mode}-{frz_prefix}-frz-test_n_way{test_n_way}-model{model}.npy'\n",
    "        elif test_n_way:\n",
    "            file_name = f'results-{exp_name}-{mode}-{frz_prefix}-frz-test_n_way{test_n_way}.npy'\n",
    "        else:\n",
    "            file_name = f'results-{exp_name}-{mode}-{frz_prefix}-frz.npy'\n",
    "        print(file_name)\n",
    "        if not os.path.exists(os.path.join('../../../avalanche-experiments', project_name, exp_name, file_name)):\n",
    "            return np.array([0])\n",
    "\n",
    "        results = np.load(os.path.join('../../../avalanche-experiments', project_name, exp_name, file_name), allow_pickle=True)\n",
    "\n",
    "    accs = []\n",
    "\n",
    "    for k, v in results[-1].items():\n",
    "        if k.startswith(name):\n",
    "            accs.append(v)\n",
    "\n",
    "    return np.array(accs)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Load one exp"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test acc: [0.8967]\n",
      "['Top1_Acc_Exp/eval_phase/test_stream/Task000/Exp000', 'Top1_Acc_Exp/eval_phase/test_stream/Task001/Exp001', 'Top1_Acc_Exp/eval_phase/test_stream/Task002/Exp002', 'Top1_Acc_Exp/eval_phase/test_stream/Task003/Exp003', 'Top1_Acc_Exp/eval_phase/test_stream/Task004/Exp004', 'Top1_Acc_Exp/eval_phase/test_stream/Task005/Exp005', 'Top1_Acc_Exp/eval_phase/test_stream/Task006/Exp006', 'Top1_Acc_Exp/eval_phase/test_stream/Task007/Exp007', 'Top1_Acc_Exp/eval_phase/test_stream/Task008/Exp008', 'Top1_Acc_Exp/eval_phase/test_stream/Task009/Exp009']\n",
      "Test forgetting: [0.]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# return_task_id = True\n",
    "exp_name = 'MT-naive-tsk_True-lr0_0001'\n",
    "postfix = ''        # '' for MT, '-2' or '-9' for others\n",
    "# exps = ['naive-cls-lr0_003', 'er-cls-lr0_003', 'gem-cls-lr0_01-p32-m0_3', 'lwf-cls-lr0_005-a1-t1', 'ewc-cls-lr0_005-lambda0_1']\n",
    "# exps = ['naive-tsk-lr0_008', 'er-tsk-lr0_0008', 'gem-tsk-lr0_001-p32-m0_3', 'lwf-tsk-lr0_01-a1-t1', 'ewc-tsk-lr0_005-lambda2']\n",
    "# Multi-task: MT-naive-tsk_False-lr0_005; MT-naive-tsk_True-lr0_001\n",
    "# ViT baselines:\n",
    "#     ht-vit-naive-cls-lr0_0001; ht-vit-naive-tsk-lr0_0001;\n",
    "#     ht-vit-er-cls-lr0_0001; ht-vit-er-tsk-lr0_0001;\n",
    "#     ht-vit-gem-cls-lr5e-05; ht-vit-gem-tsk-lr1e-05;\n",
    "#     ht-vit-lwf-cls-lr0_0001; ht-vit-lwf-tsk-lr0_0001\n",
    "#     ht-vit-ewc-cls-lr0_0001; ht-vit-ewc-tsk-lr0_0001\n",
    "\n",
    "# cobj\n",
    "# 'HT-MT-3tasks-naive-tsk_True-lr0_00231', 'HT-MT-3tasks-naive-tsk_False-lr0_00123',\n",
    "# 'HT-naive-tsk_True-lr0_001', 'HT-naive-tsk_False-lr0_001',\n",
    "# 'HT-er-tsk_True-lr0_01', 'HT-er-tsk_False-lr0_01',\n",
    "# 'HT-gem-tsk_True-lr0_001-p16-m0_3', 'HT-gem-tsk_False-lr0_001-p256-m0_00139',\n",
    "# 'HT-lwf-tsk_True-lr0_001-a1-t2', 'HT-lwf-tsk_False-lr0_001-a1-t1_52',\n",
    "# 'HT-ewc-tsk_True-lr0_01-lambda100', 'HT-ewc-tsk_False-lr0_00053-lambda10',\n",
    "\n",
    "results = np.load(os.path.join('../../../avalanche-experiments', 'CGQA', exp_name, f'results-{exp_name}{postfix}.npy'), allow_pickle=True)\n",
    "first_result = results[0]\n",
    "last_result = results[-1]\n",
    "print('Test acc:', get_accs(exp_name, mode='continual', project_name='CGQA', name='Top1_Acc_Stream/eval_phase/test_stream', index=postfix))\n",
    "# 只有最后一个task的test acc，奇怪\n",
    "print([k for k in last_result.keys() if k.startswith('Top1_Acc_Exp/eval_phase/test_stream')])\n",
    "\n",
    "# forget\n",
    "print('Test forgetting:', get_accs(exp_name, mode='continual', project_name='CGQA', name='StreamForgetting/eval_phase/test_stream', index=postfix))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{lrrr}\n",
      "{Task} & {0} & {1} & {2} \\\\\n",
      "{Exp} & {} & {} & {} \\\\\n",
      "0 & 50.200000 & 0.000000 & 0.000000 \\\\\n",
      "1 & 0.000000 & 56.100000 & 0.000000 \\\\\n",
      "2 & 0.000000 & 0.000000 & 55.100000 \\\\\n",
      "\\end{tabular}\n",
      "\n",
      "Task     0     1     2\n",
      "Exp                   \n",
      "0     50.2   0.0   0.0\n",
      "1      0.0  56.1   0.0\n",
      "2      0.0   0.0  55.1\n"
     ]
    },
    {
     "data": {
      "text/plain": "<Figure size 1500x300 with 2 Axes>",
      "image/png": "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\n"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "return_task_id = False\n",
    "num_experiments = 3      # 1 for MT, 3 for others\n",
    "num_tasks = 3            # all for 3\n",
    "exp_name = 'HT-ewc-tsk_False-lr0_00053-lambda10'\n",
    "\n",
    "# cobj\n",
    "# 'HT-MT-3tasks-naive-tsk_True-lr0_001', 'HT-MT-3tasks-naive-tsk_False-lr0_001',\n",
    "# # 'HT-MT-naive-tsk_True-lr0_00231', 'HT-MT-naive-tsk_False-lr0_00123',    # 10tasks\n",
    "# 'HT-naive-tsk_True-lr0_001', 'HT-naive-tsk_False-lr0_001',\n",
    "# 'HT-er-tsk_True-lr0_01', 'HT-er-tsk_False-lr0_01',\n",
    "# 'HT-gem-tsk_True-lr0_001-p16-m0_3', 'HT-gem-tsk_False-lr0_001-p256-m0_00139',\n",
    "# 'HT-lwf-tsk_True-lr0_001-a1-t2', 'HT-lwf-tsk_False-lr0_001-a1-t1_52',\n",
    "# 'HT-ewc-tsk_True-lr0_01-lambda100', 'HT-ewc-tsk_False-lr0_00053-lambda10',\n",
    "\n",
    "results = []\n",
    "for task_id in range(num_experiments):\n",
    "    postfix = f'-{task_id}' if 'MT' not in exp_name else ''\n",
    "    result = np.load(os.path.join('../../../avalanche-experiments', 'COBJ', exp_name, f'results-{exp_name}{postfix}.npy'), allow_pickle=True)\n",
    "    results.append(result[-1])\n",
    "\n",
    "'''collect exps'''\n",
    "collected_data = []\n",
    "\n",
    "for exp_id, exp_results in enumerate(results):     # all ten results\n",
    "    for task_id in range(num_tasks):\n",
    "        if return_task_id:\n",
    "            task_id_str = '%03d' % task_id    # task-il: 000 -> 009\n",
    "        else:\n",
    "            task_id_str = '%03d' % 0    # class-il: 000\n",
    "        exp_id_str = '%03d' % task_id   # 000 -> 009\n",
    "        result = exp_results[f'Top1_Acc_Exp/eval_phase/test_stream/Task{task_id_str}/Exp{exp_id_str}']\n",
    "        collected_data.append(pd.DataFrame({'Method': exp_name, 'Exp': exp_id, 'Task': task_id, 'Accuracy': result*100}, index=[0]))\n",
    "\n",
    "collected_data = pd.concat(collected_data, ignore_index=True)\n",
    "\n",
    "# to latex\n",
    "\n",
    "data = collected_data.pivot(index='Exp', columns='Task', values='Accuracy')\n",
    "# data = data[modes]\n",
    "# data = data.reindex(exps_dis)\n",
    "print(data.style.to_latex())\n",
    "print(data)\n",
    "\n",
    "plt.figure(figsize=(15, 3))\n",
    "# sns.set_palette('Paired')\n",
    "matplotlib.rcParams.update({'font.size': 18})\n",
    "\n",
    "# ax = sns.scatterplot(x=\"Weight\", y=\"learning rate\", data=collected_data, hue='Method')\n",
    "# plt.legend(loc = 'upper left', bbox_to_anchor = (1, 1))\n",
    "# ax = sns.heatmap(collected_data.pivot(\"Method\", \"learning rate\", \"Accuracy\"), annot=True, fmt=\".2f\", linewidth=.5)\n",
    "ax = sns.heatmap(data, annot=True, fmt=\".2f\", linewidth=.5)\n",
    "# ax = px.line(collected_data, x=\"Train size\", y=\"Accuracy\", hover_data=['Accuracy', 'Method'], color='Method')\n",
    "# ax.show()\n",
    "\n",
    "# plt.ylim([5, 6.5])\n",
    "\n",
    "fig = ax.get_figure()\n",
    "fig.savefig(os.path.join('D:', 'Downloads', 'fig.png'), dpi = 400, bbox_inches='tight')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Generate average test table"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TaskI_codaPrompt_FC-10tasks-tsk_True-lr0_001-p16-m0_3-seed1\n",
      "continual\n",
      "[0.892]\n",
      "sys\n",
      "results-TaskI_codaPrompt_FC-10tasks-tsk_True-lr0_001-p16-m0_3-seed1-sys-codaPrompt-frz-test_n_way10.npy\n",
      "[0.38 0.3  0.32 0.29 0.33 0.27 0.31 0.36 0.28 0.27 0.37 0.45 0.37 0.29\n",
      " 0.17 0.25 0.3  0.29 0.2  0.34 0.31 0.29 0.27 0.31 0.31 0.25 0.36 0.36\n",
      " 0.27 0.21 0.26 0.35 0.34 0.4  0.21 0.37 0.36 0.4  0.31 0.17 0.41 0.24\n",
      " 0.22 0.31 0.41 0.3  0.39 0.38 0.43 0.34]\n",
      "pro\n",
      "results-TaskI_codaPrompt_FC-10tasks-tsk_True-lr0_001-p16-m0_3-seed1-pro-codaPrompt-frz-test_n_way10.npy\n",
      "[0.24 0.19 0.26 0.21 0.21 0.32 0.12 0.31 0.33 0.27 0.3  0.39 0.24 0.3\n",
      " 0.33 0.23 0.33 0.18 0.24 0.32 0.25 0.37 0.19 0.19 0.23 0.24 0.28 0.26\n",
      " 0.29 0.23 0.22 0.17 0.22 0.15 0.19 0.2  0.23 0.21 0.21 0.18 0.19 0.29\n",
      " 0.27 0.23 0.31 0.21 0.36 0.17 0.39 0.3 ]\n",
      "non\n",
      "results-TaskI_codaPrompt_FC-10tasks-tsk_True-lr0_001-p16-m0_3-seed1-non-codaPrompt-frz-test_n_way10.npy\n",
      "[0.22 0.28 0.25 0.32 0.14 0.31 0.3  0.3  0.27 0.24 0.29 0.4  0.32 0.23\n",
      " 0.14 0.26 0.32 0.28 0.26 0.36 0.29 0.31 0.35 0.2  0.29 0.38 0.33 0.26\n",
      " 0.29 0.1  0.33 0.25 0.41 0.33 0.29 0.26 0.16 0.23 0.25 0.27 0.29 0.24\n",
      " 0.2  0.2  0.16 0.25 0.24 0.32 0.24 0.29]\n",
      "noc\n",
      "results-TaskI_codaPrompt_FC-10tasks-tsk_True-lr0_001-p16-m0_3-seed1-noc-codaPrompt-frz-test_n_way10.npy\n",
      "[0.53 0.56 0.38 0.59 0.58 0.57 0.35 0.53 0.53 0.54 0.51 0.51 0.59 0.53\n",
      " 0.42 0.55 0.41 0.53 0.4  0.48 0.54 0.5  0.53 0.61 0.39 0.53 0.53 0.59\n",
      " 0.45 0.44 0.65 0.52 0.57 0.54 0.52 0.52 0.36 0.42 0.41 0.64 0.46 0.54\n",
      " 0.43 0.57 0.38 0.57 0.58 0.56 0.67 0.52]\n",
      "TaskI_dualPrompt_FC-10tasks-tsk_True-lr0_001-p16-m0_3-seed1\n",
      "continual\n",
      "[0.88933333]\n",
      "sys\n",
      "results-TaskI_dualPrompt_FC-10tasks-tsk_True-lr0_001-p16-m0_3-seed1-sys-dualPrompt-frz-test_n_way10.npy\n",
      "[0.33 0.25 0.25 0.32 0.29 0.35 0.17 0.43 0.32 0.23 0.32 0.5  0.27 0.37\n",
      " 0.26 0.17 0.19 0.34 0.26 0.31 0.21 0.35 0.24 0.25 0.33 0.38 0.31 0.37\n",
      " 0.28 0.26 0.28 0.42 0.32 0.39 0.43 0.31 0.34 0.38 0.48 0.37 0.37 0.28\n",
      " 0.15 0.22 0.38 0.43 0.33 0.31 0.41 0.21]\n",
      "pro\n",
      "results-TaskI_dualPrompt_FC-10tasks-tsk_True-lr0_001-p16-m0_3-seed1-pro-dualPrompt-frz-test_n_way10.npy\n",
      "[0.26 0.29 0.22 0.25 0.26 0.17 0.41 0.21 0.29 0.28 0.19 0.36 0.28 0.3\n",
      " 0.31 0.36 0.27 0.25 0.17 0.27 0.17 0.38 0.23 0.22 0.19 0.2  0.24 0.37\n",
      " 0.17 0.31 0.2  0.13 0.22 0.2  0.18 0.23 0.28 0.2  0.26 0.18 0.28 0.25\n",
      " 0.28 0.31 0.33 0.17 0.33 0.17 0.4  0.21]\n",
      "non\n",
      "results-TaskI_dualPrompt_FC-10tasks-tsk_True-lr0_001-p16-m0_3-seed1-non-dualPrompt-frz-test_n_way10.npy\n",
      "[0.34 0.26 0.23 0.31 0.35 0.25 0.21 0.29 0.18 0.21 0.32 0.24 0.27 0.28\n",
      " 0.21 0.1  0.35 0.31 0.3  0.2  0.31 0.33 0.36 0.17 0.49 0.36 0.19 0.34\n",
      " 0.3  0.33 0.19 0.21 0.35 0.31 0.19 0.32 0.27 0.15 0.2  0.28 0.3  0.26\n",
      " 0.23 0.32 0.29 0.21 0.25 0.39 0.14 0.41]\n",
      "noc\n",
      "results-TaskI_dualPrompt_FC-10tasks-tsk_True-lr0_001-p16-m0_3-seed1-noc-dualPrompt-frz-test_n_way10.npy\n",
      "[0.58 0.57 0.45 0.67 0.57 0.54 0.46 0.5  0.48 0.53 0.52 0.5  0.57 0.6\n",
      " 0.42 0.61 0.53 0.48 0.52 0.46 0.54 0.58 0.46 0.55 0.44 0.47 0.45 0.59\n",
      " 0.47 0.43 0.6  0.44 0.53 0.51 0.45 0.65 0.45 0.45 0.42 0.56 0.42 0.47\n",
      " 0.46 0.59 0.34 0.61 0.51 0.61 0.65 0.53]\n",
      "TaskI_fine-tune_FC-10tasks-tsk_True-lr0_001-p16-m0_3-seed1\n",
      "continual\n",
      "[0.89066667]\n",
      "sys\n",
      "results-TaskI_fine-tune_FC-10tasks-tsk_True-lr0_001-p16-m0_3-seed1-sys-fine-tune-frz-test_n_way10.npy\n",
      "[0.25 0.26 0.27 0.26 0.21 0.35 0.3  0.5  0.23 0.29 0.25 0.53 0.22 0.34\n",
      " 0.13 0.26 0.3  0.31 0.27 0.32 0.2  0.33 0.34 0.23 0.37 0.29 0.23 0.34\n",
      " 0.14 0.27 0.35 0.33 0.47 0.38 0.35 0.4  0.27 0.42 0.42 0.31 0.24 0.15\n",
      " 0.2  0.23 0.36 0.39 0.39 0.35 0.32 0.33]\n",
      "pro\n",
      "results-TaskI_fine-tune_FC-10tasks-tsk_True-lr0_001-p16-m0_3-seed1-pro-fine-tune-frz-test_n_way10.npy\n",
      "[0.25 0.32 0.23 0.24 0.29 0.19 0.3  0.37 0.3  0.23 0.21 0.26 0.25 0.31\n",
      " 0.33 0.17 0.32 0.2  0.21 0.32 0.25 0.28 0.16 0.22 0.2  0.19 0.16 0.23\n",
      " 0.2  0.32 0.24 0.13 0.18 0.21 0.14 0.24 0.3  0.19 0.22 0.18 0.22 0.27\n",
      " 0.31 0.21 0.41 0.23 0.31 0.09 0.44 0.12]\n",
      "non\n",
      "results-TaskI_fine-tune_FC-10tasks-tsk_True-lr0_001-p16-m0_3-seed1-non-fine-tune-frz-test_n_way10.npy\n",
      "[0.26 0.3  0.25 0.25 0.3  0.33 0.29 0.31 0.19 0.18 0.27 0.22 0.26 0.25\n",
      " 0.22 0.31 0.27 0.18 0.28 0.27 0.37 0.24 0.35 0.16 0.34 0.31 0.31 0.16\n",
      " 0.34 0.25 0.27 0.25 0.42 0.27 0.26 0.24 0.24 0.26 0.21 0.28 0.32 0.31\n",
      " 0.27 0.29 0.36 0.29 0.39 0.37 0.21 0.38]\n",
      "noc\n",
      "results-TaskI_fine-tune_FC-10tasks-tsk_True-lr0_001-p16-m0_3-seed1-noc-fine-tune-frz-test_n_way10.npy\n",
      "[0.5  0.46 0.53 0.65 0.61 0.59 0.45 0.5  0.49 0.59 0.58 0.46 0.61 0.61\n",
      " 0.51 0.59 0.51 0.47 0.45 0.58 0.58 0.54 0.43 0.6  0.56 0.57 0.55 0.56\n",
      " 0.47 0.48 0.6  0.56 0.49 0.59 0.53 0.52 0.44 0.32 0.49 0.55 0.49 0.54\n",
      " 0.47 0.54 0.46 0.62 0.46 0.6  0.52 0.51]\n",
      "TaskI_l2p_TaskI_l2p_plus_FC-10tasks-tsk_True-lr0_001-p16-m0_3-seed1\n",
      "continual\n",
      "[0.89466667]\n",
      "sys\n",
      "results-TaskI_l2p_TaskI_l2p_plus_FC-10tasks-tsk_True-lr0_001-p16-m0_3-seed1-sys-l2p-frz-test_n_way10.npy\n",
      "[0.22 0.24 0.22 0.26 0.29 0.3  0.3  0.47 0.31 0.15 0.32 0.59 0.34 0.38\n",
      " 0.18 0.3  0.26 0.33 0.16 0.21 0.3  0.44 0.11 0.19 0.4  0.27 0.27 0.37\n",
      " 0.31 0.33 0.23 0.28 0.16 0.32 0.36 0.45 0.38 0.45 0.44 0.26 0.29 0.3\n",
      " 0.27 0.36 0.27 0.33 0.35 0.22 0.41 0.29]\n",
      "pro\n",
      "results-TaskI_l2p_TaskI_l2p_plus_FC-10tasks-tsk_True-lr0_001-p16-m0_3-seed1-pro-l2p-frz-test_n_way10.npy\n",
      "[0.18 0.33 0.39 0.24 0.25 0.22 0.36 0.32 0.43 0.25 0.25 0.4  0.24 0.21\n",
      " 0.22 0.22 0.33 0.22 0.26 0.19 0.29 0.24 0.12 0.24 0.33 0.27 0.27 0.27\n",
      " 0.18 0.22 0.19 0.14 0.29 0.18 0.22 0.19 0.24 0.24 0.2  0.12 0.21 0.26\n",
      " 0.26 0.39 0.23 0.18 0.38 0.19 0.39 0.19]\n",
      "non\n",
      "results-TaskI_l2p_TaskI_l2p_plus_FC-10tasks-tsk_True-lr0_001-p16-m0_3-seed1-non-l2p-frz-test_n_way10.npy\n",
      "[0.24 0.29 0.38 0.33 0.27 0.33 0.21 0.25 0.17 0.18 0.22 0.32 0.33 0.22\n",
      " 0.19 0.32 0.27 0.22 0.32 0.26 0.25 0.33 0.17 0.24 0.46 0.36 0.28 0.29\n",
      " 0.35 0.39 0.29 0.32 0.28 0.37 0.23 0.26 0.1  0.15 0.22 0.29 0.28 0.18\n",
      " 0.27 0.29 0.34 0.16 0.36 0.35 0.14 0.34]\n",
      "noc\n",
      "results-TaskI_l2p_TaskI_l2p_plus_FC-10tasks-tsk_True-lr0_001-p16-m0_3-seed1-noc-l2p-frz-test_n_way10.npy\n",
      "[0.55 0.58 0.44 0.63 0.51 0.61 0.46 0.54 0.52 0.5  0.34 0.48 0.53 0.61\n",
      " 0.4  0.44 0.54 0.56 0.48 0.57 0.53 0.56 0.52 0.51 0.51 0.54 0.44 0.48\n",
      " 0.44 0.41 0.55 0.52 0.54 0.56 0.54 0.56 0.4  0.44 0.44 0.55 0.41 0.5\n",
      " 0.55 0.6  0.49 0.45 0.52 0.59 0.68 0.48]\n",
      "TaskI_naive_FC-10tasks-tsk_True-lr0_001-p16-m0_3-seed1\n",
      "continual\n",
      "[0.565]\n",
      "sys\n",
      "results-TaskI_naive_FC-10tasks-tsk_True-lr0_001-p16-m0_3-seed1-sys-naive-frz-test_n_way10.npy\n",
      "[0.21 0.22 0.25 0.18 0.26 0.25 0.27 0.24 0.19 0.26 0.2  0.25 0.26 0.3\n",
      " 0.23 0.18 0.22 0.19 0.2  0.22 0.23 0.27 0.23 0.18 0.18 0.28 0.19 0.23\n",
      " 0.2  0.27 0.21 0.18 0.28 0.23 0.26 0.16 0.27 0.18 0.2  0.27 0.26 0.17\n",
      " 0.13 0.15 0.18 0.26 0.23 0.17 0.3  0.18]\n",
      "pro\n",
      "results-TaskI_naive_FC-10tasks-tsk_True-lr0_001-p16-m0_3-seed1-pro-naive-frz-test_n_way10.npy\n",
      "[0.23 0.29 0.19 0.2  0.19 0.16 0.18 0.21 0.24 0.24 0.2  0.24 0.18 0.17\n",
      " 0.2  0.21 0.26 0.2  0.21 0.25 0.15 0.22 0.21 0.2  0.12 0.2  0.27 0.23\n",
      " 0.22 0.17 0.23 0.23 0.15 0.19 0.15 0.21 0.22 0.22 0.22 0.12 0.18 0.28\n",
      " 0.16 0.27 0.25 0.19 0.22 0.13 0.25 0.2 ]\n",
      "non\n",
      "results-TaskI_naive_FC-10tasks-tsk_True-lr0_001-p16-m0_3-seed1-non-naive-frz-test_n_way10.npy\n",
      "[0.24 0.19 0.22 0.22 0.2  0.28 0.23 0.28 0.25 0.18 0.2  0.19 0.23 0.14\n",
      " 0.2  0.3  0.22 0.16 0.19 0.22 0.31 0.26 0.28 0.19 0.25 0.25 0.26 0.17\n",
      " 0.24 0.24 0.16 0.2  0.27 0.23 0.26 0.23 0.1  0.31 0.23 0.19 0.21 0.23\n",
      " 0.15 0.25 0.22 0.15 0.29 0.27 0.17 0.25]\n",
      "noc\n",
      "results-TaskI_naive_FC-10tasks-tsk_True-lr0_001-p16-m0_3-seed1-noc-naive-frz-test_n_way10.npy\n",
      "[0.44 0.33 0.34 0.4  0.34 0.3  0.28 0.34 0.28 0.4  0.33 0.32 0.3  0.36\n",
      " 0.22 0.31 0.27 0.22 0.29 0.24 0.28 0.3  0.32 0.35 0.23 0.28 0.2  0.28\n",
      " 0.25 0.29 0.37 0.32 0.29 0.29 0.22 0.33 0.33 0.22 0.23 0.38 0.28 0.34\n",
      " 0.3  0.26 0.3  0.29 0.2  0.33 0.31 0.35]\n"
     ]
    }
   ],
   "source": [
    "'''collect exps rebuttal l2p dualprompt'''\n",
    "'''collect exps'''\n",
    "project_name = 'COBJ'   # 'CGQA'\n",
    "# modes = ['continual', 'sys', 'pro', 'sub', 'non', 'noc']    # , 'forgetting'\n",
    "modes = ['continual', 'sys', 'pro', 'non', 'noc']    # , 'forgetting'\n",
    "\n",
    "exps = ['TaskI_codaPrompt_FC-10tasks-tsk_True-lr0_001-p16-m0_3-seed1', 'TaskI_dualPrompt_FC-10tasks-tsk_True-lr0_001-p16-m0_3-seed1', 'TaskI_fine-tune_FC-10tasks-tsk_True-lr0_001-p16-m0_3-seed1', 'TaskI_l2p_TaskI_l2p_plus_FC-10tasks-tsk_True-lr0_001-p16-m0_3-seed1', 'TaskI_naive_FC-10tasks-tsk_True-lr0_001-p16-m0_3-seed1']\n",
    "exps_dis = ['codaPrompt', 'dualPrompt', 'FT_Classifier', 'l2p', 'FT_backbone']\n",
    "\n",
    "collected_data = []\n",
    "for exp_idx, exp_name in enumerate(exps):\n",
    "    print(exp_name)\n",
    "    for mode in modes:\n",
    "        print(mode)\n",
    "        if mode == 'forgetting':\n",
    "            accs = get_accs(exp_name, mode, project_name=project_name, index='' if 'MT' not in exp_name else '', name='StreamForgetting/eval_phase/test_stream')\n",
    "            print(accs)\n",
    "        else:\n",
    "            accs = get_accs(exp_name, mode, project_name=project_name, index='-last', test_n_way=10, frz_prefix=exp_name.split('_')[1])\n",
    "        print(accs)\n",
    "        collected_data.append(pd.DataFrame({'Method': exps_dis[exp_idx], 'Phase': mode, 'Accuracy': accs}))\n",
    "\n",
    "collected_data = pd.concat(collected_data, ignore_index=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{lllllllll}\n",
      "{Phase} & {continual} & {sys} & {pro} & {Hn} & {non} & {noc} & {Hr} & {Ha} \\\\\n",
      "{Method} & {} & {} & {} & {} & {} & {} & {} & {} \\\\\n",
      "codaPrompt & 89.20 & 31.36 $\\pm$ 1.83 & 25.10 $\\pm$ 1.76 & 27.88 & 27.00 $\\pm$ 1.79 & 51.26 $\\pm$ 2.14 & 35.37 & 31.18 \\\\\n",
      "dualPrompt & 88.93 & 31.44 $\\pm$ 2.22 & 25.38 $\\pm$ 1.88 & 28.09 & 27.32 $\\pm$ 2.10 & 51.58 $\\pm$ 2.00 & 35.72 & 31.45 \\\\\n",
      "FT_Classifier & 89.07 & 30.50 $\\pm$ 2.37 & 24.30 $\\pm$ 1.99 & 27.05 & 27.82 $\\pm$ 1.64 & 52.76 $\\pm$ 1.77 & 36.43 & 31.05 \\\\\n",
      "l2p & 89.47 & 30.48 $\\pm$ 2.57 & 25.26 $\\pm$ 2.05 & 27.63 & 27.32 $\\pm$ 2.06 & 51.20 $\\pm$ 1.86 & 35.63 & 31.12 \\\\\n",
      "FT_backbone & 56.50 & 22.22 $\\pm$ 1.15 & 20.62 $\\pm$ 1.10 & 21.39 & 22.32 $\\pm$ 1.28 & 30.06 $\\pm$ 1.47 & 25.62 & 23.31 \\\\\n",
      "\\end{tabular}\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": "Phase         continual            sys            pro     Hn            non  \\\nMethod                                                                        \ncodaPrompt        89.20  31.36 +- 1.83  25.10 +- 1.76  27.88  27.00 +- 1.79   \ndualPrompt        88.93  31.44 +- 2.22  25.38 +- 1.88  28.09  27.32 +- 2.10   \nFT_Classifier     89.07  30.50 +- 2.37  24.30 +- 1.99  27.05  27.82 +- 1.64   \nl2p               89.47  30.48 +- 2.57  25.26 +- 2.05  27.63  27.32 +- 2.06   \nFT_backbone       56.50  22.22 +- 1.15  20.62 +- 1.10  21.39  22.32 +- 1.28   \n\nPhase                    noc     Hr     Ha  \nMethod                                      \ncodaPrompt     51.26 +- 2.14  35.37  31.18  \ndualPrompt     51.58 +- 2.00  35.72  31.45  \nFT_Classifier  52.76 +- 1.77  36.43  31.05  \nl2p            51.20 +- 1.86  35.63  31.12  \nFT_backbone    30.06 +- 1.47  25.62  23.31  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th>Phase</th>\n      <th>continual</th>\n      <th>sys</th>\n      <th>pro</th>\n      <th>Hn</th>\n      <th>non</th>\n      <th>noc</th>\n      <th>Hr</th>\n      <th>Ha</th>\n    </tr>\n    <tr>\n      <th>Method</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>codaPrompt</th>\n      <td>89.20</td>\n      <td>31.36 +- 1.83</td>\n      <td>25.10 +- 1.76</td>\n      <td>27.88</td>\n      <td>27.00 +- 1.79</td>\n      <td>51.26 +- 2.14</td>\n      <td>35.37</td>\n      <td>31.18</td>\n    </tr>\n    <tr>\n      <th>dualPrompt</th>\n      <td>88.93</td>\n      <td>31.44 +- 2.22</td>\n      <td>25.38 +- 1.88</td>\n      <td>28.09</td>\n      <td>27.32 +- 2.10</td>\n      <td>51.58 +- 2.00</td>\n      <td>35.72</td>\n      <td>31.45</td>\n    </tr>\n    <tr>\n      <th>FT_Classifier</th>\n      <td>89.07</td>\n      <td>30.50 +- 2.37</td>\n      <td>24.30 +- 1.99</td>\n      <td>27.05</td>\n      <td>27.82 +- 1.64</td>\n      <td>52.76 +- 1.77</td>\n      <td>36.43</td>\n      <td>31.05</td>\n    </tr>\n    <tr>\n      <th>l2p</th>\n      <td>89.47</td>\n      <td>30.48 +- 2.57</td>\n      <td>25.26 +- 2.05</td>\n      <td>27.63</td>\n      <td>27.32 +- 2.06</td>\n      <td>51.20 +- 1.86</td>\n      <td>35.63</td>\n      <td>31.12</td>\n    </tr>\n    <tr>\n      <th>FT_backbone</th>\n      <td>56.50</td>\n      <td>22.22 +- 1.15</td>\n      <td>20.62 +- 1.10</td>\n      <td>21.39</td>\n      <td>22.32 +- 1.28</td>\n      <td>30.06 +- 1.47</td>\n      <td>25.62</td>\n      <td>23.31</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''todo: cal Harmonic mean (among sys, pro, sub) and Harmonic mean (between non, noc)'''\n",
    "data = []\n",
    "for exp in exps_dis:\n",
    "    mean_dict = {}\n",
    "    for mode in modes:\n",
    "        acc_list = collected_data[(collected_data['Method'] == exp) & (collected_data['Phase'] == mode)]['Accuracy']\n",
    "        mean = acc_list.mean()\n",
    "        mean_dict[mode] = mean\n",
    "        ci95 = 1.96 * (acc_list.std()/np.sqrt(len(acc_list)))\n",
    "        acc_str = f'{mean*100:.2f} +- {ci95*100:.2f}' if mode not in ['continual', 'forgetting'] else f'{mean*100:.2f}'\n",
    "        data.append(pd.DataFrame({'Method': exp, 'Phase': mode, 'mean': mean, 'ci95': ci95, 'str': acc_str}, index=[0]))\n",
    "\n",
    "    # 3x1*x2*x3/(x1*x2+x1*x3+x2*x3)\n",
    "    # hm_nov = 3 * mean_dict['sys'] * mean_dict['pro'] * mean_dict['sub'] / (mean_dict['sys'] * mean_dict['pro'] + mean_dict['sys'] * mean_dict['sub'] + mean_dict['pro'] * mean_dict['sub'])\n",
    "    hm_nov = 2 * mean_dict['sys'] * mean_dict['pro'] / (mean_dict['sys'] + mean_dict['pro'])\n",
    "    data.append(pd.DataFrame({'Method': exp, 'Phase': f'Hn', 'str': f'{hm_nov*100:.2f}'}, index=[0]))\n",
    "\n",
    "    # 2x1*x2/(x1+x2)\n",
    "    hm_ref = 2 * mean_dict['non'] * mean_dict['noc'] / (mean_dict['non'] + mean_dict['noc'])\n",
    "    data.append(pd.DataFrame({'Method': exp, 'Phase': f'Hr', 'str': f'{hm_ref*100:.2f}'}, index=[0]))\n",
    "\n",
    "    # hm_all = 5 / (1/mean_dict['sys'] + 1/mean_dict['pro'] + 1/mean_dict['sub'] + 1/mean_dict['non'] + 1/mean_dict['noc'])\n",
    "    hm_all = 4 / (1/mean_dict['sys'] + 1/mean_dict['pro'] + 1/mean_dict['non'] + 1/mean_dict['noc'])\n",
    "    data.append(pd.DataFrame({'Method': exp, 'Phase': f'Ha', 'str': f'{hm_all*100:.2f}'}, index=[0]))\n",
    "\n",
    "data = pd.concat(data, ignore_index=True)\n",
    "# print(data)\n",
    "\n",
    "# to latex\n",
    "\n",
    "data = data.pivot(index='Method', values='str', columns='Phase')\n",
    "# data = data[['continual', 'forgetting', 'sys', 'pro', 'sub', 'Hn', 'non',  'noc', 'Hr', 'Ha']]\n",
    "# data = data[['continual', 'sys', 'pro', 'sub', 'Hn', 'non',  'noc', 'Hr', 'Ha']]\n",
    "data = data[['continual', 'sys', 'pro', 'Hn', 'non',  'noc', 'Hr', 'Ha']]\n",
    "data = data.reindex(exps_dis)\n",
    "print(data.style.to_latex().replace('+-', '$\\\\pm$'))\n",
    "data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HT-naive-tsk_True-lr0_001\n",
      "0\n",
      "[0.39433333]\n",
      "[0.42 0.35 0.48 0.4  0.45 0.47 0.39 0.43 0.35 0.4  0.41 0.34 0.42 0.39\n",
      " 0.39 0.38 0.38 0.36 0.38 0.45 0.39 0.34 0.54 0.38 0.39 0.4  0.47 0.43\n",
      " 0.48 0.41 0.35 0.36 0.49 0.34 0.39 0.38 0.43 0.65 0.31 0.37 0.35 0.33\n",
      " 0.44 0.34 0.45 0.23 0.44 0.5  0.38 0.43]\n",
      "[0.44 0.32 0.34 0.32 0.3  0.37 0.28 0.32 0.33 0.38 0.23 0.34 0.46 0.23\n",
      " 0.33 0.31 0.38 0.39 0.39 0.34 0.38 0.35 0.39 0.23 0.35 0.29 0.43 0.41\n",
      " 0.38 0.31 0.34 0.35 0.47 0.37 0.4  0.3  0.3  0.47 0.35 0.25 0.39 0.31\n",
      " 0.31 0.35 0.29 0.4  0.41 0.29 0.36 0.36]\n",
      "[0.73 0.6  0.46 0.46 0.64 0.4  0.46 0.38 0.59 0.52 0.55 0.65 0.49 0.48\n",
      " 0.48 0.43 0.39 0.44 0.51 0.46 0.65 0.33 0.37 0.61 0.52 0.36 0.45 0.33\n",
      " 0.6  0.49 0.44 0.53 0.39 0.37 0.46 0.42 0.35 0.45 0.51 0.58 0.54 0.45\n",
      " 0.52 0.48 0.45 0.4  0.38 0.44 0.5  0.6 ]\n",
      "[0.25 0.3  0.39 0.25 0.31 0.39 0.39 0.35 0.39 0.25 0.21 0.39 0.3  0.33\n",
      " 0.42 0.34 0.39 0.4  0.35 0.3  0.28 0.38 0.4  0.35 0.32 0.3  0.45 0.4\n",
      " 0.29 0.5  0.29 0.3  0.35 0.3  0.35 0.26 0.22 0.31 0.29 0.31 0.29 0.33\n",
      " 0.37 0.42 0.38 0.33 0.36 0.33 0.32 0.31]\n",
      "1\n",
      "[0.39433333]\n",
      "[0.31 0.21 0.41 0.44 0.33 0.45 0.33 0.39 0.32 0.45 0.34 0.22 0.31 0.33\n",
      " 0.3  0.35 0.42 0.35 0.35 0.3  0.33 0.31 0.47 0.32 0.32 0.46 0.38 0.3\n",
      " 0.34 0.35 0.32 0.31 0.4  0.24 0.36 0.41 0.25 0.48 0.18 0.34 0.28 0.32\n",
      " 0.35 0.32 0.39 0.38 0.43 0.31 0.32 0.42]\n",
      "[0.42 0.41 0.26 0.32 0.41 0.42 0.37 0.35 0.42 0.38 0.41 0.36 0.38 0.28\n",
      " 0.42 0.38 0.34 0.37 0.36 0.38 0.34 0.39 0.38 0.46 0.46 0.39 0.52 0.35\n",
      " 0.45 0.41 0.45 0.51 0.44 0.42 0.41 0.31 0.35 0.58 0.56 0.39 0.36 0.35\n",
      " 0.44 0.41 0.25 0.35 0.49 0.38 0.47 0.45]\n",
      "[0.42 0.48 0.41 0.43 0.72 0.39 0.66 0.5  0.45 0.59 0.57 0.47 0.47 0.58\n",
      " 0.44 0.56 0.38 0.52 0.45 0.39 0.61 0.36 0.6  0.51 0.63 0.39 0.52 0.39\n",
      " 0.42 0.41 0.49 0.4  0.5  0.44 0.55 0.52 0.39 0.49 0.41 0.57 0.5  0.55\n",
      " 0.52 0.65 0.47 0.67 0.43 0.39 0.55 0.43]\n",
      "[0.31 0.38 0.4  0.29 0.29 0.34 0.4  0.41 0.37 0.31 0.3  0.38 0.37 0.41\n",
      " 0.42 0.36 0.32 0.29 0.45 0.36 0.34 0.34 0.34 0.31 0.25 0.36 0.43 0.28\n",
      " 0.3  0.44 0.37 0.33 0.27 0.34 0.28 0.39 0.19 0.43 0.33 0.32 0.27 0.28\n",
      " 0.41 0.3  0.32 0.26 0.38 0.36 0.34 0.37]\n",
      "2\n",
      "[0.39433333]\n",
      "[0.37 0.4  0.47 0.45 0.38 0.53 0.39 0.48 0.51 0.45 0.44 0.38 0.46 0.36\n",
      " 0.42 0.34 0.47 0.41 0.44 0.44 0.41 0.34 0.47 0.4  0.36 0.5  0.45 0.45\n",
      " 0.53 0.39 0.48 0.4  0.38 0.43 0.48 0.45 0.47 0.53 0.26 0.35 0.35 0.45\n",
      " 0.48 0.43 0.44 0.44 0.44 0.37 0.36 0.47]\n",
      "[0.39 0.39 0.33 0.31 0.37 0.39 0.33 0.37 0.35 0.47 0.43 0.35 0.38 0.25\n",
      " 0.44 0.36 0.34 0.25 0.45 0.39 0.4  0.39 0.33 0.43 0.39 0.34 0.47 0.4\n",
      " 0.38 0.44 0.34 0.41 0.44 0.44 0.44 0.46 0.45 0.51 0.5  0.33 0.44 0.33\n",
      " 0.4  0.38 0.33 0.45 0.42 0.41 0.46 0.4 ]\n",
      "[0.4  0.52 0.43 0.49 0.68 0.52 0.53 0.48 0.55 0.52 0.47 0.47 0.49 0.6\n",
      " 0.57 0.52 0.53 0.59 0.5  0.45 0.66 0.48 0.53 0.61 0.6  0.53 0.59 0.51\n",
      " 0.65 0.66 0.49 0.54 0.58 0.6  0.45 0.53 0.5  0.59 0.55 0.53 0.6  0.5\n",
      " 0.5  0.53 0.48 0.54 0.47 0.49 0.51 0.5 ]\n",
      "[0.35 0.38 0.33 0.22 0.36 0.43 0.36 0.38 0.38 0.3  0.36 0.37 0.4  0.44\n",
      " 0.46 0.29 0.36 0.28 0.45 0.37 0.36 0.45 0.29 0.37 0.33 0.4  0.45 0.36\n",
      " 0.31 0.4  0.41 0.31 0.37 0.44 0.34 0.32 0.27 0.35 0.33 0.33 0.32 0.35\n",
      " 0.29 0.38 0.29 0.35 0.44 0.42 0.41 0.31]\n",
      "HT-naive-tsk_False-lr0_001\n",
      "0\n",
      "[0.176]\n",
      "[0.44 0.32 0.44 0.43 0.42 0.48 0.33 0.43 0.33 0.33 0.4  0.3  0.47 0.41\n",
      " 0.44 0.32 0.46 0.38 0.45 0.49 0.43 0.28 0.51 0.45 0.37 0.48 0.46 0.43\n",
      " 0.38 0.38 0.45 0.44 0.45 0.41 0.41 0.36 0.37 0.51 0.33 0.4  0.39 0.33\n",
      " 0.47 0.33 0.42 0.24 0.46 0.43 0.37 0.38]\n",
      "[0.43 0.36 0.27 0.31 0.28 0.35 0.25 0.34 0.39 0.4  0.26 0.38 0.47 0.27\n",
      " 0.34 0.3  0.36 0.3  0.38 0.26 0.36 0.39 0.38 0.28 0.35 0.26 0.47 0.31\n",
      " 0.4  0.34 0.34 0.41 0.44 0.4  0.43 0.39 0.32 0.49 0.35 0.29 0.31 0.29\n",
      " 0.27 0.37 0.28 0.4  0.48 0.35 0.35 0.36]\n",
      "[0.73 0.55 0.44 0.47 0.64 0.38 0.51 0.44 0.53 0.53 0.56 0.63 0.48 0.42\n",
      " 0.47 0.42 0.52 0.4  0.47 0.39 0.61 0.36 0.4  0.62 0.47 0.42 0.46 0.37\n",
      " 0.62 0.42 0.37 0.56 0.47 0.39 0.38 0.47 0.45 0.52 0.43 0.58 0.51 0.41\n",
      " 0.52 0.53 0.45 0.35 0.4  0.44 0.55 0.61]\n",
      "[0.31 0.33 0.41 0.24 0.31 0.38 0.33 0.37 0.33 0.26 0.23 0.38 0.35 0.38\n",
      " 0.36 0.36 0.35 0.3  0.38 0.33 0.36 0.28 0.39 0.34 0.33 0.36 0.43 0.39\n",
      " 0.24 0.34 0.37 0.29 0.35 0.27 0.37 0.3  0.19 0.4  0.3  0.3  0.31 0.31\n",
      " 0.33 0.37 0.28 0.31 0.38 0.39 0.33 0.36]\n",
      "1\n",
      "[0.176]\n",
      "[0.34 0.24 0.48 0.43 0.28 0.46 0.37 0.31 0.38 0.37 0.24 0.29 0.31 0.42\n",
      " 0.34 0.37 0.44 0.33 0.41 0.35 0.42 0.33 0.5  0.38 0.27 0.46 0.36 0.3\n",
      " 0.32 0.33 0.34 0.38 0.48 0.31 0.33 0.32 0.34 0.53 0.23 0.36 0.31 0.34\n",
      " 0.42 0.34 0.36 0.45 0.34 0.33 0.37 0.39]\n",
      "[0.41 0.44 0.31 0.28 0.5  0.33 0.33 0.4  0.37 0.44 0.38 0.31 0.42 0.34\n",
      " 0.4  0.41 0.37 0.39 0.45 0.36 0.47 0.39 0.32 0.45 0.49 0.42 0.47 0.35\n",
      " 0.43 0.38 0.42 0.44 0.48 0.45 0.43 0.37 0.37 0.56 0.43 0.38 0.44 0.48\n",
      " 0.43 0.4  0.26 0.46 0.46 0.42 0.47 0.39]\n",
      "[0.44 0.46 0.37 0.38 0.59 0.36 0.51 0.44 0.47 0.6  0.58 0.46 0.45 0.5\n",
      " 0.38 0.49 0.39 0.52 0.32 0.41 0.53 0.37 0.58 0.53 0.65 0.42 0.57 0.47\n",
      " 0.41 0.5  0.41 0.43 0.48 0.4  0.46 0.55 0.39 0.46 0.43 0.55 0.48 0.39\n",
      " 0.49 0.53 0.39 0.57 0.4  0.4  0.45 0.46]\n",
      "[0.28 0.39 0.39 0.27 0.3  0.35 0.42 0.37 0.38 0.28 0.31 0.32 0.43 0.42\n",
      " 0.4  0.23 0.42 0.35 0.42 0.41 0.35 0.39 0.4  0.39 0.26 0.41 0.49 0.31\n",
      " 0.34 0.4  0.25 0.34 0.38 0.33 0.37 0.37 0.27 0.44 0.3  0.35 0.33 0.33\n",
      " 0.44 0.37 0.31 0.31 0.42 0.39 0.43 0.39]\n",
      "2\n",
      "[0.176]\n",
      "[0.52 0.34 0.48 0.45 0.36 0.47 0.4  0.35 0.36 0.42 0.39 0.29 0.42 0.33\n",
      " 0.39 0.29 0.43 0.31 0.41 0.46 0.45 0.28 0.37 0.34 0.3  0.48 0.42 0.48\n",
      " 0.43 0.37 0.45 0.37 0.43 0.42 0.52 0.44 0.46 0.52 0.33 0.39 0.3  0.38\n",
      " 0.46 0.46 0.41 0.41 0.52 0.37 0.38 0.49]\n",
      "[0.47 0.3  0.3  0.26 0.34 0.28 0.33 0.33 0.38 0.32 0.3  0.36 0.39 0.34\n",
      " 0.37 0.28 0.39 0.34 0.46 0.3  0.4  0.36 0.3  0.38 0.31 0.3  0.38 0.27\n",
      " 0.38 0.31 0.31 0.45 0.46 0.36 0.44 0.36 0.36 0.47 0.4  0.33 0.4  0.32\n",
      " 0.25 0.4  0.23 0.36 0.42 0.37 0.35 0.43]\n",
      "[0.42 0.53 0.45 0.57 0.56 0.5  0.46 0.31 0.52 0.44 0.42 0.45 0.48 0.6\n",
      " 0.59 0.42 0.68 0.48 0.53 0.55 0.55 0.58 0.48 0.54 0.57 0.55 0.54 0.62\n",
      " 0.58 0.55 0.55 0.43 0.7  0.59 0.41 0.46 0.5  0.56 0.57 0.45 0.55 0.52\n",
      " 0.44 0.42 0.44 0.44 0.42 0.51 0.47 0.5 ]\n",
      "[0.25 0.39 0.3  0.28 0.27 0.4  0.38 0.34 0.42 0.3  0.3  0.35 0.35 0.44\n",
      " 0.47 0.32 0.39 0.23 0.35 0.33 0.35 0.45 0.29 0.37 0.3  0.37 0.47 0.47\n",
      " 0.32 0.33 0.27 0.24 0.32 0.36 0.29 0.4  0.22 0.36 0.3  0.26 0.33 0.3\n",
      " 0.34 0.42 0.3  0.34 0.41 0.35 0.34 0.37]\n",
      "HT-er-tsk_True-lr0_01\n",
      "0\n",
      "[0.47966667]\n",
      "[0.32 0.3  0.39 0.27 0.32 0.42 0.33 0.39 0.35 0.33 0.36 0.27 0.32 0.3\n",
      " 0.36 0.35 0.37 0.28 0.35 0.36 0.35 0.26 0.37 0.26 0.32 0.36 0.3  0.32\n",
      " 0.39 0.29 0.29 0.3  0.32 0.36 0.37 0.37 0.34 0.52 0.32 0.39 0.3  0.44\n",
      " 0.33 0.4  0.31 0.26 0.44 0.38 0.36 0.39]\n",
      "[0.31 0.29 0.15 0.34 0.3  0.18 0.25 0.2  0.24 0.3  0.2  0.33 0.42 0.28\n",
      " 0.26 0.19 0.29 0.32 0.33 0.24 0.32 0.29 0.26 0.23 0.32 0.22 0.25 0.31\n",
      " 0.39 0.39 0.25 0.29 0.3  0.26 0.39 0.28 0.28 0.32 0.3  0.27 0.3  0.22\n",
      " 0.24 0.33 0.28 0.35 0.31 0.34 0.23 0.29]\n",
      "[0.48 0.48 0.34 0.39 0.41 0.32 0.41 0.22 0.51 0.45 0.41 0.38 0.36 0.39\n",
      " 0.27 0.28 0.21 0.38 0.33 0.33 0.49 0.26 0.41 0.43 0.35 0.24 0.41 0.25\n",
      " 0.43 0.37 0.41 0.42 0.4  0.41 0.35 0.33 0.31 0.41 0.34 0.38 0.38 0.43\n",
      " 0.4  0.32 0.26 0.29 0.34 0.32 0.48 0.41]\n",
      "[0.19 0.27 0.24 0.28 0.2  0.35 0.29 0.28 0.33 0.31 0.29 0.32 0.3  0.33\n",
      " 0.4  0.32 0.26 0.29 0.35 0.26 0.28 0.33 0.29 0.31 0.31 0.32 0.4  0.24\n",
      " 0.15 0.31 0.28 0.32 0.31 0.2  0.38 0.27 0.22 0.28 0.26 0.41 0.29 0.25\n",
      " 0.35 0.2  0.33 0.21 0.34 0.31 0.32 0.28]\n",
      "1\n",
      "[0.47966667]\n",
      "[0.38 0.29 0.45 0.41 0.28 0.47 0.31 0.38 0.33 0.34 0.36 0.27 0.37 0.35\n",
      " 0.31 0.31 0.43 0.31 0.42 0.41 0.33 0.27 0.46 0.33 0.29 0.42 0.45 0.35\n",
      " 0.43 0.32 0.39 0.31 0.36 0.34 0.3  0.44 0.4  0.48 0.28 0.39 0.36 0.38\n",
      " 0.37 0.36 0.41 0.36 0.42 0.37 0.39 0.46]\n",
      "[0.41 0.4  0.29 0.37 0.48 0.31 0.3  0.39 0.34 0.47 0.31 0.34 0.4  0.34\n",
      " 0.41 0.34 0.37 0.37 0.3  0.38 0.38 0.44 0.29 0.41 0.4  0.35 0.45 0.42\n",
      " 0.38 0.38 0.41 0.42 0.39 0.36 0.51 0.31 0.37 0.52 0.36 0.25 0.32 0.4\n",
      " 0.29 0.41 0.27 0.44 0.36 0.42 0.41 0.47]\n",
      "[0.54 0.41 0.42 0.44 0.51 0.34 0.5  0.35 0.47 0.5  0.48 0.37 0.34 0.45\n",
      " 0.31 0.33 0.27 0.41 0.42 0.37 0.58 0.31 0.49 0.53 0.46 0.43 0.48 0.43\n",
      " 0.41 0.43 0.49 0.47 0.48 0.41 0.35 0.47 0.38 0.4  0.45 0.49 0.49 0.4\n",
      " 0.45 0.37 0.36 0.43 0.39 0.32 0.45 0.4 ]\n",
      "[0.19 0.38 0.32 0.26 0.22 0.35 0.43 0.37 0.36 0.28 0.29 0.35 0.3  0.47\n",
      " 0.41 0.3  0.36 0.24 0.39 0.33 0.33 0.42 0.27 0.29 0.29 0.35 0.44 0.24\n",
      " 0.25 0.37 0.33 0.27 0.35 0.37 0.43 0.3  0.22 0.44 0.31 0.32 0.34 0.32\n",
      " 0.37 0.23 0.35 0.31 0.39 0.35 0.36 0.31]\n",
      "2\n",
      "[0.47966667]\n",
      "[0.41 0.34 0.48 0.45 0.35 0.47 0.33 0.39 0.42 0.39 0.34 0.34 0.38 0.33\n",
      " 0.39 0.31 0.47 0.35 0.43 0.41 0.44 0.32 0.48 0.36 0.36 0.4  0.36 0.41\n",
      " 0.43 0.36 0.4  0.39 0.42 0.42 0.38 0.49 0.34 0.49 0.28 0.38 0.43 0.38\n",
      " 0.48 0.36 0.47 0.39 0.4  0.41 0.36 0.42]\n",
      "[0.39 0.4  0.26 0.4  0.36 0.42 0.31 0.31 0.3  0.47 0.24 0.28 0.36 0.4\n",
      " 0.4  0.3  0.37 0.32 0.39 0.32 0.45 0.43 0.44 0.41 0.47 0.31 0.39 0.33\n",
      " 0.4  0.43 0.41 0.41 0.34 0.4  0.47 0.36 0.43 0.51 0.42 0.36 0.33 0.36\n",
      " 0.34 0.4  0.28 0.4  0.46 0.4  0.34 0.41]\n",
      "[0.5  0.52 0.46 0.51 0.5  0.37 0.49 0.4  0.53 0.47 0.48 0.46 0.37 0.43\n",
      " 0.43 0.38 0.4  0.53 0.49 0.46 0.6  0.43 0.59 0.55 0.57 0.4  0.55 0.43\n",
      " 0.46 0.58 0.45 0.36 0.54 0.46 0.43 0.51 0.41 0.47 0.47 0.53 0.52 0.51\n",
      " 0.41 0.46 0.44 0.49 0.41 0.4  0.49 0.42]\n",
      "[0.15 0.27 0.27 0.35 0.32 0.31 0.34 0.33 0.32 0.29 0.26 0.39 0.34 0.46\n",
      " 0.38 0.28 0.32 0.29 0.4  0.36 0.38 0.31 0.35 0.32 0.34 0.33 0.45 0.31\n",
      " 0.26 0.38 0.32 0.4  0.29 0.31 0.37 0.44 0.25 0.43 0.34 0.26 0.25 0.34\n",
      " 0.39 0.27 0.38 0.26 0.38 0.28 0.3  0.27]\n",
      "HT-er-tsk_False-lr0_01\n",
      "0\n",
      "[0.194]\n",
      "[0.39 0.34 0.33 0.36 0.35 0.43 0.37 0.39 0.27 0.35 0.34 0.2  0.3  0.37\n",
      " 0.3  0.32 0.33 0.29 0.34 0.37 0.36 0.3  0.28 0.26 0.22 0.35 0.35 0.34\n",
      " 0.32 0.29 0.38 0.28 0.37 0.31 0.29 0.31 0.35 0.48 0.25 0.32 0.32 0.31\n",
      " 0.32 0.31 0.41 0.26 0.35 0.37 0.34 0.4 ]\n",
      "[0.35 0.33 0.22 0.27 0.26 0.2  0.18 0.28 0.3  0.3  0.25 0.29 0.35 0.3\n",
      " 0.27 0.21 0.22 0.2  0.28 0.25 0.27 0.29 0.28 0.24 0.26 0.28 0.33 0.23\n",
      " 0.34 0.29 0.26 0.33 0.28 0.35 0.31 0.29 0.33 0.36 0.33 0.27 0.27 0.26\n",
      " 0.19 0.3  0.27 0.33 0.28 0.29 0.23 0.3 ]\n",
      "[0.57 0.37 0.38 0.42 0.46 0.34 0.41 0.33 0.47 0.44 0.43 0.41 0.34 0.35\n",
      " 0.36 0.24 0.28 0.35 0.33 0.31 0.55 0.28 0.32 0.44 0.34 0.41 0.35 0.33\n",
      " 0.39 0.34 0.29 0.36 0.41 0.34 0.35 0.39 0.25 0.43 0.33 0.42 0.36 0.35\n",
      " 0.38 0.27 0.23 0.37 0.33 0.29 0.48 0.4 ]\n",
      "[0.2  0.36 0.33 0.24 0.2  0.29 0.24 0.33 0.31 0.27 0.26 0.35 0.19 0.37\n",
      " 0.42 0.21 0.3  0.3  0.31 0.23 0.37 0.34 0.22 0.33 0.28 0.29 0.32 0.24\n",
      " 0.28 0.28 0.31 0.29 0.34 0.29 0.37 0.28 0.16 0.32 0.28 0.32 0.17 0.29\n",
      " 0.26 0.24 0.32 0.26 0.31 0.34 0.27 0.28]\n",
      "1\n",
      "[0.194]\n",
      "[0.28 0.28 0.46 0.34 0.22 0.37 0.31 0.32 0.35 0.27 0.36 0.2  0.27 0.32\n",
      " 0.32 0.32 0.42 0.3  0.33 0.26 0.36 0.31 0.38 0.32 0.25 0.44 0.38 0.32\n",
      " 0.39 0.36 0.41 0.32 0.32 0.26 0.29 0.36 0.26 0.37 0.25 0.32 0.35 0.33\n",
      " 0.34 0.35 0.35 0.31 0.37 0.33 0.35 0.36]\n",
      "[0.4  0.38 0.28 0.27 0.33 0.33 0.26 0.32 0.27 0.36 0.31 0.28 0.39 0.3\n",
      " 0.41 0.28 0.34 0.29 0.38 0.33 0.36 0.31 0.31 0.33 0.39 0.33 0.44 0.35\n",
      " 0.36 0.35 0.35 0.39 0.38 0.33 0.34 0.34 0.27 0.49 0.41 0.23 0.33 0.34\n",
      " 0.38 0.36 0.22 0.39 0.35 0.33 0.34 0.32]\n",
      "[0.37 0.47 0.45 0.47 0.64 0.44 0.64 0.49 0.58 0.49 0.52 0.42 0.37 0.59\n",
      " 0.43 0.5  0.39 0.52 0.4  0.42 0.57 0.42 0.6  0.56 0.58 0.4  0.56 0.43\n",
      " 0.47 0.43 0.54 0.44 0.5  0.38 0.54 0.57 0.34 0.48 0.5  0.52 0.59 0.51\n",
      " 0.46 0.54 0.45 0.54 0.41 0.48 0.55 0.46]\n",
      "[0.18 0.35 0.35 0.31 0.22 0.35 0.31 0.3  0.34 0.3  0.22 0.31 0.28 0.31\n",
      " 0.36 0.29 0.32 0.29 0.29 0.28 0.28 0.38 0.32 0.3  0.24 0.38 0.43 0.22\n",
      " 0.24 0.29 0.34 0.29 0.31 0.33 0.29 0.31 0.17 0.31 0.31 0.29 0.26 0.34\n",
      " 0.24 0.21 0.31 0.25 0.35 0.31 0.25 0.33]\n",
      "2\n",
      "[0.194]\n",
      "[0.38 0.39 0.46 0.39 0.28 0.46 0.35 0.33 0.29 0.41 0.34 0.29 0.4  0.32\n",
      " 0.41 0.39 0.44 0.34 0.39 0.36 0.45 0.34 0.41 0.29 0.34 0.45 0.34 0.33\n",
      " 0.4  0.34 0.37 0.44 0.47 0.35 0.32 0.39 0.33 0.49 0.25 0.32 0.36 0.37\n",
      " 0.37 0.37 0.4  0.45 0.46 0.3  0.33 0.33]\n",
      "[0.4  0.38 0.3  0.3  0.37 0.38 0.31 0.4  0.31 0.39 0.32 0.25 0.42 0.34\n",
      " 0.37 0.25 0.36 0.35 0.39 0.35 0.34 0.31 0.35 0.48 0.38 0.33 0.42 0.41\n",
      " 0.38 0.37 0.39 0.45 0.41 0.37 0.45 0.35 0.34 0.44 0.41 0.33 0.37 0.33\n",
      " 0.33 0.4  0.27 0.41 0.47 0.4  0.44 0.34]\n",
      "[0.4  0.49 0.43 0.46 0.63 0.42 0.69 0.52 0.58 0.59 0.53 0.47 0.42 0.56\n",
      " 0.56 0.5  0.45 0.63 0.49 0.43 0.62 0.37 0.61 0.53 0.55 0.4  0.59 0.52\n",
      " 0.52 0.61 0.54 0.5  0.62 0.58 0.47 0.58 0.44 0.54 0.48 0.56 0.64 0.61\n",
      " 0.46 0.55 0.45 0.53 0.43 0.53 0.47 0.51]\n",
      "[0.25 0.36 0.35 0.27 0.39 0.39 0.31 0.34 0.34 0.35 0.3  0.34 0.33 0.37\n",
      " 0.36 0.35 0.27 0.2  0.31 0.29 0.33 0.4  0.29 0.31 0.22 0.39 0.47 0.38\n",
      " 0.24 0.35 0.36 0.29 0.35 0.3  0.33 0.27 0.24 0.4  0.36 0.31 0.3  0.31\n",
      " 0.33 0.26 0.38 0.23 0.36 0.29 0.35 0.31]\n",
      "HT-gem-tsk_True-lr0_001-p16-m0_3\n",
      "0\n",
      "[0.25866667]\n",
      "[0.49 0.32 0.45 0.46 0.35 0.49 0.37 0.41 0.36 0.41 0.37 0.36 0.34 0.41\n",
      " 0.39 0.4  0.45 0.38 0.49 0.47 0.39 0.39 0.48 0.39 0.4  0.42 0.35 0.4\n",
      " 0.42 0.38 0.44 0.41 0.46 0.33 0.36 0.4  0.41 0.53 0.27 0.42 0.43 0.31\n",
      " 0.39 0.42 0.42 0.36 0.51 0.4  0.46 0.43]\n",
      "[0.38 0.32 0.3  0.31 0.3  0.35 0.23 0.33 0.39 0.37 0.33 0.39 0.44 0.3\n",
      " 0.31 0.32 0.35 0.31 0.47 0.29 0.34 0.31 0.3  0.25 0.38 0.35 0.48 0.32\n",
      " 0.44 0.33 0.31 0.4  0.39 0.39 0.42 0.29 0.37 0.47 0.34 0.28 0.41 0.35\n",
      " 0.26 0.29 0.27 0.4  0.47 0.33 0.28 0.33]\n",
      "[0.76 0.49 0.54 0.48 0.59 0.43 0.41 0.41 0.58 0.6  0.62 0.62 0.52 0.52\n",
      " 0.38 0.42 0.48 0.46 0.41 0.41 0.57 0.34 0.41 0.63 0.51 0.46 0.45 0.4\n",
      " 0.56 0.48 0.51 0.54 0.41 0.46 0.45 0.45 0.48 0.53 0.46 0.59 0.49 0.44\n",
      " 0.63 0.56 0.52 0.35 0.43 0.41 0.53 0.59]\n",
      "[0.28 0.29 0.4  0.21 0.3  0.4  0.4  0.31 0.38 0.28 0.3  0.34 0.33 0.31\n",
      " 0.44 0.27 0.36 0.34 0.42 0.34 0.3  0.36 0.3  0.41 0.3  0.38 0.49 0.35\n",
      " 0.32 0.42 0.37 0.32 0.4  0.32 0.33 0.27 0.18 0.34 0.3  0.32 0.26 0.34\n",
      " 0.44 0.33 0.28 0.29 0.39 0.42 0.26 0.32]\n",
      "1\n",
      "[0.25866667]\n",
      "[0.28 0.25 0.43 0.33 0.3  0.46 0.33 0.34 0.3  0.34 0.23 0.24 0.34 0.36\n",
      " 0.23 0.33 0.41 0.21 0.31 0.36 0.22 0.36 0.53 0.3  0.26 0.45 0.34 0.28\n",
      " 0.29 0.34 0.39 0.28 0.34 0.14 0.38 0.3  0.31 0.5  0.16 0.43 0.24 0.31\n",
      " 0.39 0.28 0.31 0.33 0.36 0.3  0.29 0.35]\n",
      "[0.42 0.39 0.26 0.33 0.45 0.4  0.32 0.33 0.42 0.37 0.47 0.27 0.42 0.27\n",
      " 0.45 0.32 0.42 0.34 0.37 0.36 0.38 0.37 0.29 0.45 0.49 0.35 0.42 0.45\n",
      " 0.42 0.46 0.47 0.48 0.42 0.39 0.44 0.47 0.4  0.53 0.43 0.33 0.46 0.41\n",
      " 0.43 0.35 0.26 0.42 0.47 0.35 0.46 0.41]\n",
      "[0.5  0.5  0.43 0.43 0.62 0.38 0.75 0.54 0.52 0.64 0.52 0.53 0.5  0.52\n",
      " 0.45 0.64 0.46 0.59 0.43 0.44 0.64 0.38 0.7  0.52 0.67 0.42 0.64 0.51\n",
      " 0.47 0.42 0.55 0.42 0.48 0.41 0.63 0.62 0.44 0.42 0.45 0.6  0.68 0.67\n",
      " 0.56 0.68 0.5  0.67 0.41 0.5  0.59 0.42]\n",
      "[0.35 0.25 0.27 0.31 0.19 0.33 0.35 0.33 0.27 0.25 0.29 0.27 0.34 0.36\n",
      " 0.35 0.24 0.32 0.33 0.41 0.39 0.21 0.37 0.27 0.25 0.26 0.28 0.33 0.24\n",
      " 0.24 0.25 0.3  0.31 0.33 0.26 0.33 0.29 0.29 0.34 0.22 0.3  0.24 0.26\n",
      " 0.32 0.38 0.29 0.26 0.38 0.34 0.32 0.21]\n",
      "2\n",
      "[0.25866667]\n",
      "[0.42 0.31 0.5  0.39 0.41 0.52 0.38 0.45 0.38 0.41 0.38 0.36 0.38 0.3\n",
      " 0.34 0.39 0.48 0.37 0.4  0.52 0.43 0.37 0.39 0.4  0.31 0.48 0.43 0.49\n",
      " 0.42 0.36 0.43 0.4  0.44 0.37 0.48 0.44 0.41 0.54 0.2  0.43 0.39 0.47\n",
      " 0.51 0.41 0.5  0.4  0.46 0.45 0.37 0.42]\n",
      "[0.46 0.38 0.34 0.35 0.51 0.36 0.34 0.41 0.37 0.32 0.39 0.31 0.37 0.38\n",
      " 0.45 0.34 0.39 0.32 0.49 0.39 0.45 0.43 0.33 0.47 0.41 0.31 0.43 0.42\n",
      " 0.37 0.51 0.41 0.46 0.49 0.41 0.45 0.45 0.37 0.49 0.49 0.29 0.39 0.38\n",
      " 0.4  0.42 0.25 0.5  0.49 0.39 0.47 0.44]\n",
      "[0.44 0.59 0.49 0.5  0.63 0.53 0.54 0.52 0.53 0.52 0.52 0.5  0.45 0.56\n",
      " 0.61 0.51 0.52 0.62 0.44 0.55 0.68 0.47 0.6  0.55 0.64 0.47 0.64 0.56\n",
      " 0.59 0.63 0.51 0.54 0.62 0.6  0.52 0.51 0.5  0.62 0.52 0.57 0.64 0.53\n",
      " 0.54 0.59 0.47 0.5  0.43 0.47 0.59 0.48]\n",
      "[0.34 0.28 0.36 0.31 0.28 0.38 0.38 0.33 0.35 0.31 0.31 0.36 0.32 0.39\n",
      " 0.46 0.28 0.34 0.34 0.34 0.36 0.33 0.32 0.28 0.42 0.26 0.33 0.46 0.35\n",
      " 0.26 0.37 0.26 0.29 0.3  0.38 0.35 0.27 0.25 0.39 0.24 0.31 0.25 0.35\n",
      " 0.34 0.34 0.27 0.28 0.44 0.35 0.32 0.29]\n",
      "HT-gem-tsk_False-lr0_001-p256-m0_00139\n",
      "0\n",
      "[0.17633333]\n",
      "[0.36 0.36 0.4  0.4  0.4  0.54 0.36 0.39 0.32 0.4  0.42 0.35 0.5  0.43\n",
      " 0.39 0.34 0.43 0.35 0.43 0.48 0.42 0.33 0.47 0.4  0.41 0.46 0.44 0.42\n",
      " 0.38 0.43 0.38 0.39 0.46 0.29 0.4  0.37 0.44 0.6  0.31 0.4  0.34 0.41\n",
      " 0.48 0.4  0.43 0.3  0.47 0.44 0.45 0.41]\n",
      "[0.41 0.37 0.34 0.35 0.29 0.38 0.26 0.35 0.38 0.41 0.25 0.35 0.39 0.24\n",
      " 0.32 0.31 0.37 0.27 0.42 0.26 0.35 0.32 0.38 0.33 0.38 0.25 0.45 0.38\n",
      " 0.44 0.34 0.37 0.38 0.4  0.37 0.44 0.35 0.31 0.49 0.37 0.25 0.41 0.37\n",
      " 0.34 0.37 0.28 0.39 0.42 0.34 0.45 0.4 ]\n",
      "[0.73 0.48 0.44 0.43 0.6  0.35 0.46 0.43 0.58 0.56 0.58 0.59 0.56 0.46\n",
      " 0.44 0.5  0.47 0.41 0.49 0.47 0.61 0.37 0.4  0.54 0.49 0.41 0.46 0.38\n",
      " 0.61 0.47 0.44 0.52 0.49 0.36 0.45 0.42 0.4  0.53 0.46 0.55 0.48 0.44\n",
      " 0.53 0.43 0.41 0.32 0.41 0.43 0.47 0.62]\n",
      "[0.3  0.36 0.35 0.28 0.33 0.38 0.33 0.35 0.37 0.25 0.29 0.39 0.31 0.32\n",
      " 0.43 0.31 0.38 0.38 0.45 0.37 0.32 0.37 0.37 0.35 0.33 0.39 0.5  0.32\n",
      " 0.25 0.39 0.28 0.28 0.38 0.29 0.31 0.29 0.23 0.35 0.32 0.32 0.32 0.3\n",
      " 0.38 0.41 0.33 0.3  0.36 0.33 0.29 0.33]\n",
      "1\n",
      "[0.17633333]\n",
      "[0.35 0.27 0.46 0.41 0.28 0.37 0.39 0.39 0.29 0.47 0.29 0.18 0.39 0.41\n",
      " 0.33 0.37 0.43 0.27 0.39 0.32 0.31 0.28 0.48 0.4  0.23 0.42 0.37 0.34\n",
      " 0.34 0.43 0.31 0.41 0.43 0.28 0.41 0.41 0.26 0.42 0.2  0.34 0.3  0.35\n",
      " 0.38 0.28 0.39 0.41 0.4  0.42 0.31 0.45]\n",
      "[0.45 0.41 0.33 0.32 0.4  0.32 0.3  0.36 0.39 0.4  0.39 0.32 0.42 0.31\n",
      " 0.37 0.39 0.33 0.38 0.46 0.43 0.39 0.36 0.33 0.48 0.5  0.37 0.4  0.36\n",
      " 0.45 0.39 0.4  0.48 0.49 0.43 0.45 0.39 0.36 0.52 0.46 0.37 0.4  0.46\n",
      " 0.37 0.35 0.25 0.4  0.44 0.42 0.51 0.43]\n",
      "[0.44 0.48 0.4  0.41 0.53 0.37 0.52 0.46 0.46 0.56 0.5  0.41 0.44 0.49\n",
      " 0.42 0.49 0.42 0.51 0.36 0.38 0.59 0.32 0.64 0.46 0.61 0.38 0.55 0.43\n",
      " 0.39 0.45 0.34 0.34 0.47 0.42 0.43 0.5  0.42 0.42 0.39 0.55 0.52 0.48\n",
      " 0.43 0.53 0.5  0.62 0.37 0.41 0.41 0.41]\n",
      "[0.3  0.44 0.3  0.27 0.32 0.33 0.39 0.38 0.4  0.22 0.25 0.44 0.35 0.38\n",
      " 0.42 0.32 0.38 0.3  0.39 0.38 0.33 0.32 0.34 0.3  0.28 0.37 0.42 0.32\n",
      " 0.32 0.31 0.28 0.34 0.34 0.36 0.34 0.34 0.24 0.34 0.27 0.31 0.27 0.32\n",
      " 0.4  0.38 0.34 0.26 0.41 0.37 0.4  0.37]\n",
      "2\n",
      "[0.17633333]\n",
      "[0.41 0.31 0.38 0.39 0.31 0.53 0.42 0.36 0.41 0.37 0.37 0.31 0.44 0.38\n",
      " 0.38 0.38 0.39 0.38 0.4  0.44 0.45 0.38 0.43 0.43 0.26 0.47 0.38 0.52\n",
      " 0.47 0.36 0.35 0.41 0.38 0.36 0.37 0.39 0.4  0.48 0.25 0.36 0.25 0.39\n",
      " 0.52 0.32 0.44 0.38 0.48 0.44 0.41 0.43]\n",
      "[0.38 0.32 0.3  0.26 0.3  0.34 0.29 0.37 0.42 0.3  0.35 0.31 0.38 0.29\n",
      " 0.44 0.29 0.31 0.28 0.32 0.38 0.41 0.35 0.29 0.43 0.36 0.2  0.38 0.41\n",
      " 0.42 0.4  0.34 0.45 0.41 0.32 0.45 0.33 0.34 0.42 0.48 0.37 0.45 0.34\n",
      " 0.3  0.38 0.24 0.34 0.4  0.4  0.36 0.34]\n",
      "[0.4  0.51 0.49 0.59 0.55 0.61 0.51 0.43 0.55 0.49 0.45 0.47 0.45 0.58\n",
      " 0.57 0.46 0.54 0.56 0.61 0.7  0.59 0.63 0.49 0.61 0.67 0.62 0.55 0.61\n",
      " 0.66 0.65 0.48 0.54 0.71 0.7  0.48 0.51 0.53 0.55 0.72 0.58 0.56 0.55\n",
      " 0.44 0.51 0.52 0.49 0.51 0.47 0.57 0.48]\n",
      "[0.31 0.35 0.34 0.28 0.34 0.38 0.36 0.37 0.32 0.27 0.33 0.29 0.29 0.38\n",
      " 0.42 0.3  0.33 0.27 0.35 0.34 0.38 0.42 0.35 0.32 0.37 0.33 0.37 0.32\n",
      " 0.26 0.43 0.31 0.24 0.33 0.31 0.26 0.34 0.29 0.39 0.34 0.34 0.26 0.26\n",
      " 0.32 0.29 0.21 0.26 0.44 0.39 0.36 0.29]\n",
      "HT-lwf-tsk_True-lr0_001-a1-t2\n",
      "0\n",
      "[0.54533333]\n",
      "[0.39 0.36 0.52 0.38 0.43 0.49 0.4  0.42 0.35 0.41 0.43 0.35 0.44 0.41\n",
      " 0.38 0.34 0.4  0.35 0.39 0.47 0.42 0.37 0.5  0.37 0.34 0.46 0.49 0.38\n",
      " 0.43 0.4  0.38 0.42 0.47 0.37 0.41 0.36 0.4  0.6  0.28 0.42 0.37 0.43\n",
      " 0.48 0.42 0.44 0.28 0.47 0.43 0.33 0.4 ]\n",
      "[0.36 0.38 0.29 0.31 0.31 0.4  0.3  0.27 0.36 0.37 0.25 0.29 0.42 0.25\n",
      " 0.35 0.27 0.42 0.31 0.45 0.35 0.35 0.35 0.3  0.27 0.32 0.27 0.46 0.43\n",
      " 0.39 0.36 0.32 0.39 0.44 0.43 0.45 0.33 0.28 0.54 0.36 0.28 0.36 0.38\n",
      " 0.35 0.37 0.28 0.42 0.37 0.35 0.36 0.35]\n",
      "[0.71 0.48 0.44 0.41 0.63 0.39 0.48 0.42 0.56 0.57 0.52 0.52 0.49 0.52\n",
      " 0.38 0.45 0.41 0.45 0.46 0.4  0.6  0.31 0.35 0.61 0.51 0.43 0.46 0.34\n",
      " 0.57 0.46 0.44 0.54 0.46 0.44 0.36 0.37 0.38 0.5  0.5  0.54 0.47 0.46\n",
      " 0.51 0.4  0.4  0.4  0.43 0.43 0.47 0.53]\n",
      "[0.3  0.36 0.38 0.26 0.3  0.34 0.39 0.36 0.4  0.36 0.27 0.44 0.31 0.36\n",
      " 0.42 0.4  0.32 0.38 0.44 0.39 0.36 0.43 0.33 0.4  0.26 0.34 0.51 0.37\n",
      " 0.18 0.47 0.3  0.3  0.41 0.34 0.38 0.28 0.26 0.32 0.36 0.39 0.34 0.35\n",
      " 0.4  0.39 0.3  0.31 0.44 0.36 0.32 0.34]\n",
      "1\n",
      "[0.54533333]\n",
      "[0.4  0.35 0.52 0.47 0.35 0.51 0.4  0.51 0.39 0.53 0.43 0.35 0.41 0.45\n",
      " 0.39 0.43 0.54 0.41 0.42 0.45 0.47 0.33 0.61 0.41 0.35 0.5  0.52 0.46\n",
      " 0.45 0.43 0.43 0.41 0.46 0.43 0.36 0.42 0.41 0.63 0.25 0.48 0.43 0.45\n",
      " 0.51 0.37 0.45 0.4  0.49 0.48 0.44 0.52]\n",
      "[0.53 0.4  0.35 0.36 0.5  0.42 0.32 0.36 0.4  0.45 0.4  0.46 0.56 0.33\n",
      " 0.49 0.38 0.39 0.4  0.57 0.45 0.43 0.4  0.37 0.48 0.45 0.35 0.54 0.45\n",
      " 0.55 0.41 0.41 0.48 0.54 0.47 0.46 0.33 0.44 0.63 0.56 0.35 0.38 0.43\n",
      " 0.44 0.4  0.35 0.43 0.51 0.47 0.5  0.42]\n",
      "[0.58 0.54 0.51 0.43 0.73 0.38 0.6  0.51 0.57 0.61 0.6  0.6  0.58 0.6\n",
      " 0.44 0.58 0.44 0.55 0.4  0.45 0.65 0.41 0.63 0.58 0.63 0.52 0.65 0.54\n",
      " 0.55 0.53 0.54 0.45 0.54 0.4  0.51 0.53 0.47 0.54 0.65 0.68 0.6  0.55\n",
      " 0.57 0.56 0.49 0.68 0.42 0.48 0.47 0.58]\n",
      "[0.41 0.35 0.38 0.31 0.36 0.4  0.5  0.38 0.45 0.36 0.42 0.45 0.39 0.56\n",
      " 0.46 0.33 0.44 0.39 0.52 0.38 0.37 0.46 0.46 0.46 0.3  0.38 0.51 0.39\n",
      " 0.35 0.51 0.36 0.4  0.36 0.33 0.41 0.37 0.25 0.46 0.36 0.39 0.37 0.37\n",
      " 0.37 0.4  0.38 0.3  0.51 0.36 0.38 0.41]\n",
      "2\n",
      "[0.54533333]\n",
      "[0.52 0.38 0.49 0.53 0.45 0.53 0.44 0.56 0.37 0.49 0.49 0.41 0.46 0.45\n",
      " 0.43 0.42 0.45 0.43 0.5  0.56 0.5  0.34 0.61 0.37 0.37 0.57 0.49 0.54\n",
      " 0.46 0.44 0.45 0.52 0.48 0.4  0.48 0.45 0.38 0.65 0.32 0.46 0.39 0.38\n",
      " 0.54 0.45 0.49 0.47 0.46 0.48 0.47 0.47]\n",
      "[0.53 0.4  0.31 0.36 0.49 0.37 0.36 0.4  0.42 0.47 0.42 0.38 0.48 0.34\n",
      " 0.43 0.31 0.39 0.38 0.45 0.39 0.43 0.42 0.4  0.49 0.47 0.32 0.47 0.53\n",
      " 0.53 0.37 0.43 0.47 0.49 0.46 0.49 0.36 0.49 0.59 0.5  0.4  0.45 0.41\n",
      " 0.39 0.43 0.28 0.52 0.54 0.47 0.44 0.45]\n",
      "[0.55 0.55 0.62 0.52 0.65 0.47 0.55 0.5  0.61 0.7  0.62 0.49 0.54 0.61\n",
      " 0.48 0.63 0.6  0.59 0.51 0.58 0.68 0.52 0.56 0.59 0.69 0.55 0.62 0.59\n",
      " 0.67 0.66 0.57 0.52 0.65 0.55 0.5  0.57 0.51 0.63 0.63 0.65 0.62 0.54\n",
      " 0.46 0.56 0.56 0.57 0.48 0.5  0.56 0.6 ]\n",
      "[0.37 0.38 0.32 0.36 0.4  0.42 0.48 0.42 0.41 0.33 0.4  0.4  0.4  0.45\n",
      " 0.56 0.37 0.42 0.37 0.51 0.44 0.37 0.45 0.38 0.41 0.33 0.33 0.49 0.43\n",
      " 0.35 0.49 0.35 0.34 0.45 0.39 0.43 0.41 0.24 0.47 0.3  0.32 0.3  0.4\n",
      " 0.42 0.44 0.35 0.37 0.5  0.39 0.34 0.43]\n",
      "HT-lwf-tsk_False-lr0_001-a1-t1_52\n",
      "0\n",
      "[0.184]\n",
      "[0.37 0.39 0.43 0.4  0.47 0.55 0.37 0.49 0.32 0.38 0.48 0.37 0.46 0.45\n",
      " 0.41 0.41 0.45 0.37 0.42 0.46 0.39 0.26 0.51 0.31 0.42 0.45 0.47 0.43\n",
      " 0.41 0.42 0.39 0.4  0.44 0.42 0.37 0.39 0.42 0.57 0.27 0.42 0.37 0.36\n",
      " 0.46 0.41 0.4  0.26 0.49 0.43 0.42 0.47]\n",
      "[0.42 0.32 0.29 0.4  0.32 0.36 0.25 0.3  0.36 0.37 0.28 0.35 0.49 0.22\n",
      " 0.32 0.3  0.32 0.33 0.4  0.3  0.38 0.33 0.4  0.2  0.29 0.25 0.46 0.41\n",
      " 0.45 0.35 0.28 0.41 0.42 0.49 0.4  0.34 0.27 0.55 0.36 0.27 0.39 0.3\n",
      " 0.33 0.37 0.28 0.41 0.46 0.35 0.37 0.41]\n",
      "[0.71 0.48 0.56 0.42 0.62 0.36 0.47 0.42 0.51 0.53 0.52 0.6  0.48 0.47\n",
      " 0.45 0.31 0.46 0.43 0.52 0.37 0.66 0.32 0.41 0.53 0.48 0.4  0.44 0.42\n",
      " 0.51 0.46 0.45 0.57 0.43 0.37 0.35 0.43 0.37 0.46 0.44 0.65 0.56 0.49\n",
      " 0.52 0.46 0.37 0.36 0.43 0.38 0.5  0.56]\n",
      "[0.26 0.3  0.43 0.23 0.31 0.3  0.4  0.4  0.37 0.23 0.23 0.37 0.31 0.37\n",
      " 0.36 0.34 0.33 0.37 0.36 0.25 0.39 0.34 0.33 0.41 0.3  0.36 0.45 0.42\n",
      " 0.31 0.48 0.29 0.39 0.39 0.32 0.32 0.27 0.21 0.38 0.34 0.34 0.23 0.33\n",
      " 0.38 0.37 0.33 0.3  0.39 0.32 0.31 0.34]\n",
      "1\n",
      "[0.184]\n",
      "[0.43 0.3  0.47 0.49 0.4  0.51 0.4  0.49 0.33 0.47 0.42 0.33 0.5  0.43\n",
      " 0.41 0.48 0.55 0.37 0.44 0.46 0.48 0.32 0.6  0.45 0.35 0.54 0.51 0.47\n",
      " 0.45 0.43 0.41 0.44 0.53 0.36 0.37 0.38 0.42 0.59 0.28 0.45 0.42 0.41\n",
      " 0.45 0.38 0.47 0.41 0.47 0.42 0.45 0.52]\n",
      "[0.45 0.42 0.31 0.42 0.44 0.41 0.35 0.33 0.33 0.45 0.39 0.41 0.51 0.31\n",
      " 0.47 0.37 0.35 0.43 0.49 0.39 0.43 0.39 0.38 0.42 0.47 0.38 0.51 0.51\n",
      " 0.49 0.35 0.46 0.49 0.47 0.5  0.5  0.35 0.4  0.61 0.56 0.41 0.44 0.4\n",
      " 0.4  0.39 0.3  0.55 0.54 0.44 0.43 0.44]\n",
      "[0.61 0.5  0.51 0.47 0.69 0.49 0.63 0.5  0.6  0.66 0.64 0.54 0.5  0.59\n",
      " 0.47 0.56 0.46 0.53 0.48 0.43 0.68 0.41 0.54 0.54 0.61 0.42 0.61 0.49\n",
      " 0.55 0.52 0.52 0.49 0.52 0.41 0.48 0.53 0.42 0.52 0.6  0.61 0.61 0.53\n",
      " 0.57 0.61 0.47 0.57 0.47 0.47 0.52 0.56]\n",
      "[0.28 0.41 0.38 0.29 0.32 0.4  0.52 0.45 0.39 0.28 0.36 0.37 0.33 0.43\n",
      " 0.46 0.34 0.4  0.41 0.43 0.42 0.32 0.4  0.47 0.4  0.27 0.4  0.5  0.4\n",
      " 0.33 0.48 0.3  0.38 0.47 0.38 0.38 0.29 0.24 0.48 0.39 0.3  0.26 0.35\n",
      " 0.38 0.4  0.35 0.3  0.45 0.41 0.43 0.38]\n",
      "2\n",
      "[0.184]\n",
      "[0.51 0.37 0.55 0.46 0.41 0.56 0.43 0.54 0.43 0.52 0.46 0.42 0.49 0.47\n",
      " 0.33 0.38 0.5  0.43 0.53 0.43 0.57 0.39 0.64 0.43 0.4  0.52 0.52 0.53\n",
      " 0.49 0.37 0.48 0.45 0.54 0.49 0.48 0.48 0.45 0.62 0.35 0.51 0.43 0.44\n",
      " 0.56 0.5  0.46 0.47 0.53 0.47 0.43 0.54]\n",
      "[0.52 0.38 0.35 0.39 0.43 0.43 0.34 0.36 0.41 0.47 0.4  0.34 0.49 0.36\n",
      " 0.47 0.37 0.38 0.31 0.51 0.35 0.43 0.42 0.38 0.5  0.45 0.36 0.46 0.48\n",
      " 0.44 0.4  0.44 0.46 0.5  0.5  0.51 0.45 0.38 0.61 0.53 0.38 0.51 0.42\n",
      " 0.39 0.46 0.26 0.49 0.59 0.45 0.45 0.38]\n",
      "[0.58 0.56 0.49 0.5  0.73 0.51 0.53 0.45 0.62 0.69 0.58 0.51 0.5  0.61\n",
      " 0.46 0.58 0.54 0.54 0.48 0.54 0.69 0.48 0.55 0.64 0.67 0.51 0.7  0.55\n",
      " 0.66 0.65 0.5  0.52 0.61 0.57 0.41 0.56 0.5  0.63 0.54 0.61 0.59 0.58\n",
      " 0.54 0.57 0.48 0.51 0.46 0.48 0.57 0.53]\n",
      "[0.35 0.41 0.41 0.35 0.41 0.49 0.45 0.47 0.44 0.37 0.3  0.4  0.39 0.48\n",
      " 0.46 0.34 0.48 0.44 0.46 0.45 0.37 0.42 0.44 0.4  0.28 0.38 0.49 0.47\n",
      " 0.36 0.46 0.31 0.31 0.47 0.44 0.45 0.42 0.31 0.52 0.42 0.35 0.38 0.38\n",
      " 0.38 0.39 0.31 0.39 0.45 0.4  0.39 0.44]\n",
      "HT-ewc-tsk_True-lr0_01-lambda100\n",
      "0\n",
      "[0.46]\n",
      "[0.29 0.32 0.38 0.32 0.31 0.39 0.4  0.39 0.3  0.38 0.33 0.24 0.35 0.32\n",
      " 0.31 0.28 0.33 0.33 0.39 0.39 0.35 0.25 0.35 0.24 0.33 0.4  0.37 0.36\n",
      " 0.33 0.27 0.36 0.33 0.41 0.34 0.3  0.27 0.35 0.41 0.27 0.34 0.36 0.32\n",
      " 0.33 0.26 0.36 0.25 0.4  0.4  0.29 0.31]\n",
      "[0.3  0.32 0.17 0.21 0.3  0.29 0.22 0.26 0.28 0.26 0.25 0.24 0.29 0.21\n",
      " 0.32 0.22 0.26 0.28 0.29 0.23 0.27 0.33 0.31 0.29 0.29 0.17 0.36 0.32\n",
      " 0.37 0.33 0.33 0.35 0.3  0.27 0.31 0.29 0.34 0.39 0.33 0.27 0.23 0.28\n",
      " 0.25 0.24 0.29 0.38 0.28 0.29 0.25 0.37]\n",
      "[0.44 0.36 0.38 0.37 0.44 0.34 0.38 0.37 0.36 0.38 0.43 0.39 0.27 0.34\n",
      " 0.29 0.35 0.28 0.32 0.35 0.3  0.43 0.24 0.32 0.44 0.34 0.31 0.35 0.26\n",
      " 0.48 0.42 0.32 0.4  0.45 0.3  0.41 0.33 0.31 0.38 0.38 0.33 0.38 0.43\n",
      " 0.3  0.3  0.39 0.28 0.29 0.34 0.42 0.41]\n",
      "[0.23 0.26 0.29 0.24 0.29 0.26 0.24 0.28 0.21 0.27 0.26 0.3  0.27 0.38\n",
      " 0.37 0.22 0.3  0.31 0.33 0.28 0.23 0.31 0.26 0.29 0.27 0.29 0.39 0.26\n",
      " 0.24 0.33 0.3  0.35 0.31 0.25 0.28 0.25 0.13 0.21 0.32 0.36 0.17 0.26\n",
      " 0.27 0.22 0.27 0.3  0.32 0.23 0.33 0.29]\n",
      "1\n",
      "[0.46]\n",
      "[0.3  0.21 0.39 0.31 0.21 0.43 0.31 0.37 0.26 0.4  0.33 0.22 0.35 0.3\n",
      " 0.35 0.35 0.43 0.25 0.39 0.29 0.33 0.23 0.37 0.33 0.31 0.39 0.42 0.35\n",
      " 0.33 0.38 0.31 0.35 0.38 0.27 0.42 0.34 0.28 0.36 0.2  0.37 0.24 0.29\n",
      " 0.42 0.26 0.37 0.32 0.37 0.35 0.35 0.41]\n",
      "[0.41 0.32 0.27 0.35 0.42 0.23 0.3  0.31 0.3  0.32 0.34 0.34 0.41 0.32\n",
      " 0.39 0.33 0.31 0.37 0.31 0.31 0.38 0.37 0.33 0.36 0.43 0.28 0.5  0.34\n",
      " 0.43 0.38 0.42 0.46 0.31 0.34 0.38 0.25 0.28 0.46 0.41 0.28 0.29 0.35\n",
      " 0.35 0.33 0.28 0.4  0.51 0.36 0.35 0.4 ]\n",
      "[0.36 0.42 0.33 0.36 0.49 0.29 0.51 0.37 0.46 0.46 0.46 0.38 0.28 0.51\n",
      " 0.35 0.44 0.27 0.42 0.33 0.37 0.46 0.28 0.45 0.42 0.42 0.3  0.52 0.4\n",
      " 0.39 0.44 0.42 0.4  0.44 0.39 0.41 0.48 0.33 0.37 0.38 0.43 0.42 0.38\n",
      " 0.43 0.36 0.43 0.47 0.37 0.38 0.42 0.39]\n",
      "[0.27 0.35 0.31 0.24 0.31 0.33 0.36 0.32 0.35 0.26 0.25 0.26 0.31 0.41\n",
      " 0.47 0.24 0.36 0.26 0.41 0.24 0.25 0.38 0.37 0.27 0.29 0.33 0.39 0.23\n",
      " 0.2  0.37 0.3  0.34 0.3  0.36 0.39 0.25 0.2  0.34 0.26 0.29 0.21 0.31\n",
      " 0.28 0.23 0.29 0.25 0.37 0.32 0.3  0.29]\n",
      "2\n",
      "[0.46]\n",
      "[0.37 0.31 0.44 0.35 0.33 0.45 0.35 0.44 0.34 0.39 0.34 0.26 0.33 0.33\n",
      " 0.28 0.36 0.4  0.34 0.38 0.37 0.33 0.26 0.38 0.31 0.29 0.42 0.44 0.34\n",
      " 0.35 0.33 0.35 0.4  0.37 0.38 0.39 0.36 0.42 0.45 0.29 0.38 0.34 0.34\n",
      " 0.4  0.28 0.4  0.37 0.44 0.43 0.29 0.36]\n",
      "[0.28 0.36 0.27 0.27 0.44 0.34 0.31 0.33 0.31 0.46 0.29 0.42 0.39 0.37\n",
      " 0.42 0.36 0.38 0.41 0.34 0.31 0.35 0.32 0.37 0.36 0.43 0.27 0.44 0.33\n",
      " 0.32 0.37 0.42 0.42 0.27 0.35 0.42 0.34 0.37 0.5  0.44 0.3  0.37 0.34\n",
      " 0.37 0.39 0.27 0.37 0.41 0.43 0.31 0.44]\n",
      "[0.42 0.48 0.41 0.47 0.47 0.31 0.48 0.45 0.52 0.52 0.41 0.43 0.39 0.42\n",
      " 0.4  0.41 0.41 0.47 0.35 0.43 0.59 0.38 0.54 0.48 0.49 0.35 0.5  0.49\n",
      " 0.53 0.42 0.34 0.38 0.48 0.47 0.49 0.48 0.34 0.46 0.57 0.48 0.45 0.47\n",
      " 0.44 0.46 0.47 0.39 0.35 0.38 0.39 0.34]\n",
      "[0.21 0.31 0.39 0.29 0.39 0.33 0.38 0.31 0.23 0.29 0.26 0.26 0.29 0.39\n",
      " 0.4  0.24 0.34 0.28 0.38 0.32 0.23 0.33 0.3  0.3  0.34 0.28 0.45 0.21\n",
      " 0.22 0.29 0.35 0.31 0.41 0.31 0.39 0.26 0.25 0.36 0.33 0.25 0.26 0.33\n",
      " 0.35 0.32 0.34 0.31 0.31 0.32 0.34 0.32]\n",
      "HT-ewc-tsk_False-lr0_00053-lambda10\n",
      "0\n",
      "[0.18366667]\n",
      "[0.34 0.33 0.45 0.36 0.37 0.49 0.34 0.37 0.41 0.36 0.42 0.38 0.45 0.46\n",
      " 0.35 0.35 0.41 0.34 0.33 0.48 0.41 0.32 0.43 0.41 0.3  0.38 0.4  0.45\n",
      " 0.46 0.38 0.46 0.45 0.47 0.43 0.36 0.46 0.48 0.56 0.26 0.4  0.43 0.33\n",
      " 0.43 0.33 0.45 0.31 0.45 0.43 0.39 0.41]\n",
      "[0.41 0.37 0.25 0.4  0.3  0.32 0.23 0.35 0.37 0.41 0.25 0.37 0.39 0.34\n",
      " 0.38 0.33 0.32 0.26 0.39 0.3  0.38 0.33 0.37 0.32 0.34 0.33 0.52 0.32\n",
      " 0.42 0.3  0.38 0.37 0.42 0.39 0.38 0.23 0.35 0.45 0.35 0.22 0.35 0.34\n",
      " 0.36 0.36 0.3  0.32 0.44 0.36 0.37 0.38]\n",
      "[0.57 0.5  0.47 0.45 0.53 0.44 0.42 0.42 0.49 0.51 0.52 0.54 0.47 0.43\n",
      " 0.42 0.41 0.32 0.43 0.45 0.42 0.58 0.34 0.45 0.47 0.49 0.42 0.42 0.32\n",
      " 0.49 0.4  0.49 0.51 0.39 0.37 0.39 0.41 0.34 0.48 0.43 0.53 0.51 0.45\n",
      " 0.49 0.47 0.38 0.36 0.4  0.39 0.44 0.45]\n",
      "[0.24 0.29 0.4  0.28 0.34 0.48 0.37 0.35 0.38 0.28 0.25 0.43 0.36 0.36\n",
      " 0.48 0.32 0.42 0.37 0.4  0.38 0.33 0.37 0.37 0.37 0.36 0.36 0.49 0.39\n",
      " 0.26 0.42 0.33 0.39 0.33 0.42 0.38 0.25 0.25 0.4  0.35 0.35 0.3  0.33\n",
      " 0.38 0.29 0.28 0.25 0.36 0.36 0.3  0.3 ]\n",
      "1\n",
      "[0.18366667]\n",
      "[0.31 0.31 0.38 0.39 0.28 0.42 0.34 0.33 0.3  0.38 0.33 0.25 0.41 0.33\n",
      " 0.3  0.44 0.5  0.31 0.34 0.4  0.36 0.3  0.45 0.31 0.22 0.4  0.37 0.34\n",
      " 0.36 0.34 0.39 0.34 0.43 0.25 0.35 0.38 0.38 0.49 0.21 0.4  0.28 0.32\n",
      " 0.43 0.28 0.35 0.38 0.35 0.27 0.34 0.5 ]\n",
      "[0.36 0.48 0.26 0.28 0.47 0.36 0.27 0.3  0.36 0.37 0.4  0.39 0.44 0.31\n",
      " 0.43 0.28 0.47 0.43 0.41 0.38 0.36 0.4  0.34 0.35 0.4  0.38 0.43 0.42\n",
      " 0.39 0.37 0.42 0.43 0.44 0.36 0.44 0.38 0.33 0.56 0.49 0.37 0.35 0.34\n",
      " 0.42 0.34 0.22 0.42 0.42 0.38 0.46 0.46]\n",
      "[0.4  0.47 0.46 0.33 0.59 0.39 0.55 0.49 0.45 0.52 0.51 0.44 0.47 0.51\n",
      " 0.39 0.55 0.39 0.48 0.38 0.38 0.56 0.3  0.61 0.49 0.47 0.41 0.52 0.5\n",
      " 0.4  0.44 0.49 0.43 0.44 0.38 0.5  0.55 0.37 0.45 0.42 0.55 0.48 0.44\n",
      " 0.49 0.63 0.47 0.53 0.46 0.34 0.46 0.44]\n",
      "[0.27 0.33 0.25 0.24 0.33 0.39 0.41 0.37 0.34 0.3  0.31 0.33 0.36 0.35\n",
      " 0.34 0.33 0.31 0.37 0.38 0.36 0.37 0.35 0.45 0.4  0.24 0.37 0.38 0.3\n",
      " 0.25 0.41 0.29 0.28 0.3  0.23 0.35 0.33 0.22 0.3  0.21 0.28 0.31 0.31\n",
      " 0.3  0.33 0.23 0.29 0.35 0.31 0.33 0.31]\n",
      "2\n",
      "[0.18366667]\n",
      "[0.4  0.41 0.45 0.43 0.37 0.5  0.39 0.4  0.4  0.39 0.41 0.38 0.43 0.32\n",
      " 0.39 0.33 0.45 0.33 0.37 0.46 0.41 0.32 0.46 0.27 0.37 0.43 0.5  0.45\n",
      " 0.49 0.38 0.44 0.46 0.51 0.44 0.45 0.4  0.44 0.51 0.25 0.33 0.37 0.41\n",
      " 0.52 0.32 0.46 0.41 0.46 0.4  0.41 0.39]\n",
      "[0.41 0.32 0.24 0.28 0.33 0.33 0.31 0.4  0.38 0.37 0.28 0.33 0.39 0.34\n",
      " 0.36 0.28 0.35 0.35 0.4  0.33 0.41 0.38 0.3  0.4  0.35 0.32 0.43 0.45\n",
      " 0.44 0.33 0.36 0.47 0.44 0.47 0.39 0.43 0.42 0.58 0.42 0.29 0.41 0.35\n",
      " 0.33 0.35 0.24 0.35 0.39 0.35 0.36 0.34]\n",
      "[0.44 0.49 0.5  0.5  0.56 0.36 0.43 0.36 0.56 0.49 0.47 0.4  0.47 0.54\n",
      " 0.49 0.4  0.49 0.54 0.42 0.47 0.59 0.46 0.54 0.54 0.63 0.4  0.5  0.52\n",
      " 0.55 0.65 0.42 0.44 0.56 0.57 0.35 0.48 0.4  0.6  0.47 0.44 0.55 0.45\n",
      " 0.45 0.48 0.39 0.42 0.44 0.39 0.45 0.47]\n",
      "[0.37 0.37 0.36 0.29 0.33 0.43 0.42 0.31 0.36 0.33 0.37 0.4  0.33 0.36\n",
      " 0.41 0.25 0.27 0.31 0.39 0.37 0.33 0.46 0.33 0.37 0.28 0.34 0.49 0.33\n",
      " 0.31 0.38 0.36 0.38 0.32 0.31 0.31 0.38 0.2  0.33 0.34 0.31 0.29 0.41\n",
      " 0.31 0.37 0.25 0.34 0.43 0.43 0.4  0.34]\n"
     ]
    }
   ],
   "source": [
    "'''collect exps rebuttal per-task Hn'''\n",
    "project_name = 'COBJ'\n",
    "models = [0, 1, 2]\n",
    "modes = ['continual', 'sys', 'pro', 'non', 'noc']\n",
    "\n",
    "'''rebuttal'''\n",
    "# exps = [\n",
    "#     'rebuttal-naive-tsk_False', 'rebuttal-er-tsk_False', 'rebuttal-gem-tsk_False', 'rebuttal-lwf-tsk_False', 'rebuttal-ewc-tsk_False',\n",
    "#     'rebuttal-naive-tsk_True', 'rebuttal-er-tsk_True', 'rebuttal-gem-tsk_True', 'rebuttal-lwf-tsk_True', 'rebuttal-ewc-tsk_True']\n",
    "exps = [\n",
    "    'HT-naive-tsk_True-lr0_001', 'HT-naive-tsk_False-lr0_001',\n",
    "        'HT-er-tsk_True-lr0_01', 'HT-er-tsk_False-lr0_01',\n",
    "        'HT-gem-tsk_True-lr0_001-p16-m0_3', 'HT-gem-tsk_False-lr0_001-p256-m0_00139',\n",
    "        'HT-lwf-tsk_True-lr0_001-a1-t2', 'HT-lwf-tsk_False-lr0_001-a1-t1_52',\n",
    "        'HT-ewc-tsk_True-lr0_01-lambda100', 'HT-ewc-tsk_False-lr0_00053-lambda10',\n",
    "]\n",
    "\n",
    "exps_dis = ['Finetune*', 'Finetune', 'ER*', 'ER', 'GEM*', 'GEM', 'LwF*', 'LwF', 'EWC*', 'EWC']\n",
    "\n",
    "collected_data = []\n",
    "for exp_idx, exp_name in enumerate(exps):\n",
    "    print(exp_name)\n",
    "    for model in models:\n",
    "        print(model)\n",
    "        for mode in modes:\n",
    "            accs = get_accs(exp_name, mode, project_name=project_name, test_n_way=10, model=model, index='-2')\n",
    "            print(accs)\n",
    "            collected_data.append(pd.DataFrame({'Method': exps_dis[exp_idx], 'Phase': mode, 'model': model, 'Accuracy': accs}))\n",
    "\n",
    "collected_data = pd.concat(collected_data, ignore_index=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Method  Phase      mean      ci95            str\n",
      "0    Finetune*  con_0  0.394333       NaN          39.43\n",
      "1    Finetune*  sys_0  0.404600  0.018165  40.46 +- 1.82\n",
      "2    Finetune*  pro_0  0.347800  0.016088  34.78 +- 1.61\n",
      "3    Finetune*  non_0  0.481800  0.025421  48.18 +- 2.54\n",
      "4    Finetune*  noc_0  0.335800  0.016347  33.58 +- 1.63\n",
      "..         ...    ...       ...       ...            ...\n",
      "235        EWC  non_2  0.479600  0.019536  47.96 +- 1.95\n",
      "236        EWC  noc_2  0.349200  0.015406  34.92 +- 1.54\n",
      "237        EWC   Hn_2       NaN       NaN          38.68\n",
      "238        EWC   Hr_2       NaN       NaN          40.41\n",
      "239        EWC   Ha_2       NaN       NaN          39.53\n",
      "\n",
      "[240 rows x 5 columns]\n",
      "\\begin{tabular}{llll}\n",
      "{Phase} & {Hn_0} & {Hn_1} & {Hn_2} \\\\\n",
      "{Method} & {} & {} & {} \\\\\n",
      "Finetune* & 37.41 & 37.07 & 40.93 \\\\\n",
      "Finetune & 37.57 & 38.32 & 37.82 \\\\\n",
      "ER* & 31.11 & 37.30 & 38.66 \\\\\n",
      "ER & 30.27 & 33.33 & 36.99 \\\\\n",
      "GEM* & 37.45 & 35.60 & 40.93 \\\\\n",
      "GEM & 38.05 & 37.60 & 37.36 \\\\\n",
      "LwF* & 37.84 & 44.01 & 44.75 \\\\\n",
      "LwF & 38.13 & 43.34 & 45.19 \\\\\n",
      "EWC* & 30.76 & 34.18 & 36.20 \\\\\n",
      "EWC & 37.34 & 36.86 & 38.68 \\\\\n",
      "\\end{tabular}\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": "Phase       Hn_0   Hn_1   Hn_2\nMethod                        \nFinetune*  37.41  37.07  40.93\nFinetune   37.57  38.32  37.82\nER*        31.11  37.30  38.66\nER         30.27  33.33  36.99\nGEM*       37.45  35.60  40.93\nGEM        38.05  37.60  37.36\nLwF*       37.84  44.01  44.75\nLwF        38.13  43.34  45.19\nEWC*       30.76  34.18  36.20\nEWC        37.34  36.86  38.68",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th>Phase</th>\n      <th>Hn_0</th>\n      <th>Hn_1</th>\n      <th>Hn_2</th>\n    </tr>\n    <tr>\n      <th>Method</th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>Finetune*</th>\n      <td>37.41</td>\n      <td>37.07</td>\n      <td>40.93</td>\n    </tr>\n    <tr>\n      <th>Finetune</th>\n      <td>37.57</td>\n      <td>38.32</td>\n      <td>37.82</td>\n    </tr>\n    <tr>\n      <th>ER*</th>\n      <td>31.11</td>\n      <td>37.30</td>\n      <td>38.66</td>\n    </tr>\n    <tr>\n      <th>ER</th>\n      <td>30.27</td>\n      <td>33.33</td>\n      <td>36.99</td>\n    </tr>\n    <tr>\n      <th>GEM*</th>\n      <td>37.45</td>\n      <td>35.60</td>\n      <td>40.93</td>\n    </tr>\n    <tr>\n      <th>GEM</th>\n      <td>38.05</td>\n      <td>37.60</td>\n      <td>37.36</td>\n    </tr>\n    <tr>\n      <th>LwF*</th>\n      <td>37.84</td>\n      <td>44.01</td>\n      <td>44.75</td>\n    </tr>\n    <tr>\n      <th>LwF</th>\n      <td>38.13</td>\n      <td>43.34</td>\n      <td>45.19</td>\n    </tr>\n    <tr>\n      <th>EWC*</th>\n      <td>30.76</td>\n      <td>34.18</td>\n      <td>36.20</td>\n    </tr>\n    <tr>\n      <th>EWC</th>\n      <td>37.34</td>\n      <td>36.86</td>\n      <td>38.68</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''todo: cal Harmonic mean (among sys, pro, sub) and Harmonic mean (between non, noc)'''\n",
    "data = []\n",
    "for exp in exps_dis:\n",
    "    mean_dict = {}\n",
    "    for model in models:\n",
    "        for mode in modes:\n",
    "            acc_list = collected_data[(collected_data['Method'] == exp) & (collected_data['Phase'] == mode) & (collected_data['model'] == model)]['Accuracy']\n",
    "            mean = acc_list.mean()\n",
    "            mean_dict[mode[:3]] = mean\n",
    "            ci95 = 1.96 * (acc_list.std()/np.sqrt(len(acc_list)))\n",
    "            acc_str = f'{mean*100:.2f} +- {ci95*100:.2f}' if mode not in ['continual', 'forgetting'] else f'{mean*100:.2f}'\n",
    "            Phase = f'{mode[:3]}_{model}'\n",
    "            data.append(pd.DataFrame({'Method': exp, 'Phase': Phase, 'mean': mean, 'ci95': ci95, 'str': acc_str}, index=[0]))\n",
    "\n",
    "        # 3x1*x2*x3/(x1*x2+x1*x3+x2*x3)\n",
    "        hm_nov = 2 * mean_dict['sys'] * mean_dict['pro'] / (mean_dict['sys'] + mean_dict['pro'])\n",
    "        data.append(pd.DataFrame({'Method': exp, 'Phase': f'Hn_{model}', 'str': f'{hm_nov*100:.2f}'}, index=[0]))\n",
    "\n",
    "        # 2x1*x2/(x1+x2)\n",
    "        hm_ref = 2 * mean_dict['non'] * mean_dict['noc'] / (mean_dict['non'] + mean_dict['noc'])\n",
    "        data.append(pd.DataFrame({'Method': exp, 'Phase': f'Hr_{model}', 'str': f'{hm_ref*100:.2f}'}, index=[0]))\n",
    "\n",
    "        hm_all = 4 / (1/mean_dict['sys'] + 1/mean_dict['pro'] + 1/mean_dict['non'] + 1/mean_dict['noc'])\n",
    "        data.append(pd.DataFrame({'Method': exp, 'Phase': f'Ha_{model}', 'str': f'{hm_all*100:.2f}'}, index=[0]))\n",
    "\n",
    "data = pd.concat(data, ignore_index=True)\n",
    "print(data)\n",
    "\n",
    "# to latex\n",
    "data = data.pivot(index='Method', values='str', columns='Phase')\n",
    "# data = data[['continual', 'forgetting', 'sys', 'pro', 'sub', 'Hn', 'non',  'noc', 'Hr', 'Ha']]\n",
    "# sele = []\n",
    "# for model in models:\n",
    "#     sele.extend([f'con_{model}', f'sys_{model}', f'pro_{model}', f'Hn_{model}', f'non_{model}',  f'noc_{model}', f'Hr_{model}', f'Ha_{model}'])\n",
    "sele = [f'Hn_0', f'Hn_1', f'Hn_2']\n",
    "data = data[sele]\n",
    "# data = data[[f'Hn_{model}' for model in models]]\n",
    "data = data.reindex(exps_dis)\n",
    "print(data.style.to_latex().replace('+-', '$\\\\pm$'))\n",
    "data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "naive-cls-lr0_003\n",
      "continual\n",
      "[0.0838]\n",
      "sys\n",
      "[0.65 0.62 0.57 0.5  0.68 0.65 0.73 0.71 0.58 0.66 0.69 0.77 0.59 0.61\n",
      " 0.58 0.68 0.64 0.59 0.51 0.68 0.68 0.5  0.77 0.75 0.75 0.65 0.54 0.7\n",
      " 0.72 0.68 0.67 0.63 0.7  0.68 0.62 0.65 0.69 0.53 0.73 0.62 0.53 0.64\n",
      " 0.56 0.72 0.46 0.64 0.67 0.56 0.68 0.64 0.65 0.58 0.8  0.77 0.53 0.6\n",
      " 0.66 0.69 0.71 0.75 0.63 0.57 0.57 0.66 0.7  0.69 0.55 0.6  0.65 0.59\n",
      " 0.64 0.62 0.7  0.61 0.7  0.71 0.5  0.62 0.82 0.72 0.69 0.54 0.56 0.6\n",
      " 0.58 0.48 0.67 0.54 0.7  0.69 0.69 0.7  0.62 0.59 0.56 0.61 0.67 0.7\n",
      " 0.74 0.61 0.73 0.7  0.56 0.64 0.63 0.54 0.62 0.68 0.67 0.61 0.66 0.61\n",
      " 0.64 0.51 0.66 0.65 0.62 0.6  0.65 0.62 0.6  0.55 0.57 0.73 0.71 0.73\n",
      " 0.71 0.65 0.57 0.64 0.68 0.63 0.6  0.68 0.68 0.66 0.76 0.55 0.58 0.61\n",
      " 0.73 0.7  0.61 0.65 0.65 0.79 0.61 0.75 0.69 0.59 0.63 0.67 0.62 0.62\n",
      " 0.72 0.66 0.77 0.53 0.67 0.68 0.62 0.78 0.75 0.63 0.61 0.62 0.66 0.69\n",
      " 0.61 0.6  0.67 0.61 0.74 0.65 0.61 0.58 0.65 0.74 0.52 0.55 0.61 0.75\n",
      " 0.68 0.53 0.65 0.7  0.7  0.68 0.65 0.56 0.58 0.71 0.63 0.61 0.64 0.65\n",
      " 0.57 0.69 0.76 0.68 0.68 0.62 0.76 0.71 0.65 0.7  0.69 0.58 0.66 0.7\n",
      " 0.64 0.73 0.65 0.73 0.65 0.67 0.68 0.62 0.57 0.64 0.6  0.53 0.59 0.51\n",
      " 0.54 0.7  0.62 0.78 0.62 0.6  0.57 0.67 0.63 0.57 0.64 0.72 0.62 0.58\n",
      " 0.64 0.56 0.69 0.64 0.63 0.62 0.67 0.75 0.72 0.73 0.63 0.71 0.7  0.7\n",
      " 0.74 0.65 0.66 0.58 0.75 0.58 0.76 0.73 0.68 0.56 0.68 0.61 0.7  0.7\n",
      " 0.77 0.64 0.71 0.45 0.61 0.49 0.74 0.75 0.68 0.65 0.64 0.6  0.66 0.59\n",
      " 0.73 0.68 0.66 0.75 0.67 0.65 0.59 0.47 0.64 0.65 0.74 0.7  0.68 0.51\n",
      " 0.66 0.69 0.73 0.69 0.52 0.69]\n",
      "pro\n",
      "[0.66 0.73 0.61 0.77 0.66 0.6  0.66 0.6  0.7  0.73 0.69 0.64 0.6  0.72\n",
      " 0.7  0.73 0.68 0.72 0.68 0.76 0.68 0.57 0.54 0.68 0.71 0.67 0.6  0.54\n",
      " 0.7  0.63 0.6  0.65 0.59 0.69 0.76 0.59 0.67 0.58 0.72 0.66 0.7  0.64\n",
      " 0.72 0.55 0.63 0.57 0.57 0.57 0.6  0.7  0.75 0.67 0.62 0.6  0.68 0.6\n",
      " 0.63 0.54 0.77 0.54 0.68 0.53 0.6  0.76 0.64 0.79 0.56 0.55 0.62 0.62\n",
      " 0.67 0.83 0.55 0.71 0.62 0.66 0.72 0.65 0.69 0.63 0.71 0.71 0.52 0.66\n",
      " 0.55 0.82 0.5  0.69 0.6  0.63 0.71 0.56 0.73 0.67 0.55 0.59 0.65 0.56\n",
      " 0.68 0.71 0.61 0.75 0.75 0.76 0.63 0.48 0.69 0.65 0.65 0.77 0.69 0.69\n",
      " 0.68 0.66 0.73 0.63 0.61 0.72 0.6  0.66 0.68 0.63 0.66 0.77 0.68 0.66\n",
      " 0.64 0.61 0.7  0.62 0.61 0.71 0.72 0.56 0.61 0.66 0.72 0.71 0.64 0.68\n",
      " 0.61 0.63 0.63 0.64 0.66 0.55 0.72 0.76 0.58 0.62 0.63 0.65 0.71 0.66\n",
      " 0.58 0.71 0.65 0.74 0.69 0.71 0.59 0.72 0.67 0.76 0.69 0.6  0.62 0.63\n",
      " 0.65 0.63 0.66 0.5  0.73 0.67 0.56 0.59 0.56 0.56 0.64 0.55 0.68 0.68\n",
      " 0.65 0.64 0.71 0.57 0.57 0.66 0.63 0.79 0.71 0.67 0.76 0.67 0.57 0.65\n",
      " 0.71 0.75 0.64 0.6  0.66 0.62 0.58 0.57 0.63 0.65 0.66 0.62 0.61 0.52\n",
      " 0.68 0.7  0.71 0.5  0.56 0.77 0.56 0.6  0.65 0.71 0.64 0.76 0.57 0.69\n",
      " 0.66 0.73 0.68 0.66 0.73 0.66 0.57 0.56 0.7  0.69 0.66 0.71 0.69 0.64\n",
      " 0.74 0.64 0.61 0.73 0.74 0.6  0.76 0.67 0.53 0.63 0.71 0.64 0.58 0.62\n",
      " 0.61 0.57 0.74 0.69 0.64 0.76 0.69 0.75 0.73 0.73 0.63 0.65 0.55 0.66\n",
      " 0.72 0.71 0.65 0.67 0.66 0.61 0.75 0.6  0.66 0.66 0.66 0.68 0.67 0.58\n",
      " 0.77 0.71 0.65 0.63 0.66 0.59 0.66 0.64 0.7  0.66 0.64 0.6  0.58 0.76\n",
      " 0.61 0.69 0.68 0.67 0.63 0.61]\n",
      "sub\n",
      "[0.59 0.64 0.73 0.66 0.63 0.58 0.55 0.66 0.57 0.55 0.42 0.64 0.65 0.63\n",
      " 0.63 0.64 0.59 0.69 0.61 0.57 0.63 0.49 0.61 0.52 0.51 0.62 0.6  0.62\n",
      " 0.61 0.61 0.62 0.55 0.62 0.57 0.68 0.7  0.52 0.6  0.59 0.62 0.69 0.51\n",
      " 0.54 0.71 0.67 0.6  0.5  0.54 0.59 0.58 0.6  0.57 0.62 0.45 0.59 0.68\n",
      " 0.62 0.7  0.55 0.6  0.54 0.51 0.74 0.63 0.67 0.64 0.67 0.65 0.59 0.61\n",
      " 0.61 0.59 0.56 0.61 0.65 0.62 0.63 0.64 0.68 0.55 0.53 0.58 0.61 0.46\n",
      " 0.74 0.57 0.67 0.57 0.56 0.57 0.66 0.66 0.58 0.6  0.64 0.64 0.68 0.68\n",
      " 0.67 0.6  0.63 0.6  0.56 0.61 0.6  0.67 0.65 0.53 0.61 0.73 0.64 0.55\n",
      " 0.51 0.61 0.66 0.64 0.57 0.67 0.57 0.59 0.67 0.69 0.65 0.6  0.65 0.49\n",
      " 0.56 0.58 0.7  0.62 0.66 0.6  0.61 0.56 0.64 0.59 0.53 0.53 0.64 0.69\n",
      " 0.57 0.56 0.55 0.7  0.71 0.59 0.51 0.62 0.68 0.62 0.65 0.6  0.5  0.58\n",
      " 0.59 0.56 0.65 0.54 0.71 0.65 0.54 0.59 0.72 0.65 0.57 0.58 0.55 0.7\n",
      " 0.57 0.71 0.6  0.68 0.57 0.65 0.61 0.49 0.57 0.58 0.59 0.65 0.64 0.64\n",
      " 0.63 0.53 0.5  0.53 0.61 0.53 0.6  0.65 0.65 0.53 0.63 0.56 0.48 0.64\n",
      " 0.66 0.57 0.62 0.61 0.68 0.6  0.51 0.68 0.63 0.58 0.66 0.61 0.63 0.66\n",
      " 0.56 0.7  0.59 0.59 0.57 0.55 0.61 0.57 0.62 0.61 0.64 0.6  0.65 0.67\n",
      " 0.53 0.56 0.53 0.55 0.6  0.6  0.5  0.62 0.61 0.65 0.62 0.54 0.74 0.7\n",
      " 0.64 0.73 0.53 0.59 0.67 0.6  0.67 0.55 0.62 0.53 0.73 0.62 0.6  0.7\n",
      " 0.67 0.64 0.66 0.62 0.64 0.66 0.66 0.61 0.63 0.54 0.62 0.63 0.6  0.65\n",
      " 0.66 0.63 0.59 0.65 0.57 0.59 0.67 0.54 0.58 0.63 0.53 0.65 0.73 0.61\n",
      " 0.63 0.67 0.69 0.68 0.73 0.65 0.63 0.6  0.77 0.71 0.62 0.64 0.54 0.63\n",
      " 0.59 0.65 0.75 0.69 0.63 0.64]\n",
      "non\n",
      "[0.54 0.74 0.82 0.68 0.61 0.68 0.64 0.69 0.57 0.59 0.54 0.51 0.74 0.76\n",
      " 0.74 0.65 0.66 0.83 0.66 0.69 0.56 0.68 0.81 0.7  0.66 0.7  0.76 0.61\n",
      " 0.67 0.69 0.6  0.67 0.64 0.84 0.64 0.75 0.63 0.67 0.65 0.64 0.74 0.46\n",
      " 0.66 0.72 0.72 0.61 0.55 0.71 0.69 0.73 0.61 0.56 0.62 0.62 0.73 0.66\n",
      " 0.67 0.58 0.63 0.63 0.66 0.52 0.62 0.78 0.71 0.69 0.62 0.66 0.69 0.68\n",
      " 0.69 0.68 0.72 0.73 0.73 0.66 0.77 0.75 0.67 0.76 0.63 0.66 0.57 0.63\n",
      " 0.64 0.72 0.72 0.71 0.69 0.8  0.8  0.79 0.69 0.7  0.67 0.71 0.71 0.67\n",
      " 0.82 0.71 0.57 0.68 0.72 0.73 0.7  0.64 0.55 0.65 0.69 0.78 0.65 0.72\n",
      " 0.59 0.57 0.65 0.72 0.55 0.64 0.64 0.64 0.83 0.69 0.77 0.64 0.78 0.61\n",
      " 0.7  0.65 0.73 0.7  0.74 0.54 0.75 0.66 0.73 0.62 0.61 0.63 0.64 0.73\n",
      " 0.72 0.67 0.72 0.75 0.73 0.75 0.56 0.66 0.82 0.79 0.67 0.61 0.58 0.71\n",
      " 0.61 0.66 0.71 0.6  0.71 0.7  0.67 0.68 0.65 0.73 0.63 0.68 0.66 0.82\n",
      " 0.66 0.66 0.77 0.79 0.66 0.73 0.69 0.47 0.69 0.76 0.73 0.58 0.71 0.68\n",
      " 0.57 0.61 0.8  0.64 0.67 0.68 0.71 0.75 0.62 0.6  0.8  0.57 0.67 0.67\n",
      " 0.77 0.59 0.74 0.68 0.69 0.75 0.67 0.82 0.68 0.7  0.68 0.56 0.67 0.73\n",
      " 0.55 0.77 0.7  0.78 0.63 0.68 0.72 0.65 0.81 0.64 0.87 0.58 0.68 0.7\n",
      " 0.57 0.75 0.67 0.7  0.66 0.7  0.55 0.71 0.71 0.67 0.66 0.79 0.72 0.68\n",
      " 0.71 0.66 0.69 0.68 0.7  0.65 0.72 0.66 0.74 0.74 0.75 0.69 0.66 0.77\n",
      " 0.81 0.6  0.71 0.61 0.65 0.78 0.79 0.8  0.72 0.57 0.71 0.68 0.68 0.67\n",
      " 0.73 0.74 0.74 0.66 0.71 0.57 0.74 0.63 0.6  0.73 0.68 0.66 0.85 0.81\n",
      " 0.74 0.7  0.77 0.81 0.71 0.68 0.69 0.67 0.75 0.81 0.71 0.79 0.66 0.72\n",
      " 0.66 0.67 0.85 0.72 0.64 0.73]\n",
      "noc\n",
      "[0.52 0.39 0.44 0.42 0.35 0.31 0.44 0.26 0.48 0.36 0.53 0.5  0.36 0.36\n",
      " 0.49 0.44 0.4  0.37 0.36 0.41 0.32 0.41 0.42 0.33 0.47 0.37 0.37 0.36\n",
      " 0.39 0.41 0.37 0.37 0.29 0.39 0.38 0.34 0.39 0.44 0.42 0.43 0.35 0.4\n",
      " 0.37 0.44 0.57 0.39 0.39 0.54 0.22 0.51 0.41 0.39 0.39 0.34 0.41 0.41\n",
      " 0.43 0.32 0.43 0.33 0.5  0.3  0.56 0.36 0.29 0.48 0.47 0.41 0.44 0.44\n",
      " 0.38 0.34 0.39 0.35 0.36 0.45 0.38 0.43 0.39 0.41 0.44 0.4  0.45 0.44\n",
      " 0.38 0.53 0.45 0.43 0.44 0.37 0.42 0.3  0.42 0.41 0.45 0.39 0.42 0.4\n",
      " 0.43 0.4  0.42 0.43 0.37 0.4  0.32 0.44 0.43 0.44 0.39 0.44 0.43 0.5\n",
      " 0.34 0.34 0.38 0.28 0.32 0.49 0.52 0.42 0.46 0.46 0.43 0.47 0.41 0.4\n",
      " 0.31 0.35 0.38 0.48 0.4  0.46 0.36 0.38 0.33 0.34 0.37 0.45 0.37 0.5\n",
      " 0.44 0.38 0.39 0.37 0.44 0.42 0.37 0.47 0.29 0.49 0.44 0.48 0.36 0.36\n",
      " 0.39 0.36 0.44 0.43 0.48 0.43 0.33 0.49 0.32 0.44 0.34 0.42 0.43 0.4\n",
      " 0.34 0.43 0.27 0.57 0.4  0.45 0.48 0.4  0.36 0.4  0.4  0.43 0.35 0.39\n",
      " 0.44 0.42 0.34 0.36 0.45 0.42 0.43 0.46 0.46 0.48 0.38 0.34 0.44 0.41\n",
      " 0.3  0.45 0.41 0.54 0.54 0.29 0.44 0.42 0.39 0.47 0.42 0.4  0.35 0.29\n",
      " 0.47 0.37 0.39 0.42 0.51 0.4  0.42 0.4  0.39 0.33 0.47 0.31 0.36 0.29\n",
      " 0.31 0.42 0.43 0.38 0.37 0.44 0.31 0.38 0.37 0.37 0.35 0.34 0.43 0.5\n",
      " 0.45 0.42 0.36 0.39 0.42 0.33 0.39 0.3  0.38 0.43 0.41 0.37 0.47 0.57\n",
      " 0.34 0.45 0.35 0.54 0.42 0.39 0.29 0.44 0.27 0.47 0.33 0.39 0.34 0.42\n",
      " 0.36 0.44 0.37 0.37 0.37 0.25 0.43 0.42 0.49 0.33 0.6  0.38 0.32 0.47\n",
      " 0.21 0.47 0.38 0.47 0.46 0.31 0.33 0.4  0.48 0.38 0.46 0.47 0.27 0.46\n",
      " 0.41 0.43 0.42 0.39 0.45 0.42]\n",
      "er-cls-lr0_003\n",
      "continual\n",
      "[0.1978]\n",
      "sys\n",
      "[0.7  0.74 0.64 0.62 0.8  0.82 0.77 0.66 0.69 0.68 0.69 0.8  0.73 0.72\n",
      " 0.73 0.78 0.71 0.64 0.62 0.68 0.84 0.67 0.8  0.74 0.8  0.74 0.66 0.79\n",
      " 0.78 0.74 0.7  0.65 0.74 0.76 0.69 0.71 0.7  0.59 0.8  0.72 0.66 0.66\n",
      " 0.66 0.74 0.52 0.74 0.69 0.56 0.79 0.7  0.65 0.69 0.79 0.73 0.59 0.78\n",
      " 0.76 0.76 0.77 0.75 0.7  0.76 0.76 0.71 0.77 0.75 0.63 0.71 0.65 0.69\n",
      " 0.7  0.69 0.75 0.68 0.71 0.71 0.64 0.68 0.85 0.67 0.77 0.6  0.69 0.67\n",
      " 0.7  0.62 0.68 0.67 0.76 0.7  0.75 0.71 0.79 0.71 0.6  0.64 0.78 0.78\n",
      " 0.78 0.62 0.73 0.76 0.58 0.71 0.74 0.67 0.75 0.71 0.78 0.67 0.76 0.68\n",
      " 0.79 0.63 0.71 0.7  0.75 0.59 0.71 0.66 0.74 0.68 0.62 0.75 0.69 0.83\n",
      " 0.8  0.73 0.52 0.57 0.72 0.71 0.67 0.84 0.81 0.63 0.82 0.66 0.64 0.71\n",
      " 0.72 0.71 0.67 0.71 0.64 0.76 0.74 0.81 0.76 0.65 0.71 0.75 0.73 0.63\n",
      " 0.82 0.75 0.79 0.52 0.67 0.78 0.75 0.85 0.81 0.73 0.71 0.77 0.7  0.73\n",
      " 0.74 0.71 0.71 0.67 0.8  0.75 0.86 0.67 0.7  0.75 0.73 0.78 0.63 0.81\n",
      " 0.69 0.65 0.69 0.69 0.71 0.73 0.67 0.63 0.56 0.83 0.66 0.68 0.72 0.7\n",
      " 0.7  0.79 0.81 0.77 0.76 0.72 0.81 0.7  0.75 0.79 0.69 0.7  0.72 0.74\n",
      " 0.69 0.71 0.73 0.77 0.64 0.7  0.7  0.65 0.68 0.6  0.8  0.62 0.62 0.6\n",
      " 0.55 0.7  0.72 0.82 0.79 0.58 0.66 0.71 0.73 0.65 0.6  0.77 0.67 0.77\n",
      " 0.71 0.76 0.8  0.72 0.7  0.62 0.72 0.87 0.77 0.75 0.69 0.79 0.7  0.83\n",
      " 0.71 0.72 0.76 0.67 0.75 0.62 0.69 0.77 0.66 0.7  0.76 0.76 0.76 0.78\n",
      " 0.78 0.72 0.69 0.53 0.7  0.6  0.79 0.64 0.8  0.7  0.75 0.65 0.72 0.67\n",
      " 0.76 0.79 0.8  0.82 0.71 0.58 0.69 0.66 0.61 0.76 0.78 0.76 0.78 0.67\n",
      " 0.74 0.75 0.71 0.72 0.77 0.75]\n",
      "pro\n",
      "[0.68 0.71 0.55 0.8  0.68 0.7  0.82 0.71 0.69 0.76 0.64 0.78 0.71 0.74\n",
      " 0.66 0.76 0.71 0.71 0.64 0.74 0.68 0.61 0.67 0.77 0.66 0.7  0.78 0.72\n",
      " 0.8  0.72 0.6  0.73 0.67 0.72 0.78 0.65 0.72 0.65 0.74 0.77 0.72 0.75\n",
      " 0.74 0.7  0.71 0.62 0.65 0.54 0.71 0.68 0.72 0.63 0.65 0.76 0.68 0.66\n",
      " 0.68 0.61 0.75 0.67 0.67 0.7  0.71 0.75 0.64 0.75 0.65 0.64 0.66 0.74\n",
      " 0.74 0.76 0.65 0.71 0.69 0.71 0.75 0.66 0.7  0.69 0.65 0.68 0.66 0.58\n",
      " 0.6  0.77 0.61 0.73 0.7  0.69 0.8  0.66 0.73 0.65 0.6  0.57 0.73 0.61\n",
      " 0.82 0.74 0.63 0.64 0.74 0.74 0.66 0.63 0.7  0.76 0.68 0.75 0.71 0.68\n",
      " 0.65 0.68 0.7  0.74 0.73 0.73 0.69 0.72 0.71 0.72 0.67 0.74 0.65 0.67\n",
      " 0.68 0.71 0.79 0.66 0.66 0.67 0.73 0.65 0.65 0.71 0.73 0.74 0.76 0.65\n",
      " 0.59 0.78 0.78 0.78 0.69 0.66 0.71 0.73 0.66 0.71 0.63 0.7  0.75 0.75\n",
      " 0.75 0.67 0.7  0.68 0.77 0.7  0.69 0.67 0.68 0.71 0.75 0.71 0.73 0.75\n",
      " 0.72 0.57 0.65 0.64 0.77 0.72 0.73 0.73 0.73 0.62 0.64 0.56 0.77 0.66\n",
      " 0.74 0.73 0.69 0.65 0.63 0.78 0.73 0.7  0.75 0.64 0.76 0.65 0.68 0.7\n",
      " 0.74 0.82 0.7  0.67 0.69 0.63 0.67 0.7  0.69 0.59 0.73 0.67 0.65 0.59\n",
      " 0.66 0.78 0.8  0.67 0.61 0.78 0.7  0.62 0.68 0.74 0.8  0.75 0.71 0.8\n",
      " 0.67 0.7  0.75 0.71 0.82 0.68 0.68 0.72 0.71 0.74 0.7  0.79 0.69 0.74\n",
      " 0.62 0.72 0.69 0.72 0.74 0.65 0.78 0.62 0.68 0.61 0.71 0.75 0.69 0.73\n",
      " 0.76 0.7  0.68 0.76 0.71 0.83 0.71 0.72 0.68 0.68 0.81 0.78 0.65 0.67\n",
      " 0.64 0.75 0.7  0.64 0.75 0.73 0.74 0.69 0.77 0.82 0.68 0.71 0.65 0.58\n",
      " 0.77 0.67 0.77 0.65 0.76 0.67 0.68 0.72 0.66 0.82 0.71 0.66 0.66 0.86\n",
      " 0.65 0.78 0.7  0.74 0.75 0.56]\n",
      "sub\n",
      "[0.52 0.62 0.81 0.7  0.62 0.63 0.62 0.63 0.76 0.59 0.48 0.64 0.72 0.64\n",
      " 0.66 0.64 0.65 0.56 0.62 0.6  0.59 0.54 0.6  0.6  0.57 0.71 0.69 0.58\n",
      " 0.59 0.72 0.61 0.6  0.67 0.57 0.72 0.72 0.54 0.63 0.66 0.7  0.7  0.59\n",
      " 0.73 0.67 0.59 0.6  0.66 0.6  0.56 0.67 0.69 0.54 0.63 0.52 0.67 0.62\n",
      " 0.56 0.68 0.63 0.62 0.62 0.45 0.74 0.71 0.65 0.7  0.72 0.63 0.53 0.62\n",
      " 0.61 0.57 0.58 0.67 0.69 0.57 0.65 0.69 0.66 0.68 0.56 0.65 0.7  0.58\n",
      " 0.66 0.64 0.65 0.67 0.56 0.61 0.6  0.7  0.6  0.55 0.61 0.6  0.51 0.69\n",
      " 0.57 0.64 0.61 0.7  0.62 0.67 0.7  0.64 0.63 0.62 0.61 0.78 0.72 0.61\n",
      " 0.64 0.61 0.65 0.66 0.57 0.66 0.66 0.66 0.65 0.62 0.69 0.54 0.68 0.62\n",
      " 0.57 0.65 0.68 0.63 0.64 0.56 0.66 0.61 0.7  0.63 0.72 0.59 0.68 0.84\n",
      " 0.59 0.58 0.67 0.78 0.66 0.64 0.63 0.61 0.72 0.69 0.63 0.63 0.62 0.61\n",
      " 0.64 0.61 0.59 0.55 0.76 0.65 0.56 0.67 0.75 0.71 0.59 0.65 0.56 0.72\n",
      " 0.63 0.67 0.71 0.69 0.67 0.74 0.7  0.52 0.69 0.57 0.69 0.58 0.6  0.72\n",
      " 0.61 0.6  0.63 0.52 0.68 0.61 0.64 0.74 0.63 0.63 0.68 0.57 0.57 0.68\n",
      " 0.65 0.63 0.68 0.63 0.65 0.62 0.71 0.74 0.7  0.59 0.68 0.63 0.73 0.6\n",
      " 0.67 0.66 0.56 0.65 0.68 0.66 0.62 0.5  0.7  0.66 0.75 0.7  0.68 0.64\n",
      " 0.57 0.63 0.64 0.57 0.61 0.73 0.61 0.66 0.68 0.69 0.75 0.58 0.69 0.72\n",
      " 0.65 0.68 0.63 0.6  0.64 0.75 0.65 0.61 0.69 0.66 0.81 0.67 0.58 0.69\n",
      " 0.71 0.71 0.56 0.65 0.71 0.65 0.58 0.64 0.65 0.67 0.67 0.58 0.59 0.69\n",
      " 0.62 0.66 0.69 0.54 0.62 0.53 0.65 0.56 0.6  0.66 0.56 0.65 0.7  0.6\n",
      " 0.63 0.65 0.73 0.59 0.69 0.65 0.68 0.67 0.71 0.66 0.77 0.76 0.55 0.58\n",
      " 0.6  0.71 0.77 0.7  0.68 0.72]\n",
      "non\n",
      "[0.77 0.81 0.88 0.88 0.73 0.87 0.68 0.68 0.68 0.74 0.66 0.73 0.83 0.78\n",
      " 0.79 0.69 0.75 0.83 0.73 0.76 0.65 0.71 0.86 0.88 0.7  0.76 0.87 0.83\n",
      " 0.77 0.79 0.76 0.73 0.7  0.84 0.75 0.78 0.72 0.73 0.78 0.77 0.82 0.6\n",
      " 0.77 0.81 0.84 0.77 0.63 0.74 0.79 0.76 0.81 0.67 0.79 0.75 0.78 0.8\n",
      " 0.74 0.78 0.75 0.75 0.76 0.7  0.8  0.89 0.79 0.81 0.71 0.74 0.82 0.86\n",
      " 0.73 0.76 0.77 0.84 0.8  0.75 0.75 0.79 0.81 0.73 0.72 0.81 0.77 0.62\n",
      " 0.74 0.76 0.73 0.74 0.7  0.79 0.9  0.83 0.76 0.77 0.69 0.81 0.81 0.62\n",
      " 0.89 0.76 0.73 0.78 0.76 0.79 0.78 0.76 0.8  0.75 0.8  0.86 0.85 0.75\n",
      " 0.78 0.8  0.64 0.78 0.74 0.72 0.74 0.69 0.85 0.74 0.87 0.77 0.82 0.8\n",
      " 0.79 0.77 0.81 0.8  0.79 0.61 0.79 0.77 0.72 0.76 0.81 0.78 0.81 0.81\n",
      " 0.84 0.76 0.84 0.82 0.78 0.77 0.72 0.78 0.84 0.76 0.77 0.75 0.7  0.73\n",
      " 0.73 0.69 0.79 0.62 0.73 0.78 0.69 0.73 0.77 0.72 0.68 0.73 0.69 0.83\n",
      " 0.76 0.79 0.83 0.87 0.69 0.85 0.69 0.64 0.73 0.85 0.89 0.83 0.8  0.78\n",
      " 0.78 0.79 0.79 0.84 0.82 0.81 0.78 0.83 0.83 0.64 0.77 0.75 0.73 0.75\n",
      " 0.83 0.77 0.89 0.77 0.76 0.83 0.77 0.89 0.77 0.78 0.8  0.64 0.82 0.74\n",
      " 0.72 0.84 0.8  0.73 0.79 0.72 0.76 0.76 0.88 0.77 0.89 0.74 0.82 0.67\n",
      " 0.67 0.85 0.86 0.83 0.79 0.65 0.65 0.81 0.72 0.76 0.72 0.8  0.71 0.78\n",
      " 0.89 0.75 0.73 0.82 0.69 0.75 0.78 0.78 0.85 0.77 0.83 0.82 0.75 0.83\n",
      " 0.82 0.74 0.73 0.72 0.8  0.78 0.8  0.86 0.81 0.82 0.82 0.76 0.73 0.8\n",
      " 0.81 0.87 0.83 0.71 0.8  0.65 0.77 0.69 0.75 0.77 0.68 0.72 0.86 0.74\n",
      " 0.8  0.81 0.79 0.82 0.8  0.73 0.78 0.83 0.84 0.83 0.84 0.85 0.72 0.76\n",
      " 0.8  0.75 0.85 0.82 0.81 0.8 ]\n",
      "noc\n",
      "[0.44 0.4  0.45 0.6  0.39 0.37 0.42 0.28 0.39 0.38 0.59 0.42 0.31 0.35\n",
      " 0.46 0.48 0.34 0.36 0.43 0.37 0.41 0.36 0.4  0.35 0.44 0.49 0.36 0.44\n",
      " 0.39 0.35 0.34 0.36 0.41 0.38 0.39 0.34 0.42 0.48 0.39 0.44 0.4  0.38\n",
      " 0.36 0.44 0.47 0.32 0.4  0.53 0.28 0.44 0.3  0.46 0.41 0.34 0.46 0.48\n",
      " 0.44 0.33 0.37 0.35 0.44 0.33 0.42 0.35 0.4  0.49 0.44 0.55 0.49 0.46\n",
      " 0.35 0.36 0.39 0.41 0.27 0.32 0.3  0.46 0.41 0.41 0.43 0.4  0.44 0.44\n",
      " 0.27 0.5  0.46 0.52 0.45 0.49 0.46 0.27 0.44 0.4  0.53 0.37 0.47 0.43\n",
      " 0.43 0.45 0.46 0.33 0.34 0.47 0.37 0.49 0.39 0.4  0.41 0.43 0.47 0.49\n",
      " 0.33 0.37 0.28 0.37 0.38 0.44 0.54 0.38 0.5  0.42 0.39 0.35 0.44 0.38\n",
      " 0.33 0.37 0.37 0.48 0.43 0.5  0.43 0.35 0.39 0.32 0.34 0.46 0.39 0.49\n",
      " 0.41 0.35 0.44 0.36 0.46 0.41 0.35 0.5  0.32 0.46 0.39 0.46 0.32 0.45\n",
      " 0.44 0.45 0.48 0.38 0.53 0.41 0.45 0.46 0.31 0.42 0.42 0.44 0.41 0.35\n",
      " 0.35 0.48 0.26 0.46 0.37 0.58 0.49 0.47 0.31 0.4  0.39 0.37 0.36 0.5\n",
      " 0.44 0.34 0.38 0.35 0.45 0.45 0.45 0.38 0.51 0.47 0.41 0.47 0.4  0.5\n",
      " 0.38 0.4  0.41 0.47 0.38 0.4  0.48 0.43 0.32 0.4  0.45 0.36 0.36 0.38\n",
      " 0.5  0.43 0.44 0.42 0.45 0.25 0.38 0.4  0.41 0.39 0.48 0.33 0.4  0.4\n",
      " 0.33 0.43 0.44 0.32 0.47 0.43 0.36 0.37 0.41 0.36 0.37 0.3  0.44 0.44\n",
      " 0.38 0.41 0.5  0.4  0.52 0.41 0.46 0.37 0.47 0.41 0.36 0.42 0.41 0.51\n",
      " 0.33 0.52 0.35 0.49 0.47 0.47 0.25 0.44 0.34 0.41 0.37 0.4  0.22 0.5\n",
      " 0.36 0.37 0.39 0.44 0.4  0.37 0.45 0.36 0.49 0.39 0.52 0.47 0.36 0.58\n",
      " 0.31 0.37 0.4  0.42 0.45 0.4  0.29 0.49 0.51 0.35 0.41 0.4  0.36 0.39\n",
      " 0.42 0.45 0.43 0.39 0.45 0.42]\n",
      "gem-cls-lr0_01-p32-m0_3\n",
      "continual\n",
      "[0.0856]\n",
      "sys\n",
      "[0.66 0.71 0.62 0.6  0.72 0.66 0.71 0.71 0.57 0.69 0.68 0.8  0.67 0.72\n",
      " 0.59 0.75 0.66 0.65 0.56 0.69 0.71 0.63 0.76 0.7  0.78 0.68 0.58 0.72\n",
      " 0.71 0.7  0.6  0.66 0.6  0.76 0.64 0.75 0.59 0.6  0.7  0.54 0.61 0.69\n",
      " 0.66 0.7  0.45 0.79 0.77 0.53 0.67 0.66 0.56 0.68 0.78 0.68 0.53 0.66\n",
      " 0.67 0.72 0.72 0.73 0.65 0.67 0.59 0.65 0.64 0.67 0.6  0.59 0.63 0.56\n",
      " 0.66 0.69 0.79 0.6  0.66 0.69 0.58 0.63 0.82 0.73 0.69 0.57 0.58 0.59\n",
      " 0.62 0.51 0.66 0.56 0.73 0.71 0.68 0.75 0.64 0.72 0.56 0.59 0.67 0.79\n",
      " 0.68 0.64 0.73 0.77 0.58 0.61 0.69 0.61 0.68 0.67 0.68 0.7  0.68 0.63\n",
      " 0.69 0.63 0.71 0.69 0.68 0.58 0.66 0.61 0.58 0.65 0.64 0.77 0.59 0.71\n",
      " 0.71 0.74 0.5  0.69 0.62 0.68 0.64 0.77 0.7  0.64 0.74 0.63 0.57 0.66\n",
      " 0.72 0.71 0.59 0.71 0.61 0.71 0.62 0.75 0.74 0.58 0.62 0.65 0.71 0.65\n",
      " 0.78 0.6  0.8  0.48 0.57 0.69 0.69 0.84 0.8  0.65 0.65 0.61 0.69 0.62\n",
      " 0.61 0.61 0.69 0.7  0.77 0.74 0.68 0.61 0.66 0.71 0.63 0.63 0.61 0.8\n",
      " 0.72 0.6  0.65 0.65 0.71 0.66 0.66 0.58 0.61 0.72 0.66 0.59 0.75 0.6\n",
      " 0.6  0.71 0.7  0.77 0.67 0.65 0.71 0.75 0.72 0.66 0.66 0.58 0.76 0.69\n",
      " 0.65 0.7  0.68 0.73 0.59 0.69 0.62 0.65 0.65 0.62 0.71 0.55 0.63 0.55\n",
      " 0.61 0.68 0.58 0.78 0.67 0.52 0.59 0.73 0.61 0.68 0.71 0.8  0.63 0.67\n",
      " 0.53 0.71 0.7  0.75 0.62 0.59 0.63 0.76 0.75 0.71 0.6  0.75 0.68 0.79\n",
      " 0.73 0.61 0.77 0.64 0.74 0.53 0.73 0.8  0.66 0.58 0.71 0.74 0.73 0.78\n",
      " 0.73 0.71 0.64 0.55 0.7  0.59 0.76 0.77 0.67 0.6  0.69 0.63 0.66 0.56\n",
      " 0.73 0.7  0.67 0.68 0.65 0.66 0.59 0.62 0.63 0.63 0.71 0.62 0.68 0.59\n",
      " 0.68 0.67 0.69 0.65 0.62 0.64]\n",
      "pro\n",
      "[0.68 0.76 0.73 0.75 0.66 0.68 0.85 0.77 0.69 0.72 0.75 0.75 0.78 0.76\n",
      " 0.75 0.86 0.72 0.79 0.65 0.76 0.66 0.6  0.73 0.78 0.72 0.74 0.67 0.75\n",
      " 0.77 0.72 0.67 0.76 0.75 0.73 0.74 0.72 0.74 0.77 0.77 0.74 0.75 0.76\n",
      " 0.77 0.76 0.75 0.63 0.73 0.69 0.69 0.76 0.82 0.72 0.82 0.71 0.73 0.69\n",
      " 0.72 0.68 0.7  0.63 0.69 0.69 0.71 0.75 0.72 0.73 0.58 0.64 0.67 0.69\n",
      " 0.79 0.84 0.68 0.79 0.73 0.79 0.68 0.76 0.76 0.74 0.74 0.65 0.69 0.72\n",
      " 0.65 0.78 0.63 0.6  0.67 0.73 0.74 0.72 0.79 0.72 0.62 0.69 0.72 0.68\n",
      " 0.85 0.83 0.77 0.8  0.76 0.77 0.76 0.63 0.78 0.78 0.76 0.77 0.72 0.75\n",
      " 0.74 0.71 0.77 0.69 0.65 0.72 0.68 0.8  0.7  0.75 0.7  0.88 0.76 0.68\n",
      " 0.81 0.82 0.7  0.67 0.59 0.71 0.77 0.69 0.68 0.72 0.78 0.7  0.74 0.69\n",
      " 0.73 0.68 0.72 0.76 0.81 0.67 0.76 0.78 0.61 0.75 0.67 0.7  0.77 0.73\n",
      " 0.71 0.78 0.83 0.79 0.8  0.82 0.76 0.71 0.79 0.8  0.77 0.78 0.77 0.79\n",
      " 0.73 0.71 0.73 0.68 0.74 0.69 0.67 0.85 0.78 0.66 0.72 0.74 0.77 0.72\n",
      " 0.74 0.77 0.75 0.67 0.71 0.8  0.77 0.82 0.77 0.72 0.71 0.73 0.7  0.73\n",
      " 0.67 0.83 0.72 0.66 0.77 0.73 0.74 0.68 0.81 0.69 0.83 0.74 0.69 0.69\n",
      " 0.67 0.76 0.83 0.72 0.64 0.81 0.57 0.71 0.64 0.76 0.75 0.81 0.76 0.83\n",
      " 0.77 0.76 0.76 0.75 0.78 0.8  0.69 0.69 0.73 0.77 0.77 0.72 0.76 0.74\n",
      " 0.79 0.81 0.77 0.72 0.82 0.68 0.82 0.68 0.68 0.67 0.78 0.79 0.66 0.72\n",
      " 0.77 0.7  0.72 0.77 0.75 0.78 0.79 0.77 0.82 0.74 0.8  0.67 0.71 0.73\n",
      " 0.8  0.72 0.72 0.72 0.76 0.67 0.77 0.66 0.75 0.77 0.69 0.79 0.74 0.64\n",
      " 0.82 0.72 0.76 0.75 0.7  0.78 0.71 0.68 0.69 0.77 0.81 0.66 0.71 0.84\n",
      " 0.71 0.81 0.69 0.74 0.75 0.69]\n",
      "sub\n",
      "[0.51 0.63 0.69 0.61 0.65 0.54 0.54 0.65 0.63 0.57 0.42 0.6  0.76 0.77\n",
      " 0.62 0.61 0.69 0.62 0.63 0.59 0.44 0.55 0.59 0.61 0.65 0.69 0.65 0.66\n",
      " 0.65 0.61 0.51 0.54 0.69 0.61 0.68 0.75 0.64 0.61 0.69 0.66 0.73 0.5\n",
      " 0.65 0.67 0.67 0.55 0.5  0.56 0.59 0.6  0.62 0.55 0.64 0.46 0.56 0.65\n",
      " 0.63 0.68 0.64 0.64 0.53 0.48 0.71 0.72 0.64 0.65 0.78 0.68 0.61 0.57\n",
      " 0.6  0.57 0.63 0.61 0.72 0.6  0.67 0.61 0.63 0.52 0.52 0.6  0.66 0.63\n",
      " 0.73 0.6  0.73 0.67 0.51 0.65 0.63 0.64 0.63 0.59 0.6  0.64 0.59 0.63\n",
      " 0.62 0.65 0.65 0.72 0.56 0.63 0.68 0.57 0.6  0.59 0.63 0.74 0.64 0.67\n",
      " 0.47 0.56 0.57 0.61 0.55 0.55 0.58 0.53 0.7  0.61 0.67 0.59 0.61 0.56\n",
      " 0.49 0.55 0.66 0.68 0.67 0.44 0.68 0.7  0.66 0.66 0.65 0.61 0.62 0.77\n",
      " 0.66 0.54 0.67 0.73 0.75 0.54 0.55 0.68 0.71 0.6  0.61 0.68 0.62 0.6\n",
      " 0.64 0.62 0.71 0.57 0.75 0.59 0.61 0.61 0.69 0.66 0.61 0.57 0.54 0.68\n",
      " 0.55 0.72 0.63 0.71 0.65 0.65 0.62 0.59 0.62 0.64 0.66 0.62 0.57 0.66\n",
      " 0.64 0.56 0.66 0.53 0.66 0.59 0.67 0.69 0.6  0.6  0.62 0.62 0.6  0.63\n",
      " 0.56 0.7  0.63 0.53 0.63 0.63 0.62 0.74 0.67 0.61 0.67 0.64 0.7  0.56\n",
      " 0.63 0.69 0.66 0.63 0.65 0.69 0.67 0.59 0.75 0.55 0.65 0.62 0.69 0.67\n",
      " 0.55 0.64 0.66 0.63 0.64 0.59 0.54 0.72 0.59 0.7  0.63 0.54 0.71 0.64\n",
      " 0.66 0.66 0.62 0.61 0.67 0.7  0.57 0.65 0.74 0.63 0.72 0.61 0.63 0.64\n",
      " 0.68 0.61 0.58 0.69 0.62 0.65 0.7  0.67 0.57 0.59 0.68 0.61 0.62 0.6\n",
      " 0.51 0.58 0.6  0.63 0.62 0.59 0.72 0.54 0.66 0.61 0.57 0.67 0.7  0.57\n",
      " 0.54 0.57 0.66 0.66 0.73 0.66 0.69 0.59 0.75 0.65 0.64 0.7  0.5  0.63\n",
      " 0.62 0.69 0.72 0.69 0.68 0.69]\n",
      "non\n",
      "[0.66 0.71 0.82 0.76 0.68 0.78 0.68 0.73 0.61 0.7  0.51 0.62 0.82 0.77\n",
      " 0.86 0.67 0.71 0.82 0.68 0.75 0.64 0.64 0.76 0.79 0.67 0.76 0.73 0.67\n",
      " 0.73 0.67 0.64 0.8  0.7  0.88 0.76 0.73 0.68 0.67 0.69 0.7  0.8  0.58\n",
      " 0.81 0.74 0.74 0.7  0.63 0.67 0.71 0.73 0.71 0.59 0.64 0.7  0.74 0.76\n",
      " 0.7  0.6  0.69 0.72 0.67 0.54 0.66 0.78 0.68 0.78 0.63 0.73 0.71 0.78\n",
      " 0.74 0.73 0.76 0.73 0.86 0.7  0.76 0.71 0.7  0.71 0.69 0.71 0.66 0.65\n",
      " 0.75 0.74 0.68 0.77 0.64 0.79 0.8  0.82 0.74 0.72 0.68 0.72 0.73 0.63\n",
      " 0.87 0.69 0.69 0.82 0.71 0.7  0.79 0.7  0.72 0.77 0.69 0.77 0.79 0.74\n",
      " 0.67 0.66 0.7  0.7  0.62 0.74 0.66 0.63 0.76 0.72 0.8  0.68 0.76 0.71\n",
      " 0.75 0.71 0.79 0.7  0.73 0.55 0.82 0.66 0.79 0.64 0.66 0.67 0.7  0.84\n",
      " 0.8  0.77 0.8  0.79 0.76 0.74 0.69 0.75 0.82 0.75 0.74 0.69 0.64 0.61\n",
      " 0.68 0.75 0.76 0.68 0.73 0.73 0.65 0.76 0.79 0.73 0.75 0.63 0.72 0.9\n",
      " 0.64 0.66 0.74 0.82 0.72 0.75 0.73 0.6  0.71 0.78 0.75 0.77 0.72 0.83\n",
      " 0.66 0.76 0.77 0.74 0.74 0.7  0.71 0.85 0.67 0.72 0.72 0.72 0.75 0.77\n",
      " 0.72 0.66 0.78 0.64 0.71 0.76 0.69 0.85 0.7  0.75 0.74 0.59 0.8  0.74\n",
      " 0.59 0.78 0.68 0.8  0.67 0.76 0.75 0.7  0.85 0.69 0.83 0.71 0.79 0.65\n",
      " 0.59 0.81 0.72 0.78 0.85 0.7  0.58 0.84 0.75 0.74 0.75 0.71 0.72 0.72\n",
      " 0.74 0.68 0.64 0.71 0.66 0.67 0.75 0.76 0.81 0.72 0.74 0.79 0.69 0.79\n",
      " 0.82 0.8  0.71 0.67 0.69 0.78 0.76 0.8  0.74 0.66 0.71 0.7  0.76 0.72\n",
      " 0.74 0.78 0.75 0.62 0.78 0.59 0.73 0.68 0.7  0.85 0.67 0.71 0.79 0.78\n",
      " 0.82 0.8  0.76 0.82 0.78 0.78 0.75 0.78 0.8  0.83 0.76 0.88 0.71 0.69\n",
      " 0.72 0.8  0.81 0.75 0.67 0.69]\n",
      "noc\n",
      "[0.46 0.4  0.43 0.55 0.38 0.33 0.5  0.29 0.38 0.38 0.63 0.45 0.44 0.38\n",
      " 0.51 0.45 0.27 0.39 0.37 0.38 0.4  0.39 0.42 0.4  0.47 0.37 0.36 0.49\n",
      " 0.44 0.37 0.4  0.41 0.4  0.43 0.29 0.35 0.35 0.43 0.44 0.35 0.38 0.41\n",
      " 0.34 0.43 0.53 0.31 0.42 0.53 0.21 0.45 0.33 0.4  0.47 0.36 0.38 0.4\n",
      " 0.4  0.28 0.43 0.3  0.45 0.35 0.51 0.47 0.43 0.45 0.42 0.52 0.52 0.44\n",
      " 0.37 0.33 0.39 0.38 0.38 0.38 0.39 0.43 0.45 0.42 0.52 0.32 0.5  0.42\n",
      " 0.42 0.58 0.46 0.39 0.49 0.42 0.49 0.38 0.4  0.38 0.43 0.44 0.4  0.39\n",
      " 0.41 0.42 0.49 0.42 0.38 0.38 0.41 0.46 0.38 0.37 0.45 0.5  0.41 0.42\n",
      " 0.37 0.29 0.3  0.29 0.36 0.43 0.49 0.4  0.49 0.49 0.37 0.41 0.4  0.4\n",
      " 0.29 0.52 0.41 0.49 0.38 0.46 0.52 0.46 0.44 0.41 0.4  0.51 0.42 0.53\n",
      " 0.35 0.31 0.45 0.36 0.42 0.38 0.44 0.4  0.27 0.45 0.44 0.45 0.32 0.41\n",
      " 0.47 0.4  0.45 0.36 0.6  0.38 0.43 0.36 0.41 0.36 0.41 0.33 0.37 0.45\n",
      " 0.42 0.45 0.35 0.41 0.34 0.52 0.55 0.43 0.48 0.38 0.44 0.46 0.48 0.48\n",
      " 0.48 0.38 0.3  0.44 0.35 0.47 0.4  0.34 0.51 0.47 0.45 0.41 0.36 0.42\n",
      " 0.39 0.46 0.43 0.49 0.4  0.42 0.49 0.33 0.45 0.4  0.46 0.33 0.39 0.4\n",
      " 0.51 0.44 0.41 0.4  0.48 0.4  0.36 0.41 0.37 0.44 0.46 0.33 0.45 0.43\n",
      " 0.33 0.3  0.41 0.41 0.5  0.42 0.47 0.44 0.46 0.37 0.38 0.31 0.49 0.49\n",
      " 0.43 0.39 0.37 0.37 0.44 0.48 0.44 0.45 0.4  0.42 0.45 0.48 0.53 0.5\n",
      " 0.36 0.44 0.47 0.52 0.36 0.39 0.34 0.39 0.4  0.45 0.36 0.46 0.31 0.48\n",
      " 0.29 0.39 0.38 0.38 0.38 0.37 0.43 0.46 0.47 0.39 0.54 0.49 0.27 0.48\n",
      " 0.26 0.48 0.46 0.42 0.43 0.38 0.32 0.53 0.53 0.36 0.39 0.45 0.43 0.45\n",
      " 0.49 0.44 0.51 0.47 0.42 0.35]\n",
      "lwf-cls-lr0_005-a1-t1\n",
      "continual\n",
      "[0.0911]\n",
      "sys\n",
      "[0.67 0.74 0.63 0.65 0.76 0.71 0.79 0.73 0.68 0.68 0.7  0.84 0.66 0.73\n",
      " 0.74 0.79 0.67 0.77 0.6  0.63 0.74 0.63 0.82 0.71 0.73 0.63 0.65 0.82\n",
      " 0.75 0.74 0.68 0.66 0.67 0.76 0.76 0.75 0.66 0.59 0.77 0.68 0.68 0.72\n",
      " 0.62 0.74 0.58 0.78 0.71 0.61 0.75 0.72 0.65 0.73 0.81 0.72 0.51 0.73\n",
      " 0.72 0.77 0.75 0.8  0.75 0.67 0.67 0.7  0.7  0.76 0.71 0.71 0.67 0.63\n",
      " 0.71 0.7  0.78 0.68 0.67 0.78 0.54 0.76 0.85 0.78 0.86 0.58 0.66 0.64\n",
      " 0.65 0.55 0.78 0.63 0.74 0.76 0.69 0.76 0.7  0.68 0.64 0.69 0.74 0.89\n",
      " 0.72 0.7  0.79 0.83 0.57 0.67 0.71 0.61 0.68 0.69 0.75 0.68 0.75 0.65\n",
      " 0.76 0.68 0.76 0.71 0.73 0.57 0.77 0.63 0.73 0.63 0.61 0.81 0.77 0.79\n",
      " 0.74 0.69 0.61 0.67 0.76 0.75 0.73 0.72 0.72 0.6  0.73 0.6  0.67 0.72\n",
      " 0.76 0.75 0.66 0.69 0.69 0.78 0.66 0.81 0.75 0.64 0.63 0.75 0.75 0.69\n",
      " 0.81 0.67 0.81 0.65 0.65 0.67 0.75 0.81 0.81 0.71 0.69 0.71 0.73 0.73\n",
      " 0.75 0.63 0.74 0.72 0.79 0.77 0.8  0.67 0.75 0.73 0.7  0.7  0.67 0.79\n",
      " 0.74 0.68 0.7  0.75 0.69 0.66 0.7  0.62 0.53 0.78 0.63 0.62 0.76 0.67\n",
      " 0.58 0.77 0.79 0.83 0.69 0.72 0.82 0.66 0.8  0.76 0.75 0.68 0.74 0.75\n",
      " 0.66 0.72 0.77 0.78 0.71 0.72 0.71 0.72 0.7  0.65 0.76 0.7  0.58 0.63\n",
      " 0.61 0.73 0.73 0.77 0.7  0.54 0.66 0.68 0.7  0.73 0.68 0.8  0.73 0.72\n",
      " 0.63 0.77 0.77 0.74 0.71 0.7  0.71 0.76 0.79 0.77 0.7  0.82 0.74 0.77\n",
      " 0.7  0.64 0.73 0.72 0.74 0.61 0.8  0.8  0.71 0.66 0.76 0.78 0.76 0.77\n",
      " 0.77 0.71 0.65 0.57 0.76 0.61 0.82 0.75 0.79 0.77 0.78 0.67 0.67 0.65\n",
      " 0.73 0.75 0.76 0.74 0.67 0.63 0.68 0.62 0.66 0.79 0.81 0.68 0.76 0.7\n",
      " 0.71 0.82 0.76 0.69 0.78 0.73]\n",
      "pro\n",
      "[0.77 0.75 0.67 0.8  0.68 0.78 0.84 0.75 0.71 0.74 0.75 0.74 0.66 0.79\n",
      " 0.68 0.8  0.76 0.74 0.73 0.81 0.74 0.62 0.67 0.75 0.66 0.7  0.67 0.69\n",
      " 0.77 0.77 0.71 0.71 0.67 0.71 0.8  0.65 0.76 0.76 0.73 0.74 0.72 0.79\n",
      " 0.8  0.68 0.77 0.62 0.74 0.7  0.69 0.73 0.8  0.78 0.7  0.7  0.76 0.72\n",
      " 0.77 0.63 0.69 0.7  0.72 0.78 0.74 0.77 0.74 0.79 0.64 0.63 0.74 0.72\n",
      " 0.78 0.83 0.73 0.78 0.73 0.75 0.66 0.73 0.74 0.68 0.73 0.63 0.68 0.74\n",
      " 0.65 0.83 0.71 0.65 0.67 0.74 0.77 0.69 0.78 0.76 0.65 0.61 0.75 0.67\n",
      " 0.79 0.77 0.68 0.74 0.76 0.8  0.62 0.64 0.75 0.68 0.76 0.79 0.79 0.73\n",
      " 0.74 0.75 0.75 0.75 0.82 0.75 0.7  0.79 0.73 0.69 0.76 0.85 0.74 0.74\n",
      " 0.75 0.74 0.78 0.65 0.74 0.71 0.78 0.66 0.7  0.72 0.77 0.74 0.79 0.71\n",
      " 0.68 0.74 0.8  0.75 0.75 0.68 0.79 0.78 0.64 0.7  0.7  0.73 0.77 0.73\n",
      " 0.76 0.73 0.7  0.76 0.76 0.8  0.72 0.73 0.73 0.81 0.77 0.81 0.71 0.77\n",
      " 0.77 0.65 0.74 0.63 0.77 0.67 0.71 0.77 0.74 0.7  0.69 0.68 0.74 0.69\n",
      " 0.72 0.75 0.81 0.72 0.65 0.84 0.74 0.79 0.78 0.78 0.82 0.74 0.61 0.62\n",
      " 0.61 0.83 0.77 0.72 0.8  0.68 0.74 0.69 0.84 0.71 0.75 0.69 0.69 0.64\n",
      " 0.75 0.74 0.82 0.62 0.72 0.76 0.71 0.71 0.6  0.83 0.81 0.77 0.75 0.76\n",
      " 0.7  0.74 0.78 0.77 0.8  0.72 0.71 0.68 0.67 0.74 0.68 0.74 0.74 0.7\n",
      " 0.82 0.79 0.77 0.73 0.8  0.62 0.75 0.71 0.66 0.68 0.69 0.84 0.72 0.73\n",
      " 0.78 0.63 0.77 0.81 0.68 0.86 0.79 0.76 0.69 0.7  0.83 0.81 0.7  0.75\n",
      " 0.77 0.71 0.77 0.74 0.77 0.74 0.8  0.67 0.7  0.79 0.75 0.74 0.73 0.59\n",
      " 0.78 0.73 0.76 0.75 0.73 0.68 0.74 0.72 0.68 0.74 0.76 0.7  0.73 0.87\n",
      " 0.68 0.79 0.75 0.74 0.71 0.64]\n",
      "sub\n",
      "[0.67 0.67 0.75 0.68 0.66 0.61 0.57 0.69 0.7  0.71 0.48 0.7  0.76 0.79\n",
      " 0.61 0.67 0.68 0.66 0.67 0.68 0.57 0.69 0.67 0.64 0.67 0.7  0.7  0.73\n",
      " 0.63 0.72 0.67 0.65 0.64 0.67 0.69 0.83 0.58 0.69 0.73 0.75 0.71 0.62\n",
      " 0.74 0.76 0.74 0.74 0.65 0.61 0.66 0.64 0.65 0.68 0.71 0.6  0.69 0.7\n",
      " 0.62 0.77 0.73 0.66 0.64 0.59 0.8  0.8  0.74 0.75 0.77 0.67 0.66 0.65\n",
      " 0.67 0.74 0.66 0.66 0.73 0.67 0.69 0.73 0.68 0.68 0.52 0.62 0.74 0.73\n",
      " 0.81 0.68 0.75 0.69 0.63 0.7  0.71 0.77 0.59 0.62 0.65 0.7  0.72 0.73\n",
      " 0.62 0.74 0.67 0.75 0.68 0.77 0.75 0.65 0.68 0.69 0.68 0.83 0.78 0.63\n",
      " 0.56 0.69 0.68 0.67 0.62 0.69 0.66 0.62 0.64 0.72 0.67 0.63 0.66 0.67\n",
      " 0.65 0.66 0.71 0.72 0.67 0.55 0.71 0.67 0.67 0.72 0.63 0.65 0.72 0.78\n",
      " 0.68 0.67 0.65 0.76 0.73 0.64 0.62 0.65 0.79 0.66 0.74 0.69 0.66 0.53\n",
      " 0.75 0.73 0.62 0.58 0.78 0.72 0.67 0.67 0.72 0.7  0.57 0.63 0.58 0.8\n",
      " 0.71 0.78 0.73 0.7  0.72 0.69 0.67 0.64 0.66 0.7  0.77 0.68 0.7  0.68\n",
      " 0.61 0.62 0.68 0.64 0.64 0.72 0.66 0.76 0.67 0.63 0.79 0.66 0.56 0.74\n",
      " 0.63 0.67 0.73 0.72 0.7  0.73 0.69 0.78 0.71 0.65 0.7  0.64 0.63 0.69\n",
      " 0.68 0.71 0.64 0.63 0.64 0.71 0.73 0.58 0.74 0.66 0.67 0.72 0.78 0.78\n",
      " 0.61 0.72 0.68 0.62 0.65 0.73 0.61 0.68 0.76 0.73 0.75 0.64 0.64 0.79\n",
      " 0.77 0.65 0.66 0.69 0.65 0.7  0.61 0.63 0.78 0.71 0.83 0.75 0.7  0.78\n",
      " 0.67 0.7  0.6  0.72 0.71 0.72 0.69 0.69 0.71 0.65 0.67 0.58 0.75 0.74\n",
      " 0.66 0.7  0.7  0.79 0.66 0.69 0.64 0.64 0.71 0.66 0.59 0.73 0.76 0.68\n",
      " 0.66 0.56 0.77 0.73 0.73 0.66 0.74 0.67 0.71 0.75 0.74 0.79 0.65 0.72\n",
      " 0.66 0.74 0.79 0.78 0.7  0.75]\n",
      "non\n",
      "[0.71 0.79 0.84 0.72 0.79 0.84 0.73 0.73 0.68 0.66 0.61 0.71 0.83 0.8\n",
      " 0.74 0.7  0.73 0.77 0.7  0.68 0.69 0.7  0.82 0.78 0.71 0.72 0.81 0.72\n",
      " 0.77 0.87 0.67 0.76 0.79 0.81 0.78 0.79 0.68 0.76 0.73 0.74 0.79 0.62\n",
      " 0.79 0.72 0.78 0.76 0.63 0.74 0.83 0.83 0.75 0.63 0.71 0.73 0.77 0.81\n",
      " 0.75 0.7  0.77 0.7  0.71 0.66 0.76 0.85 0.82 0.76 0.82 0.73 0.78 0.81\n",
      " 0.72 0.75 0.79 0.8  0.85 0.79 0.74 0.77 0.74 0.83 0.7  0.81 0.71 0.7\n",
      " 0.82 0.73 0.73 0.78 0.7  0.82 0.84 0.87 0.8  0.76 0.69 0.74 0.78 0.7\n",
      " 0.8  0.84 0.77 0.78 0.73 0.74 0.8  0.71 0.74 0.82 0.76 0.82 0.83 0.76\n",
      " 0.62 0.71 0.74 0.78 0.73 0.75 0.74 0.64 0.8  0.76 0.88 0.76 0.82 0.71\n",
      " 0.74 0.8  0.8  0.81 0.82 0.66 0.81 0.67 0.72 0.69 0.75 0.63 0.86 0.86\n",
      " 0.82 0.83 0.8  0.82 0.76 0.83 0.69 0.78 0.86 0.77 0.77 0.73 0.75 0.76\n",
      " 0.68 0.82 0.75 0.75 0.74 0.81 0.71 0.77 0.83 0.77 0.71 0.75 0.74 0.81\n",
      " 0.7  0.79 0.74 0.8  0.72 0.83 0.75 0.73 0.78 0.77 0.87 0.78 0.78 0.76\n",
      " 0.74 0.74 0.78 0.76 0.72 0.77 0.78 0.87 0.73 0.77 0.74 0.77 0.72 0.81\n",
      " 0.75 0.77 0.83 0.68 0.75 0.84 0.72 0.9  0.8  0.76 0.76 0.64 0.79 0.77\n",
      " 0.69 0.82 0.77 0.74 0.62 0.7  0.84 0.76 0.92 0.77 0.9  0.71 0.82 0.78\n",
      " 0.64 0.83 0.77 0.82 0.83 0.72 0.64 0.82 0.79 0.83 0.81 0.79 0.74 0.7\n",
      " 0.88 0.73 0.71 0.76 0.68 0.71 0.73 0.75 0.78 0.79 0.81 0.81 0.78 0.82\n",
      " 0.8  0.72 0.73 0.71 0.77 0.81 0.82 0.78 0.86 0.82 0.81 0.77 0.77 0.8\n",
      " 0.72 0.83 0.71 0.78 0.86 0.7  0.82 0.82 0.72 0.82 0.66 0.76 0.81 0.77\n",
      " 0.78 0.79 0.73 0.85 0.8  0.85 0.77 0.84 0.79 0.86 0.77 0.86 0.65 0.73\n",
      " 0.77 0.74 0.92 0.81 0.84 0.8 ]\n",
      "noc\n",
      "[0.59 0.43 0.42 0.55 0.46 0.38 0.48 0.38 0.49 0.44 0.62 0.55 0.4  0.39\n",
      " 0.46 0.6  0.4  0.51 0.53 0.53 0.54 0.49 0.49 0.47 0.48 0.52 0.37 0.6\n",
      " 0.5  0.42 0.5  0.5  0.52 0.47 0.43 0.45 0.51 0.41 0.55 0.5  0.52 0.46\n",
      " 0.44 0.56 0.6  0.4  0.56 0.58 0.33 0.52 0.35 0.48 0.37 0.33 0.53 0.53\n",
      " 0.47 0.43 0.45 0.36 0.61 0.36 0.54 0.56 0.41 0.58 0.55 0.51 0.5  0.49\n",
      " 0.51 0.42 0.48 0.46 0.33 0.55 0.33 0.52 0.51 0.52 0.46 0.49 0.49 0.51\n",
      " 0.46 0.63 0.47 0.53 0.62 0.56 0.55 0.37 0.5  0.47 0.54 0.46 0.44 0.53\n",
      " 0.55 0.55 0.53 0.52 0.43 0.5  0.46 0.53 0.48 0.57 0.42 0.51 0.52 0.61\n",
      " 0.43 0.41 0.33 0.35 0.4  0.55 0.53 0.48 0.59 0.55 0.45 0.47 0.54 0.47\n",
      " 0.46 0.49 0.47 0.6  0.48 0.47 0.56 0.44 0.54 0.41 0.46 0.48 0.4  0.55\n",
      " 0.43 0.44 0.46 0.53 0.6  0.52 0.38 0.51 0.32 0.47 0.4  0.52 0.4  0.53\n",
      " 0.5  0.41 0.62 0.44 0.64 0.44 0.48 0.52 0.41 0.56 0.42 0.45 0.49 0.51\n",
      " 0.44 0.48 0.48 0.55 0.37 0.65 0.58 0.58 0.48 0.35 0.48 0.55 0.5  0.54\n",
      " 0.59 0.41 0.43 0.45 0.45 0.46 0.56 0.44 0.56 0.54 0.47 0.53 0.51 0.53\n",
      " 0.35 0.48 0.39 0.6  0.49 0.39 0.54 0.5  0.51 0.52 0.55 0.4  0.42 0.41\n",
      " 0.54 0.5  0.41 0.47 0.49 0.43 0.52 0.51 0.45 0.49 0.56 0.48 0.49 0.42\n",
      " 0.42 0.39 0.45 0.4  0.57 0.48 0.54 0.46 0.53 0.47 0.38 0.46 0.55 0.59\n",
      " 0.52 0.54 0.56 0.51 0.47 0.46 0.45 0.51 0.5  0.54 0.54 0.49 0.57 0.57\n",
      " 0.44 0.58 0.53 0.54 0.47 0.43 0.39 0.59 0.41 0.58 0.46 0.5  0.4  0.57\n",
      " 0.4  0.51 0.4  0.46 0.49 0.4  0.53 0.5  0.57 0.47 0.6  0.54 0.42 0.58\n",
      " 0.29 0.5  0.52 0.51 0.51 0.47 0.4  0.58 0.49 0.48 0.56 0.5  0.46 0.5\n",
      " 0.51 0.54 0.47 0.42 0.55 0.49]\n",
      "ewc-cls-lr0_005-lambda0_1\n",
      "continual\n",
      "[0.0822]\n",
      "sys\n",
      "[0.7  0.61 0.67 0.58 0.69 0.71 0.7  0.65 0.58 0.7  0.71 0.74 0.59 0.67\n",
      " 0.57 0.72 0.6  0.65 0.55 0.67 0.72 0.56 0.74 0.7  0.79 0.62 0.57 0.69\n",
      " 0.67 0.58 0.69 0.56 0.63 0.7  0.61 0.72 0.65 0.55 0.67 0.56 0.52 0.68\n",
      " 0.62 0.65 0.48 0.75 0.64 0.58 0.72 0.71 0.58 0.66 0.79 0.69 0.54 0.66\n",
      " 0.63 0.83 0.69 0.79 0.63 0.64 0.52 0.58 0.71 0.67 0.67 0.65 0.59 0.58\n",
      " 0.61 0.67 0.77 0.6  0.64 0.72 0.47 0.59 0.82 0.59 0.65 0.52 0.58 0.62\n",
      " 0.56 0.53 0.67 0.55 0.68 0.73 0.7  0.64 0.7  0.73 0.54 0.59 0.72 0.77\n",
      " 0.73 0.58 0.67 0.73 0.58 0.61 0.65 0.57 0.58 0.63 0.69 0.58 0.53 0.63\n",
      " 0.7  0.58 0.61 0.7  0.69 0.51 0.66 0.61 0.57 0.66 0.6  0.67 0.71 0.68\n",
      " 0.74 0.74 0.46 0.58 0.71 0.69 0.62 0.67 0.67 0.58 0.69 0.6  0.62 0.69\n",
      " 0.75 0.65 0.55 0.73 0.65 0.68 0.62 0.78 0.7  0.53 0.61 0.71 0.66 0.62\n",
      " 0.73 0.61 0.75 0.51 0.63 0.56 0.66 0.78 0.73 0.7  0.66 0.62 0.65 0.72\n",
      " 0.68 0.54 0.67 0.67 0.69 0.71 0.66 0.69 0.67 0.71 0.6  0.64 0.58 0.76\n",
      " 0.67 0.63 0.69 0.65 0.64 0.67 0.6  0.55 0.59 0.65 0.64 0.64 0.65 0.64\n",
      " 0.59 0.71 0.73 0.69 0.7  0.68 0.71 0.72 0.66 0.61 0.63 0.62 0.73 0.67\n",
      " 0.59 0.67 0.63 0.75 0.61 0.65 0.63 0.65 0.57 0.7  0.69 0.56 0.52 0.55\n",
      " 0.56 0.66 0.59 0.77 0.66 0.6  0.56 0.58 0.7  0.62 0.65 0.8  0.64 0.61\n",
      " 0.54 0.66 0.73 0.67 0.55 0.59 0.68 0.74 0.75 0.74 0.53 0.68 0.64 0.69\n",
      " 0.71 0.68 0.68 0.6  0.73 0.54 0.77 0.72 0.65 0.53 0.75 0.63 0.69 0.75\n",
      " 0.69 0.63 0.64 0.51 0.7  0.5  0.73 0.8  0.67 0.67 0.63 0.62 0.62 0.62\n",
      " 0.72 0.7  0.69 0.71 0.58 0.62 0.67 0.51 0.71 0.65 0.7  0.63 0.7  0.59\n",
      " 0.74 0.75 0.66 0.62 0.57 0.64]\n",
      "pro\n",
      "[0.66 0.76 0.68 0.8  0.73 0.72 0.79 0.78 0.68 0.8  0.78 0.65 0.74 0.81\n",
      " 0.7  0.79 0.72 0.69 0.76 0.77 0.68 0.7  0.65 0.78 0.73 0.8  0.71 0.73\n",
      " 0.88 0.72 0.7  0.71 0.66 0.74 0.77 0.68 0.64 0.66 0.79 0.82 0.67 0.79\n",
      " 0.72 0.68 0.83 0.63 0.74 0.7  0.67 0.73 0.78 0.7  0.75 0.7  0.84 0.6\n",
      " 0.78 0.64 0.72 0.73 0.72 0.68 0.7  0.74 0.67 0.77 0.69 0.7  0.69 0.78\n",
      " 0.76 0.82 0.75 0.75 0.7  0.77 0.74 0.69 0.71 0.68 0.77 0.73 0.73 0.68\n",
      " 0.65 0.83 0.64 0.66 0.67 0.77 0.81 0.67 0.84 0.72 0.69 0.68 0.69 0.65\n",
      " 0.77 0.78 0.7  0.79 0.81 0.77 0.74 0.61 0.67 0.79 0.77 0.76 0.8  0.72\n",
      " 0.8  0.72 0.79 0.75 0.76 0.73 0.75 0.75 0.7  0.64 0.68 0.8  0.73 0.79\n",
      " 0.77 0.77 0.8  0.66 0.64 0.72 0.75 0.7  0.73 0.71 0.75 0.75 0.78 0.73\n",
      " 0.75 0.68 0.79 0.78 0.77 0.7  0.79 0.82 0.7  0.68 0.67 0.73 0.83 0.73\n",
      " 0.73 0.76 0.81 0.71 0.78 0.79 0.84 0.76 0.79 0.76 0.84 0.75 0.71 0.74\n",
      " 0.72 0.68 0.77 0.7  0.75 0.64 0.63 0.8  0.68 0.71 0.77 0.7  0.73 0.69\n",
      " 0.67 0.78 0.82 0.64 0.67 0.79 0.73 0.85 0.84 0.75 0.75 0.72 0.7  0.75\n",
      " 0.58 0.84 0.68 0.67 0.77 0.67 0.79 0.73 0.76 0.73 0.78 0.7  0.75 0.78\n",
      " 0.72 0.73 0.82 0.67 0.62 0.79 0.67 0.7  0.69 0.76 0.88 0.75 0.7  0.82\n",
      " 0.69 0.8  0.83 0.73 0.78 0.77 0.66 0.68 0.71 0.79 0.78 0.7  0.79 0.59\n",
      " 0.79 0.75 0.75 0.73 0.78 0.72 0.85 0.68 0.6  0.74 0.73 0.75 0.73 0.74\n",
      " 0.74 0.73 0.79 0.84 0.68 0.8  0.71 0.78 0.78 0.76 0.8  0.85 0.67 0.79\n",
      " 0.76 0.76 0.64 0.73 0.69 0.66 0.83 0.68 0.72 0.69 0.7  0.71 0.72 0.67\n",
      " 0.73 0.7  0.73 0.68 0.71 0.74 0.69 0.82 0.7  0.74 0.78 0.67 0.69 0.8\n",
      " 0.68 0.77 0.77 0.78 0.79 0.66]\n",
      "sub\n",
      "[0.53 0.62 0.79 0.67 0.61 0.58 0.56 0.68 0.61 0.63 0.48 0.62 0.68 0.69\n",
      " 0.63 0.58 0.61 0.65 0.62 0.66 0.51 0.59 0.59 0.56 0.6  0.61 0.68 0.58\n",
      " 0.66 0.69 0.57 0.59 0.59 0.67 0.67 0.7  0.63 0.6  0.69 0.67 0.72 0.54\n",
      " 0.62 0.69 0.63 0.66 0.53 0.57 0.63 0.63 0.58 0.57 0.64 0.58 0.61 0.63\n",
      " 0.64 0.7  0.63 0.59 0.64 0.5  0.72 0.67 0.69 0.71 0.7  0.61 0.62 0.64\n",
      " 0.62 0.61 0.63 0.71 0.68 0.59 0.6  0.61 0.67 0.54 0.6  0.6  0.58 0.52\n",
      " 0.64 0.63 0.64 0.62 0.57 0.62 0.66 0.66 0.6  0.62 0.62 0.68 0.63 0.62\n",
      " 0.63 0.68 0.56 0.72 0.54 0.61 0.71 0.64 0.69 0.69 0.59 0.77 0.67 0.6\n",
      " 0.57 0.59 0.64 0.69 0.55 0.65 0.59 0.63 0.65 0.61 0.68 0.57 0.58 0.62\n",
      " 0.57 0.55 0.69 0.65 0.68 0.45 0.58 0.64 0.69 0.62 0.55 0.54 0.68 0.75\n",
      " 0.67 0.56 0.61 0.75 0.74 0.55 0.6  0.62 0.7  0.61 0.64 0.65 0.63 0.61\n",
      " 0.66 0.63 0.65 0.55 0.73 0.61 0.62 0.65 0.75 0.64 0.61 0.57 0.56 0.74\n",
      " 0.6  0.8  0.75 0.7  0.62 0.61 0.67 0.51 0.68 0.57 0.72 0.66 0.59 0.61\n",
      " 0.63 0.5  0.64 0.52 0.61 0.59 0.66 0.73 0.7  0.54 0.69 0.55 0.56 0.75\n",
      " 0.64 0.62 0.62 0.62 0.64 0.58 0.58 0.73 0.62 0.63 0.68 0.53 0.73 0.58\n",
      " 0.57 0.61 0.63 0.58 0.69 0.65 0.69 0.61 0.6  0.66 0.71 0.65 0.7  0.66\n",
      " 0.54 0.65 0.55 0.65 0.64 0.64 0.57 0.69 0.55 0.65 0.62 0.67 0.62 0.68\n",
      " 0.67 0.63 0.6  0.61 0.67 0.66 0.67 0.53 0.72 0.58 0.78 0.68 0.58 0.62\n",
      " 0.72 0.7  0.67 0.56 0.63 0.63 0.69 0.62 0.61 0.67 0.62 0.59 0.63 0.65\n",
      " 0.61 0.68 0.71 0.7  0.63 0.56 0.61 0.49 0.58 0.66 0.55 0.71 0.77 0.66\n",
      " 0.61 0.57 0.66 0.66 0.69 0.61 0.66 0.64 0.66 0.72 0.6  0.72 0.6  0.69\n",
      " 0.58 0.65 0.73 0.69 0.63 0.69]\n",
      "non\n",
      "[0.59 0.73 0.79 0.7  0.69 0.76 0.67 0.67 0.61 0.65 0.55 0.62 0.77 0.72\n",
      " 0.73 0.64 0.7  0.78 0.59 0.73 0.64 0.67 0.8  0.73 0.56 0.68 0.72 0.61\n",
      " 0.65 0.7  0.62 0.74 0.71 0.78 0.7  0.73 0.7  0.57 0.67 0.72 0.75 0.52\n",
      " 0.73 0.77 0.7  0.7  0.65 0.65 0.68 0.75 0.67 0.54 0.65 0.63 0.72 0.73\n",
      " 0.72 0.68 0.64 0.61 0.65 0.5  0.64 0.75 0.7  0.64 0.64 0.67 0.67 0.71\n",
      " 0.64 0.6  0.75 0.68 0.73 0.63 0.72 0.69 0.69 0.76 0.72 0.66 0.65 0.62\n",
      " 0.71 0.74 0.68 0.65 0.62 0.76 0.79 0.73 0.72 0.66 0.7  0.71 0.73 0.6\n",
      " 0.71 0.66 0.58 0.65 0.65 0.7  0.66 0.58 0.65 0.65 0.71 0.76 0.69 0.69\n",
      " 0.58 0.59 0.69 0.67 0.57 0.61 0.63 0.57 0.71 0.67 0.73 0.73 0.69 0.7\n",
      " 0.72 0.68 0.8  0.65 0.75 0.58 0.69 0.67 0.72 0.62 0.64 0.63 0.72 0.77\n",
      " 0.71 0.74 0.67 0.72 0.73 0.69 0.61 0.66 0.83 0.79 0.66 0.63 0.62 0.68\n",
      " 0.67 0.63 0.68 0.49 0.69 0.67 0.61 0.71 0.73 0.7  0.64 0.73 0.61 0.8\n",
      " 0.66 0.68 0.73 0.81 0.63 0.7  0.72 0.55 0.73 0.72 0.73 0.7  0.7  0.73\n",
      " 0.64 0.68 0.79 0.69 0.68 0.75 0.7  0.78 0.62 0.72 0.69 0.66 0.67 0.76\n",
      " 0.73 0.73 0.73 0.7  0.73 0.67 0.6  0.78 0.77 0.7  0.73 0.57 0.7  0.65\n",
      " 0.56 0.68 0.68 0.74 0.64 0.69 0.73 0.68 0.8  0.66 0.84 0.67 0.78 0.66\n",
      " 0.6  0.77 0.78 0.79 0.73 0.64 0.54 0.71 0.72 0.76 0.74 0.68 0.74 0.63\n",
      " 0.73 0.75 0.67 0.72 0.69 0.59 0.65 0.65 0.77 0.64 0.74 0.72 0.67 0.75\n",
      " 0.8  0.67 0.69 0.65 0.75 0.74 0.78 0.82 0.75 0.63 0.66 0.7  0.73 0.65\n",
      " 0.69 0.77 0.7  0.68 0.76 0.52 0.77 0.71 0.62 0.82 0.65 0.63 0.81 0.78\n",
      " 0.73 0.72 0.72 0.74 0.73 0.77 0.7  0.64 0.75 0.83 0.69 0.89 0.56 0.67\n",
      " 0.59 0.76 0.85 0.68 0.64 0.71]\n",
      "noc\n",
      "[0.46 0.36 0.47 0.42 0.42 0.36 0.41 0.34 0.45 0.31 0.51 0.48 0.34 0.48\n",
      " 0.51 0.49 0.26 0.37 0.41 0.37 0.44 0.43 0.38 0.36 0.4  0.47 0.33 0.5\n",
      " 0.52 0.32 0.4  0.36 0.38 0.41 0.37 0.37 0.36 0.41 0.42 0.42 0.41 0.4\n",
      " 0.39 0.46 0.46 0.31 0.44 0.51 0.25 0.48 0.26 0.5  0.4  0.41 0.42 0.46\n",
      " 0.42 0.36 0.48 0.36 0.53 0.32 0.47 0.45 0.31 0.5  0.42 0.46 0.42 0.41\n",
      " 0.43 0.35 0.4  0.42 0.36 0.5  0.34 0.44 0.45 0.37 0.5  0.47 0.41 0.44\n",
      " 0.4  0.54 0.36 0.48 0.55 0.39 0.43 0.3  0.38 0.39 0.42 0.39 0.36 0.42\n",
      " 0.48 0.41 0.51 0.35 0.36 0.43 0.37 0.46 0.42 0.4  0.45 0.47 0.38 0.55\n",
      " 0.41 0.27 0.34 0.28 0.4  0.48 0.46 0.33 0.51 0.42 0.39 0.38 0.42 0.36\n",
      " 0.41 0.41 0.42 0.48 0.47 0.45 0.55 0.39 0.43 0.29 0.3  0.44 0.43 0.49\n",
      " 0.37 0.33 0.45 0.36 0.37 0.35 0.39 0.48 0.29 0.43 0.3  0.47 0.42 0.47\n",
      " 0.36 0.36 0.52 0.43 0.51 0.44 0.41 0.52 0.36 0.49 0.36 0.34 0.44 0.42\n",
      " 0.32 0.49 0.33 0.47 0.32 0.5  0.54 0.52 0.43 0.34 0.39 0.39 0.42 0.53\n",
      " 0.37 0.39 0.39 0.34 0.41 0.4  0.46 0.42 0.45 0.5  0.46 0.44 0.41 0.4\n",
      " 0.27 0.4  0.39 0.52 0.42 0.35 0.51 0.51 0.4  0.47 0.52 0.53 0.4  0.38\n",
      " 0.45 0.37 0.39 0.43 0.44 0.32 0.36 0.44 0.41 0.43 0.4  0.34 0.36 0.41\n",
      " 0.23 0.35 0.43 0.37 0.45 0.36 0.41 0.38 0.43 0.35 0.39 0.34 0.47 0.6\n",
      " 0.37 0.39 0.41 0.4  0.47 0.44 0.45 0.43 0.44 0.46 0.51 0.45 0.51 0.45\n",
      " 0.35 0.41 0.42 0.44 0.39 0.43 0.33 0.45 0.32 0.41 0.38 0.39 0.39 0.38\n",
      " 0.44 0.47 0.35 0.43 0.41 0.37 0.44 0.49 0.39 0.46 0.52 0.3  0.32 0.49\n",
      " 0.27 0.32 0.41 0.44 0.41 0.44 0.4  0.47 0.45 0.42 0.48 0.42 0.32 0.46\n",
      " 0.39 0.43 0.42 0.48 0.43 0.41]\n",
      "naive-tsk-lr0_008\n",
      "continual\n",
      "[0.534 0.691 0.743 0.783 0.755 0.721 0.701 0.671 0.825 0.822]\n",
      "sys\n",
      "[0.65 0.71 0.71 0.62 0.67 0.76 0.7  0.69 0.68 0.8  0.71 0.78 0.68 0.7\n",
      " 0.71 0.68 0.64 0.68 0.61 0.74 0.66 0.61 0.79 0.71 0.81 0.67 0.66 0.77\n",
      " 0.83 0.73 0.74 0.62 0.79 0.72 0.65 0.75 0.76 0.57 0.79 0.66 0.68 0.73\n",
      " 0.68 0.77 0.55 0.68 0.69 0.59 0.79 0.68 0.6  0.73 0.79 0.7  0.51 0.68\n",
      " 0.75 0.76 0.76 0.8  0.69 0.7  0.62 0.6  0.75 0.72 0.72 0.66 0.67 0.66\n",
      " 0.67 0.62 0.76 0.6  0.61 0.77 0.59 0.67 0.85 0.77 0.69 0.58 0.61 0.68\n",
      " 0.57 0.57 0.78 0.62 0.71 0.78 0.75 0.75 0.72 0.74 0.58 0.63 0.71 0.75\n",
      " 0.74 0.69 0.74 0.78 0.59 0.68 0.77 0.58 0.68 0.78 0.77 0.72 0.75 0.75\n",
      " 0.73 0.69 0.74 0.75 0.72 0.51 0.73 0.7  0.67 0.65 0.62 0.75 0.71 0.75\n",
      " 0.76 0.78 0.64 0.7  0.68 0.66 0.64 0.67 0.71 0.65 0.72 0.63 0.62 0.65\n",
      " 0.76 0.74 0.66 0.68 0.65 0.74 0.68 0.74 0.75 0.65 0.67 0.67 0.75 0.74\n",
      " 0.75 0.55 0.8  0.52 0.65 0.68 0.74 0.75 0.77 0.75 0.64 0.7  0.76 0.71\n",
      " 0.73 0.68 0.75 0.73 0.78 0.75 0.76 0.68 0.63 0.78 0.68 0.69 0.63 0.79\n",
      " 0.76 0.66 0.74 0.71 0.77 0.75 0.72 0.65 0.53 0.82 0.66 0.68 0.73 0.66\n",
      " 0.6  0.7  0.81 0.81 0.73 0.69 0.8  0.75 0.71 0.82 0.72 0.64 0.74 0.72\n",
      " 0.68 0.72 0.67 0.75 0.71 0.71 0.74 0.73 0.57 0.7  0.67 0.62 0.64 0.56\n",
      " 0.71 0.77 0.67 0.78 0.69 0.65 0.63 0.7  0.74 0.68 0.69 0.78 0.71 0.69\n",
      " 0.65 0.77 0.75 0.74 0.7  0.61 0.64 0.82 0.78 0.7  0.66 0.74 0.72 0.84\n",
      " 0.79 0.76 0.73 0.66 0.75 0.6  0.84 0.72 0.66 0.65 0.74 0.78 0.71 0.82\n",
      " 0.77 0.72 0.73 0.53 0.68 0.63 0.78 0.78 0.73 0.72 0.72 0.7  0.66 0.74\n",
      " 0.72 0.66 0.76 0.75 0.68 0.7  0.68 0.71 0.7  0.71 0.78 0.66 0.78 0.7\n",
      " 0.74 0.76 0.73 0.65 0.69 0.73]\n",
      "pro\n",
      "[0.72 0.77 0.69 0.84 0.62 0.73 0.84 0.75 0.67 0.83 0.73 0.81 0.73 0.81\n",
      " 0.74 0.76 0.74 0.73 0.67 0.83 0.7  0.59 0.7  0.72 0.69 0.72 0.73 0.69\n",
      " 0.78 0.72 0.63 0.77 0.77 0.76 0.81 0.72 0.72 0.73 0.78 0.75 0.81 0.77\n",
      " 0.73 0.69 0.72 0.62 0.64 0.6  0.74 0.74 0.8  0.72 0.7  0.76 0.74 0.67\n",
      " 0.71 0.6  0.74 0.65 0.7  0.69 0.68 0.71 0.69 0.71 0.61 0.67 0.73 0.81\n",
      " 0.66 0.79 0.72 0.75 0.67 0.73 0.72 0.72 0.76 0.76 0.71 0.69 0.64 0.69\n",
      " 0.68 0.84 0.62 0.65 0.73 0.71 0.79 0.68 0.73 0.68 0.66 0.59 0.79 0.66\n",
      " 0.79 0.77 0.65 0.7  0.68 0.78 0.75 0.64 0.8  0.73 0.71 0.81 0.73 0.78\n",
      " 0.72 0.68 0.82 0.74 0.69 0.77 0.69 0.76 0.75 0.69 0.74 0.83 0.64 0.69\n",
      " 0.78 0.7  0.78 0.65 0.69 0.71 0.7  0.68 0.71 0.72 0.84 0.72 0.79 0.67\n",
      " 0.72 0.69 0.71 0.74 0.74 0.66 0.75 0.78 0.65 0.7  0.67 0.71 0.78 0.75\n",
      " 0.73 0.73 0.79 0.77 0.73 0.78 0.77 0.68 0.72 0.78 0.79 0.72 0.71 0.76\n",
      " 0.74 0.61 0.76 0.67 0.72 0.78 0.69 0.8  0.72 0.68 0.66 0.7  0.73 0.77\n",
      " 0.75 0.77 0.75 0.7  0.69 0.79 0.71 0.85 0.8  0.71 0.77 0.63 0.65 0.66\n",
      " 0.72 0.81 0.73 0.68 0.77 0.67 0.72 0.66 0.78 0.69 0.77 0.7  0.65 0.59\n",
      " 0.74 0.71 0.83 0.65 0.68 0.82 0.66 0.66 0.64 0.8  0.74 0.76 0.74 0.79\n",
      " 0.68 0.77 0.8  0.72 0.85 0.72 0.64 0.68 0.62 0.71 0.77 0.74 0.77 0.69\n",
      " 0.65 0.79 0.74 0.69 0.83 0.65 0.79 0.62 0.71 0.7  0.8  0.74 0.67 0.74\n",
      " 0.7  0.8  0.76 0.78 0.7  0.82 0.78 0.75 0.78 0.79 0.79 0.74 0.72 0.79\n",
      " 0.71 0.68 0.72 0.73 0.76 0.73 0.8  0.7  0.72 0.72 0.8  0.74 0.74 0.69\n",
      " 0.78 0.74 0.74 0.74 0.7  0.71 0.69 0.71 0.71 0.72 0.82 0.62 0.68 0.84\n",
      " 0.65 0.73 0.81 0.78 0.72 0.68]\n",
      "sub\n",
      "[0.59 0.62 0.81 0.7  0.58 0.63 0.63 0.74 0.67 0.63 0.49 0.65 0.72 0.78\n",
      " 0.63 0.66 0.73 0.64 0.71 0.62 0.55 0.57 0.64 0.61 0.7  0.63 0.76 0.6\n",
      " 0.7  0.67 0.58 0.67 0.67 0.62 0.68 0.75 0.66 0.66 0.67 0.73 0.76 0.53\n",
      " 0.66 0.7  0.68 0.62 0.67 0.64 0.59 0.63 0.68 0.66 0.68 0.52 0.67 0.73\n",
      " 0.59 0.68 0.63 0.62 0.62 0.6  0.75 0.7  0.7  0.7  0.69 0.7  0.7  0.66\n",
      " 0.69 0.65 0.72 0.73 0.72 0.56 0.65 0.74 0.68 0.65 0.49 0.63 0.71 0.59\n",
      " 0.79 0.57 0.78 0.68 0.62 0.67 0.67 0.71 0.69 0.64 0.64 0.69 0.69 0.67\n",
      " 0.66 0.61 0.62 0.71 0.57 0.71 0.75 0.65 0.76 0.61 0.62 0.76 0.72 0.59\n",
      " 0.55 0.62 0.71 0.69 0.68 0.71 0.61 0.64 0.71 0.66 0.75 0.59 0.6  0.61\n",
      " 0.58 0.57 0.69 0.73 0.76 0.55 0.68 0.6  0.71 0.67 0.65 0.65 0.74 0.8\n",
      " 0.64 0.6  0.65 0.77 0.78 0.67 0.58 0.55 0.77 0.63 0.7  0.7  0.53 0.59\n",
      " 0.66 0.69 0.62 0.61 0.78 0.59 0.68 0.74 0.75 0.74 0.6  0.68 0.54 0.68\n",
      " 0.62 0.68 0.67 0.7  0.63 0.71 0.68 0.55 0.64 0.67 0.66 0.69 0.62 0.7\n",
      " 0.64 0.6  0.61 0.53 0.66 0.62 0.69 0.79 0.62 0.58 0.74 0.57 0.57 0.64\n",
      " 0.66 0.66 0.7  0.65 0.68 0.65 0.52 0.74 0.58 0.6  0.69 0.61 0.69 0.63\n",
      " 0.65 0.7  0.62 0.61 0.65 0.71 0.68 0.54 0.69 0.68 0.69 0.69 0.72 0.65\n",
      " 0.61 0.66 0.67 0.66 0.64 0.66 0.53 0.59 0.69 0.69 0.74 0.64 0.68 0.74\n",
      " 0.72 0.65 0.59 0.68 0.67 0.75 0.64 0.67 0.73 0.63 0.81 0.71 0.64 0.71\n",
      " 0.68 0.7  0.6  0.61 0.71 0.7  0.71 0.72 0.68 0.64 0.69 0.69 0.62 0.73\n",
      " 0.58 0.66 0.7  0.72 0.7  0.62 0.69 0.6  0.75 0.73 0.62 0.62 0.76 0.57\n",
      " 0.65 0.62 0.75 0.65 0.71 0.74 0.75 0.64 0.71 0.7  0.68 0.75 0.61 0.65\n",
      " 0.62 0.68 0.72 0.75 0.72 0.68]\n",
      "non\n",
      "[0.66 0.71 0.81 0.75 0.68 0.83 0.68 0.75 0.58 0.65 0.64 0.63 0.82 0.83\n",
      " 0.78 0.68 0.75 0.86 0.77 0.73 0.71 0.71 0.8  0.8  0.68 0.83 0.8  0.79\n",
      " 0.76 0.81 0.69 0.77 0.7  0.81 0.76 0.8  0.71 0.7  0.71 0.79 0.8  0.52\n",
      " 0.82 0.77 0.75 0.72 0.62 0.69 0.74 0.8  0.77 0.64 0.71 0.75 0.85 0.78\n",
      " 0.71 0.74 0.75 0.66 0.71 0.57 0.72 0.8  0.79 0.68 0.79 0.73 0.71 0.74\n",
      " 0.76 0.77 0.81 0.74 0.75 0.72 0.76 0.82 0.8  0.72 0.75 0.78 0.8  0.68\n",
      " 0.77 0.74 0.74 0.79 0.66 0.8  0.79 0.8  0.79 0.72 0.76 0.78 0.83 0.76\n",
      " 0.84 0.75 0.7  0.77 0.69 0.79 0.84 0.68 0.74 0.83 0.78 0.86 0.7  0.74\n",
      " 0.67 0.63 0.75 0.84 0.71 0.73 0.67 0.62 0.81 0.72 0.8  0.81 0.84 0.71\n",
      " 0.78 0.78 0.77 0.78 0.81 0.66 0.8  0.72 0.73 0.7  0.74 0.66 0.81 0.86\n",
      " 0.77 0.84 0.77 0.75 0.77 0.73 0.7  0.72 0.82 0.83 0.74 0.72 0.67 0.68\n",
      " 0.69 0.78 0.81 0.69 0.74 0.8  0.7  0.69 0.78 0.77 0.71 0.66 0.66 0.89\n",
      " 0.66 0.73 0.76 0.84 0.7  0.84 0.79 0.62 0.79 0.76 0.87 0.79 0.74 0.77\n",
      " 0.66 0.75 0.8  0.75 0.71 0.76 0.78 0.83 0.78 0.65 0.71 0.67 0.76 0.82\n",
      " 0.71 0.75 0.8  0.66 0.79 0.8  0.75 0.82 0.84 0.78 0.78 0.6  0.82 0.79\n",
      " 0.69 0.81 0.69 0.78 0.65 0.68 0.8  0.76 0.86 0.75 0.88 0.73 0.82 0.6\n",
      " 0.71 0.79 0.77 0.79 0.74 0.76 0.59 0.81 0.8  0.77 0.78 0.79 0.71 0.84\n",
      " 0.84 0.72 0.7  0.8  0.67 0.68 0.74 0.77 0.81 0.68 0.82 0.86 0.75 0.76\n",
      " 0.82 0.77 0.71 0.71 0.75 0.79 0.76 0.81 0.82 0.71 0.8  0.75 0.76 0.76\n",
      " 0.76 0.79 0.73 0.7  0.79 0.67 0.75 0.82 0.73 0.8  0.69 0.76 0.86 0.74\n",
      " 0.78 0.8  0.74 0.78 0.79 0.76 0.76 0.78 0.78 0.84 0.84 0.85 0.72 0.76\n",
      " 0.76 0.75 0.92 0.77 0.71 0.8 ]\n",
      "noc\n",
      "[0.54 0.4  0.37 0.5  0.42 0.39 0.39 0.33 0.43 0.38 0.59 0.47 0.44 0.39\n",
      " 0.58 0.47 0.3  0.42 0.3  0.35 0.42 0.37 0.44 0.38 0.43 0.49 0.31 0.46\n",
      " 0.49 0.45 0.43 0.39 0.36 0.48 0.34 0.3  0.44 0.44 0.44 0.39 0.46 0.44\n",
      " 0.38 0.5  0.58 0.34 0.47 0.53 0.27 0.49 0.29 0.47 0.44 0.32 0.49 0.4\n",
      " 0.43 0.35 0.39 0.38 0.61 0.33 0.55 0.47 0.38 0.49 0.52 0.51 0.5  0.47\n",
      " 0.44 0.34 0.39 0.39 0.35 0.37 0.43 0.42 0.44 0.49 0.47 0.43 0.45 0.43\n",
      " 0.38 0.52 0.45 0.39 0.57 0.48 0.45 0.31 0.42 0.46 0.51 0.41 0.42 0.48\n",
      " 0.52 0.49 0.51 0.47 0.4  0.47 0.43 0.44 0.37 0.37 0.45 0.43 0.44 0.57\n",
      " 0.36 0.35 0.37 0.3  0.36 0.55 0.53 0.45 0.45 0.47 0.4  0.32 0.43 0.43\n",
      " 0.3  0.45 0.43 0.52 0.32 0.4  0.54 0.39 0.47 0.43 0.47 0.43 0.44 0.49\n",
      " 0.42 0.4  0.4  0.37 0.38 0.45 0.44 0.5  0.32 0.49 0.38 0.47 0.4  0.5\n",
      " 0.41 0.37 0.5  0.44 0.54 0.45 0.41 0.53 0.37 0.46 0.41 0.46 0.38 0.45\n",
      " 0.41 0.49 0.35 0.51 0.37 0.47 0.54 0.47 0.49 0.39 0.48 0.46 0.46 0.52\n",
      " 0.38 0.36 0.32 0.41 0.46 0.44 0.53 0.49 0.5  0.43 0.42 0.45 0.36 0.44\n",
      " 0.33 0.45 0.46 0.51 0.47 0.35 0.49 0.43 0.42 0.43 0.56 0.44 0.43 0.37\n",
      " 0.49 0.4  0.45 0.5  0.5  0.39 0.41 0.52 0.38 0.38 0.46 0.41 0.44 0.34\n",
      " 0.37 0.43 0.41 0.37 0.42 0.34 0.44 0.42 0.42 0.47 0.43 0.34 0.54 0.57\n",
      " 0.44 0.44 0.47 0.41 0.43 0.37 0.42 0.44 0.42 0.48 0.39 0.51 0.47 0.48\n",
      " 0.43 0.62 0.44 0.5  0.4  0.37 0.34 0.49 0.32 0.47 0.46 0.42 0.39 0.44\n",
      " 0.41 0.39 0.47 0.42 0.49 0.3  0.42 0.46 0.5  0.38 0.52 0.44 0.28 0.49\n",
      " 0.32 0.47 0.45 0.46 0.39 0.46 0.37 0.46 0.59 0.39 0.42 0.46 0.35 0.45\n",
      " 0.34 0.45 0.46 0.43 0.47 0.41]\n",
      "er-tsk-lr0_0008\n",
      "continual\n",
      "[0.669 0.773 0.745 0.821 0.763 0.766 0.715 0.702 0.806 0.845]\n",
      "sys\n",
      "[0.72 0.73 0.71 0.56 0.78 0.74 0.8  0.8  0.73 0.77 0.66 0.78 0.7  0.82\n",
      " 0.73 0.8  0.68 0.71 0.61 0.73 0.67 0.65 0.74 0.7  0.74 0.66 0.63 0.73\n",
      " 0.8  0.74 0.66 0.62 0.67 0.74 0.67 0.64 0.71 0.62 0.78 0.71 0.65 0.77\n",
      " 0.69 0.73 0.6  0.8  0.71 0.61 0.76 0.76 0.61 0.68 0.81 0.75 0.61 0.77\n",
      " 0.69 0.77 0.73 0.74 0.66 0.69 0.62 0.7  0.72 0.74 0.73 0.69 0.66 0.67\n",
      " 0.67 0.64 0.67 0.59 0.73 0.72 0.55 0.67 0.85 0.72 0.78 0.6  0.71 0.69\n",
      " 0.65 0.64 0.75 0.67 0.74 0.78 0.75 0.78 0.73 0.7  0.59 0.76 0.74 0.79\n",
      " 0.77 0.64 0.72 0.74 0.66 0.71 0.72 0.64 0.71 0.74 0.7  0.71 0.69 0.67\n",
      " 0.79 0.66 0.69 0.7  0.71 0.68 0.73 0.71 0.72 0.61 0.69 0.8  0.73 0.74\n",
      " 0.74 0.81 0.56 0.74 0.72 0.69 0.6  0.8  0.77 0.69 0.75 0.61 0.7  0.7\n",
      " 0.76 0.71 0.65 0.69 0.72 0.75 0.67 0.81 0.75 0.74 0.69 0.7  0.73 0.67\n",
      " 0.84 0.73 0.8  0.6  0.68 0.71 0.74 0.81 0.8  0.69 0.69 0.76 0.75 0.72\n",
      " 0.72 0.71 0.74 0.7  0.77 0.76 0.82 0.71 0.7  0.74 0.76 0.75 0.65 0.77\n",
      " 0.62 0.61 0.76 0.62 0.7  0.71 0.65 0.71 0.54 0.85 0.69 0.65 0.82 0.66\n",
      " 0.71 0.79 0.78 0.87 0.73 0.69 0.79 0.74 0.79 0.79 0.72 0.64 0.74 0.76\n",
      " 0.65 0.75 0.67 0.76 0.6  0.69 0.68 0.73 0.64 0.76 0.74 0.64 0.57 0.57\n",
      " 0.64 0.72 0.64 0.76 0.75 0.64 0.62 0.72 0.67 0.75 0.69 0.82 0.71 0.77\n",
      " 0.73 0.78 0.8  0.67 0.7  0.72 0.77 0.84 0.76 0.74 0.72 0.75 0.72 0.79\n",
      " 0.7  0.74 0.69 0.67 0.77 0.6  0.76 0.77 0.72 0.68 0.77 0.74 0.76 0.78\n",
      " 0.78 0.68 0.73 0.54 0.69 0.64 0.78 0.8  0.78 0.7  0.75 0.69 0.69 0.65\n",
      " 0.79 0.65 0.74 0.82 0.66 0.69 0.73 0.62 0.75 0.73 0.81 0.69 0.78 0.67\n",
      " 0.76 0.7  0.78 0.65 0.81 0.73]\n",
      "pro\n",
      "[0.72 0.69 0.66 0.83 0.66 0.76 0.84 0.76 0.65 0.81 0.71 0.76 0.73 0.83\n",
      " 0.79 0.78 0.78 0.77 0.69 0.76 0.77 0.62 0.68 0.81 0.78 0.8  0.68 0.67\n",
      " 0.83 0.77 0.68 0.77 0.71 0.68 0.78 0.67 0.7  0.79 0.72 0.75 0.76 0.66\n",
      " 0.72 0.69 0.81 0.57 0.76 0.63 0.72 0.68 0.78 0.65 0.76 0.66 0.76 0.65\n",
      " 0.73 0.62 0.73 0.63 0.7  0.64 0.73 0.74 0.61 0.78 0.6  0.64 0.77 0.73\n",
      " 0.75 0.77 0.68 0.71 0.72 0.73 0.74 0.78 0.73 0.7  0.74 0.67 0.71 0.66\n",
      " 0.59 0.83 0.61 0.68 0.69 0.68 0.81 0.72 0.76 0.76 0.64 0.59 0.74 0.6\n",
      " 0.83 0.72 0.68 0.8  0.69 0.74 0.7  0.65 0.76 0.79 0.76 0.77 0.85 0.69\n",
      " 0.74 0.72 0.8  0.76 0.77 0.8  0.8  0.73 0.7  0.73 0.7  0.84 0.66 0.71\n",
      " 0.79 0.66 0.79 0.68 0.67 0.71 0.75 0.65 0.65 0.79 0.74 0.77 0.75 0.66\n",
      " 0.73 0.76 0.81 0.76 0.71 0.64 0.73 0.83 0.63 0.69 0.61 0.65 0.83 0.75\n",
      " 0.65 0.77 0.82 0.72 0.79 0.79 0.61 0.8  0.76 0.8  0.78 0.72 0.71 0.68\n",
      " 0.76 0.6  0.78 0.64 0.75 0.56 0.69 0.78 0.72 0.72 0.7  0.69 0.71 0.69\n",
      " 0.73 0.84 0.69 0.69 0.71 0.77 0.78 0.84 0.8  0.74 0.74 0.7  0.75 0.57\n",
      " 0.74 0.79 0.69 0.67 0.78 0.71 0.68 0.67 0.75 0.65 0.75 0.72 0.63 0.72\n",
      " 0.8  0.75 0.78 0.68 0.71 0.79 0.7  0.64 0.7  0.72 0.77 0.76 0.67 0.78\n",
      " 0.71 0.76 0.8  0.72 0.85 0.74 0.69 0.64 0.73 0.83 0.72 0.69 0.74 0.68\n",
      " 0.78 0.73 0.74 0.71 0.83 0.65 0.75 0.6  0.68 0.68 0.77 0.72 0.74 0.77\n",
      " 0.81 0.7  0.78 0.8  0.7  0.84 0.75 0.79 0.78 0.68 0.81 0.75 0.65 0.74\n",
      " 0.7  0.74 0.75 0.77 0.71 0.7  0.84 0.72 0.73 0.77 0.73 0.69 0.73 0.6\n",
      " 0.74 0.78 0.76 0.71 0.69 0.65 0.73 0.77 0.68 0.74 0.72 0.68 0.61 0.83\n",
      " 0.74 0.79 0.73 0.81 0.84 0.63]\n",
      "sub\n",
      "[0.55 0.65 0.82 0.64 0.63 0.65 0.63 0.64 0.68 0.62 0.49 0.69 0.7  0.75\n",
      " 0.64 0.68 0.69 0.51 0.68 0.56 0.54 0.51 0.74 0.61 0.65 0.72 0.71 0.62\n",
      " 0.73 0.67 0.62 0.7  0.67 0.67 0.75 0.72 0.63 0.7  0.68 0.76 0.75 0.62\n",
      " 0.7  0.71 0.74 0.72 0.64 0.57 0.7  0.66 0.63 0.66 0.67 0.57 0.64 0.73\n",
      " 0.69 0.67 0.77 0.57 0.57 0.6  0.79 0.72 0.75 0.62 0.73 0.71 0.71 0.66\n",
      " 0.68 0.73 0.64 0.68 0.7  0.72 0.67 0.74 0.69 0.61 0.6  0.66 0.66 0.65\n",
      " 0.76 0.66 0.61 0.61 0.66 0.68 0.67 0.7  0.69 0.62 0.61 0.65 0.7  0.73\n",
      " 0.7  0.63 0.67 0.71 0.61 0.73 0.66 0.63 0.71 0.68 0.59 0.76 0.63 0.64\n",
      " 0.61 0.63 0.61 0.69 0.64 0.68 0.6  0.63 0.7  0.77 0.75 0.66 0.68 0.71\n",
      " 0.64 0.7  0.65 0.72 0.7  0.56 0.63 0.66 0.71 0.63 0.62 0.69 0.72 0.71\n",
      " 0.6  0.57 0.73 0.81 0.67 0.68 0.6  0.57 0.79 0.74 0.67 0.64 0.65 0.62\n",
      " 0.69 0.62 0.64 0.56 0.7  0.67 0.57 0.61 0.68 0.73 0.64 0.7  0.55 0.77\n",
      " 0.69 0.69 0.64 0.68 0.7  0.73 0.67 0.59 0.63 0.68 0.72 0.69 0.71 0.73\n",
      " 0.72 0.64 0.59 0.52 0.64 0.69 0.65 0.72 0.65 0.63 0.74 0.74 0.59 0.61\n",
      " 0.68 0.73 0.71 0.66 0.66 0.73 0.64 0.73 0.61 0.67 0.64 0.68 0.68 0.67\n",
      " 0.73 0.71 0.68 0.59 0.67 0.69 0.65 0.6  0.68 0.67 0.65 0.63 0.66 0.66\n",
      " 0.6  0.72 0.64 0.61 0.63 0.67 0.57 0.6  0.62 0.63 0.74 0.66 0.71 0.67\n",
      " 0.69 0.7  0.7  0.69 0.67 0.7  0.56 0.54 0.69 0.64 0.78 0.66 0.64 0.67\n",
      " 0.67 0.65 0.64 0.64 0.69 0.64 0.74 0.67 0.71 0.62 0.7  0.63 0.67 0.75\n",
      " 0.6  0.69 0.72 0.76 0.68 0.65 0.72 0.67 0.69 0.69 0.59 0.62 0.71 0.65\n",
      " 0.62 0.62 0.74 0.67 0.72 0.71 0.69 0.64 0.77 0.67 0.76 0.68 0.62 0.65\n",
      " 0.63 0.74 0.78 0.71 0.65 0.75]\n",
      "non\n",
      "[0.64 0.78 0.84 0.8  0.75 0.81 0.73 0.71 0.63 0.68 0.73 0.68 0.84 0.85\n",
      " 0.79 0.73 0.81 0.82 0.77 0.7  0.63 0.76 0.88 0.86 0.66 0.87 0.83 0.77\n",
      " 0.73 0.78 0.72 0.75 0.77 0.84 0.8  0.8  0.8  0.71 0.74 0.82 0.82 0.6\n",
      " 0.81 0.76 0.75 0.73 0.65 0.73 0.75 0.82 0.83 0.64 0.73 0.7  0.86 0.77\n",
      " 0.74 0.78 0.73 0.7  0.72 0.61 0.72 0.75 0.81 0.74 0.71 0.76 0.79 0.78\n",
      " 0.7  0.74 0.78 0.74 0.83 0.78 0.71 0.74 0.78 0.78 0.76 0.77 0.75 0.7\n",
      " 0.77 0.7  0.79 0.74 0.7  0.78 0.8  0.79 0.79 0.75 0.77 0.73 0.79 0.73\n",
      " 0.81 0.83 0.75 0.81 0.8  0.73 0.8  0.72 0.68 0.78 0.77 0.86 0.76 0.8\n",
      " 0.71 0.68 0.76 0.73 0.71 0.7  0.7  0.71 0.78 0.73 0.84 0.77 0.81 0.74\n",
      " 0.72 0.8  0.81 0.8  0.78 0.6  0.77 0.77 0.72 0.69 0.83 0.74 0.85 0.8\n",
      " 0.86 0.81 0.72 0.77 0.76 0.7  0.66 0.74 0.9  0.84 0.71 0.71 0.63 0.72\n",
      " 0.7  0.8  0.73 0.66 0.67 0.78 0.67 0.8  0.81 0.78 0.73 0.7  0.72 0.83\n",
      " 0.75 0.78 0.78 0.84 0.75 0.83 0.78 0.73 0.81 0.81 0.84 0.8  0.79 0.78\n",
      " 0.77 0.76 0.73 0.8  0.75 0.68 0.79 0.81 0.65 0.74 0.79 0.72 0.82 0.8\n",
      " 0.74 0.75 0.85 0.71 0.75 0.78 0.73 0.86 0.77 0.67 0.71 0.61 0.78 0.77\n",
      " 0.62 0.83 0.79 0.79 0.69 0.74 0.89 0.78 0.93 0.77 0.85 0.74 0.79 0.72\n",
      " 0.66 0.79 0.85 0.84 0.85 0.7  0.65 0.78 0.82 0.71 0.77 0.78 0.7  0.77\n",
      " 0.79 0.72 0.8  0.77 0.73 0.69 0.76 0.74 0.8  0.77 0.82 0.82 0.77 0.84\n",
      " 0.84 0.71 0.73 0.66 0.79 0.79 0.75 0.83 0.88 0.73 0.85 0.76 0.78 0.79\n",
      " 0.75 0.8  0.7  0.75 0.83 0.64 0.79 0.75 0.73 0.72 0.71 0.74 0.86 0.77\n",
      " 0.78 0.78 0.75 0.87 0.79 0.81 0.74 0.76 0.8  0.82 0.83 0.84 0.71 0.77\n",
      " 0.75 0.82 0.9  0.78 0.81 0.77]\n",
      "noc\n",
      "[0.58 0.39 0.47 0.5  0.46 0.36 0.5  0.4  0.43 0.37 0.63 0.56 0.41 0.42\n",
      " 0.54 0.56 0.45 0.47 0.48 0.51 0.46 0.49 0.55 0.42 0.41 0.53 0.43 0.55\n",
      " 0.51 0.44 0.48 0.48 0.45 0.46 0.34 0.39 0.44 0.41 0.46 0.44 0.47 0.43\n",
      " 0.35 0.55 0.61 0.47 0.5  0.57 0.26 0.43 0.31 0.44 0.43 0.38 0.46 0.42\n",
      " 0.47 0.34 0.44 0.35 0.6  0.3  0.45 0.42 0.35 0.48 0.45 0.49 0.52 0.44\n",
      " 0.44 0.43 0.42 0.42 0.41 0.36 0.36 0.49 0.38 0.46 0.42 0.48 0.44 0.51\n",
      " 0.38 0.67 0.46 0.57 0.61 0.59 0.51 0.37 0.55 0.43 0.64 0.44 0.39 0.47\n",
      " 0.5  0.45 0.49 0.43 0.41 0.53 0.52 0.47 0.44 0.39 0.4  0.49 0.43 0.57\n",
      " 0.43 0.36 0.4  0.4  0.4  0.5  0.59 0.44 0.52 0.6  0.51 0.41 0.44 0.43\n",
      " 0.41 0.48 0.38 0.58 0.45 0.51 0.49 0.48 0.47 0.49 0.45 0.51 0.29 0.46\n",
      " 0.43 0.39 0.43 0.36 0.46 0.44 0.46 0.53 0.35 0.51 0.45 0.49 0.36 0.45\n",
      " 0.43 0.44 0.57 0.45 0.6  0.41 0.48 0.47 0.44 0.49 0.42 0.45 0.49 0.46\n",
      " 0.3  0.43 0.47 0.56 0.39 0.55 0.56 0.49 0.35 0.34 0.48 0.43 0.43 0.49\n",
      " 0.5  0.38 0.39 0.43 0.36 0.43 0.47 0.5  0.46 0.51 0.48 0.38 0.48 0.47\n",
      " 0.44 0.45 0.49 0.57 0.49 0.39 0.52 0.47 0.45 0.52 0.54 0.45 0.42 0.46\n",
      " 0.54 0.42 0.42 0.58 0.49 0.38 0.45 0.51 0.43 0.42 0.43 0.42 0.43 0.38\n",
      " 0.41 0.47 0.51 0.25 0.55 0.42 0.47 0.42 0.46 0.41 0.44 0.36 0.53 0.61\n",
      " 0.47 0.46 0.48 0.44 0.44 0.4  0.42 0.48 0.43 0.43 0.46 0.36 0.51 0.53\n",
      " 0.41 0.56 0.46 0.58 0.46 0.42 0.33 0.51 0.41 0.49 0.33 0.46 0.35 0.47\n",
      " 0.42 0.48 0.44 0.39 0.43 0.44 0.47 0.39 0.4  0.47 0.59 0.52 0.35 0.53\n",
      " 0.33 0.49 0.46 0.48 0.4  0.45 0.41 0.43 0.59 0.39 0.52 0.44 0.33 0.48\n",
      " 0.45 0.55 0.5  0.5  0.48 0.48]\n",
      "gem-tsk-lr0_001-p32-m0_3\n",
      "continual\n",
      "[0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.501 0.859]\n",
      "sys\n",
      "[0.68 0.76 0.66 0.6  0.7  0.81 0.77 0.74 0.68 0.72 0.71 0.76 0.66 0.61\n",
      " 0.66 0.75 0.73 0.65 0.6  0.63 0.68 0.61 0.77 0.71 0.73 0.63 0.62 0.76\n",
      " 0.74 0.75 0.65 0.62 0.71 0.75 0.64 0.78 0.7  0.61 0.75 0.66 0.55 0.74\n",
      " 0.68 0.71 0.51 0.74 0.68 0.59 0.72 0.68 0.64 0.7  0.75 0.68 0.54 0.75\n",
      " 0.74 0.8  0.72 0.78 0.63 0.72 0.65 0.6  0.73 0.72 0.64 0.67 0.67 0.6\n",
      " 0.71 0.69 0.76 0.66 0.72 0.75 0.52 0.74 0.8  0.66 0.76 0.62 0.62 0.54\n",
      " 0.63 0.63 0.69 0.62 0.71 0.77 0.69 0.75 0.71 0.73 0.62 0.64 0.75 0.77\n",
      " 0.75 0.61 0.73 0.82 0.57 0.59 0.66 0.59 0.64 0.69 0.76 0.73 0.73 0.68\n",
      " 0.79 0.55 0.69 0.67 0.72 0.64 0.72 0.63 0.74 0.64 0.69 0.78 0.73 0.77\n",
      " 0.76 0.74 0.54 0.73 0.67 0.69 0.65 0.66 0.73 0.74 0.78 0.6  0.72 0.62\n",
      " 0.74 0.75 0.65 0.68 0.67 0.74 0.66 0.8  0.76 0.59 0.66 0.71 0.72 0.64\n",
      " 0.87 0.59 0.78 0.61 0.62 0.75 0.72 0.89 0.86 0.7  0.68 0.7  0.7  0.75\n",
      " 0.67 0.68 0.76 0.68 0.81 0.68 0.79 0.66 0.75 0.78 0.67 0.68 0.58 0.79\n",
      " 0.7  0.6  0.67 0.68 0.67 0.61 0.74 0.67 0.61 0.87 0.64 0.63 0.71 0.62\n",
      " 0.65 0.76 0.81 0.77 0.74 0.73 0.69 0.69 0.75 0.69 0.73 0.64 0.67 0.68\n",
      " 0.68 0.63 0.7  0.79 0.73 0.72 0.69 0.67 0.62 0.64 0.71 0.62 0.66 0.58\n",
      " 0.63 0.71 0.62 0.75 0.72 0.58 0.57 0.69 0.74 0.64 0.73 0.75 0.71 0.76\n",
      " 0.64 0.72 0.74 0.76 0.58 0.7  0.67 0.76 0.79 0.72 0.64 0.71 0.7  0.79\n",
      " 0.68 0.67 0.71 0.73 0.73 0.59 0.81 0.76 0.72 0.67 0.79 0.72 0.78 0.76\n",
      " 0.78 0.7  0.68 0.55 0.66 0.54 0.79 0.75 0.73 0.72 0.69 0.62 0.65 0.7\n",
      " 0.74 0.72 0.71 0.76 0.62 0.63 0.65 0.62 0.66 0.67 0.8  0.65 0.75 0.67\n",
      " 0.76 0.8  0.74 0.71 0.69 0.67]\n",
      "pro\n",
      "[0.83 0.8  0.66 0.84 0.69 0.81 0.79 0.78 0.67 0.8  0.78 0.77 0.66 0.71\n",
      " 0.65 0.77 0.73 0.77 0.69 0.8  0.64 0.6  0.7  0.81 0.74 0.84 0.63 0.7\n",
      " 0.73 0.8  0.7  0.75 0.74 0.78 0.79 0.73 0.71 0.7  0.77 0.78 0.74 0.78\n",
      " 0.81 0.66 0.79 0.59 0.72 0.73 0.69 0.77 0.81 0.69 0.71 0.72 0.79 0.59\n",
      " 0.82 0.65 0.72 0.69 0.7  0.7  0.74 0.81 0.68 0.79 0.62 0.59 0.71 0.65\n",
      " 0.77 0.83 0.68 0.73 0.69 0.69 0.76 0.78 0.7  0.71 0.75 0.75 0.67 0.68\n",
      " 0.66 0.84 0.56 0.64 0.66 0.79 0.76 0.62 0.79 0.72 0.65 0.69 0.71 0.64\n",
      " 0.8  0.79 0.7  0.8  0.77 0.85 0.71 0.58 0.7  0.73 0.74 0.69 0.8  0.67\n",
      " 0.69 0.71 0.75 0.73 0.72 0.76 0.74 0.73 0.75 0.66 0.72 0.84 0.7  0.8\n",
      " 0.76 0.76 0.81 0.62 0.7  0.71 0.77 0.61 0.64 0.72 0.74 0.73 0.75 0.77\n",
      " 0.75 0.75 0.76 0.79 0.75 0.74 0.73 0.83 0.66 0.77 0.64 0.67 0.71 0.75\n",
      " 0.64 0.79 0.8  0.75 0.75 0.68 0.75 0.78 0.72 0.67 0.75 0.76 0.71 0.68\n",
      " 0.73 0.66 0.75 0.65 0.79 0.74 0.73 0.8  0.75 0.67 0.74 0.73 0.72 0.7\n",
      " 0.72 0.78 0.74 0.73 0.64 0.84 0.78 0.8  0.79 0.72 0.74 0.81 0.72 0.6\n",
      " 0.6  0.85 0.74 0.67 0.79 0.74 0.7  0.69 0.77 0.64 0.89 0.73 0.74 0.77\n",
      " 0.71 0.72 0.81 0.64 0.62 0.86 0.74 0.65 0.7  0.75 0.72 0.78 0.77 0.81\n",
      " 0.71 0.79 0.78 0.74 0.84 0.75 0.72 0.75 0.73 0.76 0.73 0.69 0.75 0.73\n",
      " 0.73 0.81 0.73 0.64 0.77 0.72 0.73 0.61 0.65 0.72 0.87 0.78 0.75 0.69\n",
      " 0.78 0.73 0.79 0.8  0.65 0.78 0.72 0.8  0.76 0.68 0.76 0.76 0.68 0.67\n",
      " 0.83 0.77 0.69 0.68 0.73 0.76 0.81 0.66 0.75 0.73 0.73 0.72 0.76 0.66\n",
      " 0.76 0.74 0.78 0.7  0.74 0.74 0.73 0.65 0.72 0.79 0.74 0.68 0.66 0.8\n",
      " 0.73 0.74 0.76 0.73 0.84 0.65]\n",
      "sub\n",
      "[0.61 0.67 0.77 0.67 0.65 0.62 0.59 0.66 0.67 0.64 0.45 0.63 0.68 0.71\n",
      " 0.7  0.62 0.63 0.64 0.67 0.61 0.65 0.61 0.7  0.63 0.66 0.67 0.72 0.66\n",
      " 0.71 0.66 0.71 0.69 0.7  0.62 0.73 0.72 0.62 0.72 0.65 0.61 0.7  0.5\n",
      " 0.7  0.7  0.68 0.67 0.63 0.57 0.65 0.61 0.6  0.6  0.7  0.59 0.66 0.82\n",
      " 0.67 0.73 0.62 0.61 0.58 0.56 0.73 0.77 0.66 0.69 0.72 0.67 0.62 0.71\n",
      " 0.67 0.71 0.67 0.6  0.65 0.61 0.63 0.72 0.66 0.64 0.56 0.64 0.72 0.67\n",
      " 0.77 0.67 0.7  0.7  0.55 0.72 0.77 0.72 0.59 0.56 0.61 0.66 0.66 0.67\n",
      " 0.65 0.67 0.61 0.78 0.61 0.69 0.77 0.64 0.73 0.64 0.68 0.73 0.67 0.61\n",
      " 0.59 0.64 0.66 0.73 0.58 0.71 0.6  0.67 0.71 0.65 0.72 0.53 0.68 0.74\n",
      " 0.63 0.66 0.64 0.72 0.7  0.54 0.66 0.65 0.74 0.68 0.68 0.66 0.68 0.74\n",
      " 0.57 0.64 0.69 0.8  0.75 0.65 0.59 0.67 0.76 0.69 0.69 0.69 0.59 0.64\n",
      " 0.66 0.63 0.71 0.56 0.77 0.63 0.59 0.65 0.73 0.73 0.56 0.59 0.66 0.67\n",
      " 0.65 0.73 0.72 0.67 0.66 0.69 0.7  0.54 0.64 0.62 0.69 0.7  0.63 0.62\n",
      " 0.65 0.58 0.64 0.6  0.64 0.67 0.62 0.72 0.68 0.58 0.74 0.58 0.53 0.65\n",
      " 0.64 0.75 0.81 0.7  0.81 0.68 0.63 0.72 0.72 0.59 0.65 0.67 0.67 0.66\n",
      " 0.63 0.7  0.69 0.61 0.56 0.72 0.69 0.63 0.74 0.68 0.68 0.7  0.71 0.68\n",
      " 0.54 0.72 0.6  0.64 0.69 0.68 0.5  0.7  0.69 0.68 0.76 0.65 0.7  0.73\n",
      " 0.66 0.62 0.65 0.64 0.66 0.76 0.61 0.66 0.72 0.64 0.76 0.68 0.72 0.78\n",
      " 0.63 0.67 0.65 0.55 0.69 0.69 0.65 0.71 0.73 0.64 0.69 0.6  0.72 0.71\n",
      " 0.66 0.68 0.64 0.7  0.66 0.6  0.76 0.63 0.67 0.62 0.59 0.63 0.78 0.62\n",
      " 0.64 0.69 0.68 0.77 0.75 0.7  0.75 0.66 0.75 0.7  0.74 0.66 0.65 0.72\n",
      " 0.56 0.72 0.74 0.7  0.72 0.67]\n",
      "non\n",
      "[0.64 0.69 0.81 0.76 0.67 0.8  0.7  0.67 0.65 0.6  0.64 0.68 0.78 0.81\n",
      " 0.69 0.66 0.67 0.8  0.65 0.75 0.59 0.69 0.75 0.8  0.67 0.8  0.72 0.68\n",
      " 0.68 0.77 0.69 0.74 0.69 0.8  0.73 0.76 0.75 0.62 0.72 0.76 0.79 0.54\n",
      " 0.73 0.71 0.69 0.69 0.61 0.76 0.74 0.72 0.79 0.67 0.71 0.67 0.75 0.7\n",
      " 0.78 0.63 0.62 0.67 0.73 0.63 0.71 0.73 0.73 0.69 0.71 0.77 0.69 0.77\n",
      " 0.68 0.76 0.79 0.7  0.8  0.73 0.76 0.76 0.83 0.73 0.7  0.76 0.66 0.65\n",
      " 0.74 0.75 0.76 0.75 0.68 0.81 0.87 0.79 0.7  0.83 0.71 0.71 0.8  0.66\n",
      " 0.83 0.82 0.68 0.79 0.7  0.79 0.72 0.66 0.64 0.75 0.7  0.82 0.74 0.77\n",
      " 0.63 0.63 0.74 0.65 0.7  0.76 0.68 0.63 0.77 0.69 0.79 0.69 0.84 0.7\n",
      " 0.75 0.72 0.75 0.73 0.78 0.61 0.78 0.75 0.68 0.73 0.66 0.74 0.79 0.8\n",
      " 0.78 0.81 0.71 0.78 0.78 0.75 0.61 0.72 0.83 0.77 0.76 0.64 0.64 0.7\n",
      " 0.71 0.73 0.74 0.67 0.78 0.73 0.64 0.8  0.75 0.76 0.69 0.69 0.71 0.86\n",
      " 0.67 0.81 0.72 0.85 0.69 0.81 0.82 0.6  0.78 0.76 0.8  0.77 0.71 0.72\n",
      " 0.69 0.73 0.8  0.69 0.73 0.76 0.84 0.85 0.7  0.64 0.74 0.72 0.76 0.77\n",
      " 0.78 0.67 0.8  0.69 0.69 0.84 0.72 0.81 0.74 0.65 0.68 0.63 0.81 0.73\n",
      " 0.66 0.78 0.74 0.71 0.66 0.72 0.83 0.69 0.9  0.7  0.85 0.78 0.81 0.7\n",
      " 0.59 0.74 0.76 0.81 0.76 0.73 0.5  0.73 0.77 0.72 0.73 0.72 0.75 0.72\n",
      " 0.76 0.71 0.72 0.76 0.71 0.7  0.8  0.69 0.83 0.74 0.72 0.8  0.76 0.72\n",
      " 0.84 0.68 0.69 0.67 0.69 0.77 0.77 0.82 0.76 0.71 0.82 0.7  0.75 0.75\n",
      " 0.75 0.81 0.68 0.7  0.81 0.54 0.73 0.71 0.68 0.76 0.72 0.74 0.8  0.76\n",
      " 0.79 0.8  0.77 0.81 0.8  0.75 0.73 0.8  0.85 0.81 0.78 0.89 0.69 0.75\n",
      " 0.7  0.76 0.84 0.76 0.72 0.71]\n",
      "noc\n",
      "[0.49 0.38 0.48 0.54 0.45 0.47 0.44 0.33 0.42 0.42 0.59 0.51 0.31 0.47\n",
      " 0.51 0.53 0.46 0.37 0.43 0.43 0.48 0.44 0.53 0.47 0.43 0.48 0.36 0.44\n",
      " 0.49 0.38 0.43 0.37 0.47 0.39 0.37 0.38 0.47 0.48 0.49 0.47 0.48 0.42\n",
      " 0.4  0.53 0.56 0.4  0.51 0.57 0.29 0.56 0.3  0.47 0.45 0.4  0.47 0.52\n",
      " 0.42 0.42 0.48 0.4  0.58 0.34 0.5  0.43 0.36 0.52 0.49 0.51 0.56 0.46\n",
      " 0.47 0.47 0.5  0.53 0.28 0.53 0.45 0.5  0.43 0.48 0.46 0.46 0.49 0.49\n",
      " 0.41 0.52 0.52 0.51 0.55 0.47 0.51 0.37 0.47 0.46 0.59 0.41 0.46 0.44\n",
      " 0.51 0.5  0.45 0.42 0.45 0.45 0.45 0.58 0.41 0.4  0.42 0.5  0.51 0.49\n",
      " 0.41 0.35 0.34 0.42 0.34 0.58 0.54 0.41 0.5  0.59 0.49 0.48 0.39 0.54\n",
      " 0.32 0.49 0.41 0.53 0.45 0.54 0.45 0.49 0.39 0.33 0.38 0.55 0.41 0.5\n",
      " 0.54 0.42 0.44 0.41 0.43 0.4  0.5  0.51 0.35 0.45 0.45 0.59 0.39 0.55\n",
      " 0.53 0.43 0.6  0.43 0.55 0.47 0.42 0.47 0.34 0.52 0.44 0.41 0.43 0.44\n",
      " 0.44 0.48 0.43 0.46 0.39 0.56 0.6  0.55 0.51 0.37 0.4  0.47 0.46 0.49\n",
      " 0.5  0.46 0.33 0.46 0.43 0.51 0.53 0.4  0.4  0.47 0.42 0.43 0.44 0.48\n",
      " 0.36 0.38 0.47 0.57 0.45 0.4  0.6  0.5  0.48 0.47 0.53 0.46 0.46 0.37\n",
      " 0.52 0.43 0.49 0.52 0.54 0.32 0.48 0.53 0.51 0.41 0.51 0.45 0.34 0.45\n",
      " 0.36 0.43 0.54 0.36 0.52 0.47 0.52 0.41 0.52 0.4  0.4  0.36 0.52 0.59\n",
      " 0.45 0.39 0.56 0.4  0.44 0.54 0.51 0.47 0.45 0.49 0.48 0.42 0.52 0.56\n",
      " 0.44 0.52 0.42 0.5  0.41 0.44 0.4  0.6  0.37 0.48 0.41 0.44 0.48 0.47\n",
      " 0.4  0.54 0.37 0.47 0.49 0.35 0.53 0.4  0.47 0.51 0.62 0.5  0.35 0.51\n",
      " 0.37 0.43 0.57 0.57 0.46 0.51 0.4  0.44 0.48 0.44 0.46 0.44 0.34 0.49\n",
      " 0.43 0.49 0.46 0.43 0.48 0.43]\n",
      "lwf-tsk-lr0_01-a1-t1\n",
      "continual\n",
      "[0.538 0.699 0.688 0.836 0.752 0.706 0.741 0.692 0.828 0.839]\n",
      "sys\n",
      "[0.7  0.75 0.67 0.64 0.72 0.76 0.72 0.72 0.69 0.68 0.72 0.79 0.68 0.74\n",
      " 0.67 0.83 0.6  0.66 0.58 0.68 0.69 0.62 0.81 0.73 0.74 0.63 0.7  0.76\n",
      " 0.8  0.69 0.76 0.56 0.74 0.74 0.68 0.7  0.69 0.64 0.76 0.64 0.63 0.71\n",
      " 0.65 0.68 0.56 0.69 0.71 0.55 0.7  0.7  0.7  0.7  0.79 0.74 0.49 0.71\n",
      " 0.7  0.79 0.7  0.72 0.75 0.72 0.6  0.67 0.73 0.76 0.69 0.76 0.69 0.61\n",
      " 0.65 0.68 0.75 0.66 0.69 0.77 0.6  0.75 0.83 0.66 0.78 0.58 0.55 0.61\n",
      " 0.58 0.59 0.71 0.61 0.74 0.75 0.71 0.7  0.74 0.7  0.59 0.64 0.71 0.75\n",
      " 0.69 0.64 0.72 0.75 0.51 0.64 0.71 0.59 0.68 0.69 0.79 0.68 0.78 0.69\n",
      " 0.73 0.7  0.75 0.73 0.81 0.62 0.79 0.59 0.67 0.67 0.66 0.77 0.67 0.72\n",
      " 0.74 0.74 0.59 0.68 0.69 0.76 0.62 0.76 0.73 0.63 0.75 0.62 0.61 0.67\n",
      " 0.73 0.74 0.63 0.66 0.67 0.73 0.65 0.74 0.82 0.7  0.69 0.71 0.7  0.61\n",
      " 0.78 0.7  0.8  0.52 0.62 0.71 0.7  0.87 0.82 0.71 0.68 0.65 0.75 0.66\n",
      " 0.64 0.72 0.66 0.66 0.74 0.77 0.76 0.63 0.64 0.72 0.72 0.66 0.65 0.83\n",
      " 0.7  0.57 0.68 0.66 0.67 0.64 0.72 0.69 0.5  0.82 0.61 0.64 0.78 0.59\n",
      " 0.63 0.76 0.8  0.78 0.75 0.72 0.75 0.69 0.73 0.7  0.69 0.66 0.78 0.79\n",
      " 0.66 0.72 0.73 0.79 0.56 0.62 0.69 0.72 0.63 0.66 0.76 0.59 0.58 0.61\n",
      " 0.62 0.76 0.66 0.78 0.69 0.54 0.67 0.72 0.72 0.64 0.67 0.78 0.75 0.75\n",
      " 0.63 0.75 0.76 0.75 0.68 0.65 0.68 0.88 0.77 0.76 0.63 0.75 0.66 0.78\n",
      " 0.78 0.7  0.67 0.7  0.76 0.62 0.82 0.78 0.68 0.65 0.75 0.78 0.8  0.78\n",
      " 0.76 0.69 0.72 0.54 0.75 0.6  0.81 0.74 0.76 0.71 0.73 0.64 0.67 0.62\n",
      " 0.72 0.74 0.73 0.75 0.61 0.66 0.61 0.58 0.61 0.73 0.78 0.67 0.74 0.71\n",
      " 0.67 0.78 0.66 0.64 0.69 0.7 ]\n",
      "pro\n",
      "[0.74 0.78 0.67 0.76 0.71 0.65 0.85 0.76 0.67 0.79 0.71 0.77 0.68 0.79\n",
      " 0.74 0.77 0.77 0.72 0.65 0.83 0.68 0.57 0.73 0.76 0.77 0.81 0.75 0.73\n",
      " 0.78 0.82 0.74 0.8  0.78 0.77 0.81 0.7  0.78 0.72 0.76 0.77 0.77 0.76\n",
      " 0.76 0.7  0.8  0.63 0.76 0.59 0.74 0.76 0.84 0.8  0.72 0.71 0.78 0.67\n",
      " 0.74 0.7  0.73 0.67 0.71 0.8  0.71 0.69 0.65 0.75 0.64 0.65 0.79 0.76\n",
      " 0.71 0.78 0.75 0.82 0.74 0.8  0.69 0.78 0.74 0.74 0.74 0.63 0.66 0.61\n",
      " 0.67 0.83 0.67 0.65 0.77 0.74 0.8  0.78 0.78 0.74 0.64 0.59 0.82 0.61\n",
      " 0.79 0.76 0.73 0.8  0.77 0.75 0.74 0.64 0.79 0.76 0.82 0.74 0.8  0.74\n",
      " 0.75 0.69 0.82 0.79 0.7  0.79 0.66 0.77 0.72 0.66 0.75 0.78 0.78 0.69\n",
      " 0.75 0.78 0.83 0.74 0.71 0.74 0.84 0.7  0.76 0.7  0.77 0.72 0.81 0.73\n",
      " 0.8  0.74 0.76 0.83 0.68 0.61 0.72 0.81 0.68 0.71 0.66 0.73 0.82 0.73\n",
      " 0.72 0.75 0.76 0.77 0.73 0.83 0.75 0.73 0.79 0.81 0.82 0.75 0.73 0.79\n",
      " 0.76 0.69 0.7  0.66 0.78 0.66 0.72 0.79 0.76 0.68 0.76 0.63 0.76 0.76\n",
      " 0.76 0.78 0.81 0.74 0.7  0.84 0.74 0.85 0.79 0.72 0.72 0.75 0.72 0.7\n",
      " 0.7  0.78 0.72 0.69 0.78 0.68 0.75 0.66 0.8  0.69 0.75 0.73 0.75 0.64\n",
      " 0.77 0.7  0.83 0.68 0.67 0.77 0.65 0.67 0.69 0.81 0.79 0.75 0.75 0.78\n",
      " 0.73 0.8  0.78 0.77 0.84 0.77 0.67 0.73 0.73 0.85 0.71 0.72 0.71 0.72\n",
      " 0.68 0.79 0.79 0.77 0.83 0.7  0.85 0.68 0.76 0.67 0.77 0.77 0.68 0.77\n",
      " 0.81 0.7  0.71 0.87 0.67 0.85 0.83 0.73 0.75 0.75 0.81 0.8  0.71 0.77\n",
      " 0.77 0.74 0.72 0.77 0.75 0.66 0.75 0.69 0.73 0.78 0.72 0.78 0.73 0.63\n",
      " 0.82 0.82 0.77 0.76 0.8  0.81 0.7  0.75 0.73 0.76 0.73 0.63 0.68 0.8\n",
      " 0.72 0.79 0.74 0.8  0.76 0.64]\n",
      "sub\n",
      "[0.57 0.6  0.8  0.73 0.62 0.61 0.58 0.73 0.57 0.6  0.45 0.65 0.64 0.71\n",
      " 0.65 0.58 0.69 0.61 0.65 0.6  0.56 0.6  0.61 0.59 0.56 0.69 0.75 0.59\n",
      " 0.65 0.58 0.57 0.65 0.67 0.66 0.65 0.74 0.48 0.65 0.61 0.71 0.72 0.54\n",
      " 0.73 0.7  0.67 0.64 0.56 0.62 0.61 0.64 0.57 0.66 0.69 0.58 0.58 0.68\n",
      " 0.57 0.71 0.65 0.58 0.6  0.54 0.73 0.71 0.71 0.67 0.68 0.71 0.64 0.68\n",
      " 0.65 0.61 0.65 0.71 0.74 0.68 0.65 0.61 0.67 0.6  0.53 0.63 0.61 0.62\n",
      " 0.75 0.59 0.69 0.61 0.59 0.68 0.73 0.73 0.59 0.7  0.62 0.66 0.69 0.67\n",
      " 0.68 0.62 0.62 0.68 0.64 0.71 0.68 0.68 0.7  0.66 0.65 0.76 0.72 0.66\n",
      " 0.49 0.63 0.67 0.68 0.6  0.71 0.58 0.59 0.71 0.64 0.64 0.6  0.66 0.6\n",
      " 0.59 0.59 0.75 0.67 0.71 0.59 0.71 0.67 0.63 0.66 0.67 0.59 0.67 0.75\n",
      " 0.63 0.56 0.66 0.77 0.76 0.62 0.64 0.65 0.74 0.63 0.65 0.64 0.67 0.64\n",
      " 0.65 0.62 0.67 0.59 0.79 0.65 0.66 0.62 0.67 0.67 0.61 0.66 0.63 0.66\n",
      " 0.56 0.74 0.73 0.66 0.63 0.68 0.65 0.58 0.68 0.67 0.69 0.64 0.63 0.65\n",
      " 0.55 0.59 0.65 0.59 0.71 0.66 0.68 0.68 0.6  0.63 0.7  0.66 0.54 0.67\n",
      " 0.57 0.62 0.7  0.65 0.66 0.68 0.62 0.73 0.66 0.6  0.67 0.61 0.74 0.65\n",
      " 0.65 0.67 0.6  0.63 0.65 0.68 0.7  0.59 0.68 0.68 0.64 0.65 0.69 0.63\n",
      " 0.53 0.65 0.61 0.58 0.59 0.59 0.56 0.64 0.64 0.77 0.73 0.66 0.71 0.69\n",
      " 0.68 0.69 0.61 0.58 0.61 0.67 0.6  0.59 0.69 0.57 0.77 0.55 0.68 0.7\n",
      " 0.67 0.67 0.56 0.54 0.69 0.66 0.63 0.7  0.67 0.67 0.65 0.6  0.72 0.72\n",
      " 0.62 0.67 0.67 0.76 0.7  0.58 0.64 0.64 0.57 0.61 0.57 0.7  0.73 0.63\n",
      " 0.68 0.6  0.7  0.62 0.75 0.64 0.72 0.66 0.64 0.72 0.73 0.71 0.6  0.65\n",
      " 0.68 0.65 0.74 0.78 0.74 0.64]\n",
      "non\n",
      "[0.65 0.8  0.84 0.79 0.69 0.77 0.71 0.76 0.59 0.71 0.58 0.63 0.81 0.82\n",
      " 0.77 0.76 0.7  0.82 0.75 0.78 0.67 0.74 0.82 0.86 0.76 0.78 0.83 0.67\n",
      " 0.75 0.79 0.69 0.8  0.78 0.78 0.81 0.77 0.75 0.74 0.79 0.74 0.81 0.62\n",
      " 0.81 0.75 0.77 0.68 0.62 0.75 0.72 0.73 0.72 0.58 0.73 0.71 0.76 0.74\n",
      " 0.77 0.74 0.76 0.73 0.76 0.61 0.71 0.82 0.78 0.7  0.7  0.73 0.75 0.83\n",
      " 0.71 0.72 0.78 0.7  0.83 0.71 0.78 0.8  0.78 0.77 0.69 0.75 0.73 0.72\n",
      " 0.82 0.73 0.81 0.69 0.67 0.81 0.83 0.84 0.77 0.8  0.79 0.71 0.74 0.69\n",
      " 0.8  0.75 0.76 0.75 0.73 0.74 0.78 0.67 0.75 0.7  0.8  0.84 0.78 0.76\n",
      " 0.69 0.67 0.74 0.75 0.68 0.72 0.69 0.74 0.81 0.73 0.84 0.72 0.81 0.75\n",
      " 0.8  0.8  0.8  0.82 0.74 0.61 0.76 0.67 0.7  0.69 0.72 0.61 0.83 0.79\n",
      " 0.74 0.77 0.75 0.85 0.75 0.86 0.63 0.77 0.81 0.76 0.69 0.72 0.67 0.72\n",
      " 0.67 0.73 0.71 0.67 0.73 0.73 0.66 0.73 0.78 0.71 0.78 0.64 0.76 0.83\n",
      " 0.74 0.75 0.76 0.83 0.76 0.8  0.75 0.65 0.8  0.84 0.86 0.79 0.8  0.8\n",
      " 0.66 0.72 0.77 0.72 0.68 0.77 0.83 0.77 0.71 0.72 0.76 0.79 0.76 0.8\n",
      " 0.74 0.75 0.81 0.74 0.77 0.8  0.68 0.87 0.81 0.79 0.71 0.6  0.8  0.78\n",
      " 0.59 0.82 0.68 0.76 0.68 0.72 0.85 0.72 0.84 0.76 0.81 0.71 0.8  0.68\n",
      " 0.57 0.78 0.76 0.81 0.85 0.67 0.63 0.79 0.75 0.76 0.8  0.74 0.77 0.67\n",
      " 0.85 0.75 0.68 0.78 0.74 0.73 0.75 0.69 0.82 0.76 0.85 0.82 0.77 0.81\n",
      " 0.82 0.73 0.7  0.6  0.75 0.82 0.75 0.82 0.82 0.79 0.8  0.71 0.76 0.72\n",
      " 0.77 0.8  0.73 0.72 0.79 0.7  0.72 0.78 0.64 0.82 0.71 0.76 0.84 0.76\n",
      " 0.83 0.79 0.77 0.83 0.79 0.82 0.84 0.8  0.82 0.88 0.8  0.87 0.73 0.69\n",
      " 0.78 0.77 0.87 0.77 0.75 0.79]\n",
      "noc\n",
      "[0.5  0.38 0.38 0.49 0.44 0.32 0.45 0.32 0.48 0.42 0.55 0.45 0.36 0.37\n",
      " 0.45 0.46 0.3  0.38 0.49 0.44 0.4  0.42 0.41 0.36 0.44 0.49 0.38 0.43\n",
      " 0.49 0.39 0.38 0.35 0.39 0.46 0.43 0.3  0.39 0.48 0.48 0.42 0.43 0.46\n",
      " 0.43 0.48 0.52 0.36 0.43 0.52 0.29 0.43 0.21 0.44 0.43 0.41 0.44 0.45\n",
      " 0.42 0.38 0.46 0.3  0.53 0.3  0.46 0.38 0.4  0.49 0.51 0.42 0.51 0.4\n",
      " 0.44 0.37 0.44 0.43 0.32 0.48 0.33 0.53 0.47 0.39 0.48 0.44 0.41 0.48\n",
      " 0.43 0.5  0.49 0.4  0.45 0.5  0.45 0.39 0.41 0.31 0.42 0.42 0.4  0.49\n",
      " 0.44 0.48 0.53 0.35 0.41 0.53 0.49 0.46 0.39 0.31 0.48 0.48 0.44 0.53\n",
      " 0.38 0.34 0.34 0.31 0.3  0.42 0.55 0.37 0.53 0.57 0.5  0.38 0.45 0.35\n",
      " 0.43 0.51 0.44 0.46 0.42 0.48 0.45 0.4  0.42 0.3  0.35 0.43 0.4  0.51\n",
      " 0.41 0.36 0.4  0.33 0.43 0.39 0.35 0.55 0.34 0.43 0.39 0.48 0.43 0.43\n",
      " 0.44 0.31 0.47 0.38 0.5  0.42 0.49 0.46 0.41 0.47 0.38 0.33 0.38 0.43\n",
      " 0.42 0.44 0.39 0.5  0.32 0.52 0.52 0.54 0.46 0.31 0.42 0.46 0.48 0.5\n",
      " 0.42 0.39 0.34 0.33 0.4  0.49 0.43 0.41 0.47 0.53 0.41 0.47 0.4  0.49\n",
      " 0.35 0.41 0.44 0.48 0.37 0.44 0.52 0.43 0.33 0.5  0.43 0.38 0.44 0.33\n",
      " 0.53 0.43 0.44 0.45 0.48 0.38 0.4  0.43 0.37 0.46 0.42 0.35 0.49 0.38\n",
      " 0.34 0.44 0.41 0.32 0.46 0.38 0.44 0.36 0.49 0.38 0.41 0.42 0.4  0.62\n",
      " 0.43 0.42 0.43 0.4  0.51 0.41 0.35 0.46 0.43 0.4  0.46 0.51 0.47 0.53\n",
      " 0.42 0.51 0.51 0.52 0.45 0.42 0.32 0.48 0.35 0.48 0.39 0.47 0.4  0.47\n",
      " 0.35 0.4  0.34 0.36 0.39 0.39 0.46 0.4  0.49 0.39 0.59 0.38 0.31 0.45\n",
      " 0.26 0.41 0.45 0.51 0.47 0.35 0.37 0.41 0.53 0.36 0.36 0.49 0.31 0.4\n",
      " 0.45 0.45 0.46 0.4  0.46 0.4 ]\n",
      "ewc-tsk-lr0_005-lambda2\n",
      "continual\n",
      "[0.584 0.701 0.647 0.805 0.632 0.722 0.708 0.671 0.799 0.841]\n",
      "sys\n",
      "[0.67 0.68 0.59 0.65 0.66 0.74 0.76 0.68 0.67 0.71 0.74 0.83 0.64 0.69\n",
      " 0.7  0.72 0.63 0.67 0.63 0.67 0.71 0.59 0.83 0.76 0.78 0.63 0.62 0.82\n",
      " 0.72 0.71 0.65 0.56 0.69 0.75 0.71 0.75 0.64 0.63 0.75 0.7  0.61 0.76\n",
      " 0.69 0.69 0.59 0.69 0.74 0.62 0.69 0.72 0.6  0.7  0.76 0.67 0.5  0.71\n",
      " 0.71 0.75 0.72 0.8  0.7  0.7  0.56 0.65 0.68 0.73 0.62 0.74 0.65 0.63\n",
      " 0.68 0.7  0.83 0.67 0.66 0.74 0.5  0.71 0.84 0.74 0.69 0.56 0.63 0.59\n",
      " 0.54 0.62 0.69 0.66 0.69 0.79 0.73 0.67 0.71 0.72 0.62 0.64 0.76 0.82\n",
      " 0.7  0.67 0.75 0.71 0.54 0.66 0.76 0.57 0.66 0.74 0.7  0.75 0.71 0.66\n",
      " 0.74 0.68 0.73 0.71 0.72 0.57 0.72 0.65 0.68 0.65 0.64 0.78 0.66 0.72\n",
      " 0.71 0.73 0.57 0.69 0.75 0.78 0.61 0.76 0.81 0.66 0.69 0.63 0.63 0.64\n",
      " 0.79 0.74 0.65 0.73 0.65 0.75 0.64 0.76 0.74 0.64 0.68 0.69 0.71 0.64\n",
      " 0.81 0.58 0.81 0.59 0.68 0.67 0.72 0.81 0.77 0.72 0.71 0.74 0.7  0.71\n",
      " 0.67 0.65 0.73 0.74 0.78 0.71 0.76 0.65 0.65 0.74 0.71 0.72 0.65 0.87\n",
      " 0.8  0.63 0.78 0.69 0.69 0.64 0.66 0.64 0.39 0.75 0.72 0.63 0.75 0.63\n",
      " 0.67 0.75 0.85 0.77 0.71 0.72 0.77 0.73 0.78 0.75 0.66 0.65 0.74 0.75\n",
      " 0.63 0.76 0.76 0.78 0.59 0.61 0.68 0.71 0.58 0.73 0.73 0.63 0.62 0.66\n",
      " 0.6  0.74 0.67 0.73 0.75 0.52 0.57 0.74 0.73 0.64 0.56 0.89 0.75 0.62\n",
      " 0.62 0.73 0.72 0.75 0.66 0.61 0.66 0.8  0.79 0.77 0.55 0.73 0.71 0.78\n",
      " 0.73 0.7  0.71 0.71 0.7  0.63 0.77 0.8  0.66 0.58 0.77 0.7  0.75 0.71\n",
      " 0.74 0.69 0.68 0.52 0.77 0.66 0.76 0.77 0.68 0.75 0.75 0.64 0.59 0.63\n",
      " 0.74 0.7  0.74 0.7  0.61 0.57 0.63 0.63 0.72 0.75 0.81 0.69 0.74 0.72\n",
      " 0.7  0.77 0.74 0.66 0.73 0.69]\n",
      "pro\n",
      "[0.72 0.8  0.6  0.82 0.66 0.69 0.8  0.76 0.64 0.76 0.73 0.77 0.69 0.77\n",
      " 0.75 0.8  0.72 0.75 0.71 0.76 0.73 0.61 0.72 0.76 0.73 0.74 0.67 0.65\n",
      " 0.72 0.8  0.63 0.76 0.72 0.74 0.76 0.71 0.72 0.67 0.7  0.67 0.83 0.72\n",
      " 0.69 0.64 0.76 0.54 0.72 0.63 0.71 0.79 0.82 0.73 0.69 0.71 0.75 0.63\n",
      " 0.73 0.68 0.74 0.65 0.68 0.68 0.72 0.67 0.64 0.77 0.62 0.68 0.73 0.72\n",
      " 0.71 0.89 0.7  0.79 0.72 0.71 0.7  0.7  0.72 0.72 0.7  0.69 0.67 0.72\n",
      " 0.63 0.8  0.62 0.7  0.71 0.76 0.8  0.77 0.8  0.67 0.62 0.59 0.7  0.65\n",
      " 0.79 0.76 0.66 0.73 0.73 0.76 0.71 0.66 0.78 0.77 0.77 0.76 0.8  0.79\n",
      " 0.76 0.73 0.74 0.8  0.74 0.76 0.74 0.77 0.62 0.67 0.68 0.87 0.7  0.75\n",
      " 0.69 0.73 0.76 0.73 0.7  0.75 0.81 0.67 0.68 0.73 0.74 0.75 0.78 0.68\n",
      " 0.69 0.79 0.79 0.71 0.76 0.63 0.68 0.83 0.63 0.71 0.67 0.67 0.78 0.72\n",
      " 0.72 0.77 0.67 0.74 0.73 0.8  0.7  0.73 0.74 0.74 0.77 0.75 0.71 0.75\n",
      " 0.75 0.61 0.77 0.61 0.76 0.69 0.71 0.83 0.75 0.7  0.68 0.6  0.73 0.69\n",
      " 0.75 0.74 0.79 0.78 0.67 0.81 0.73 0.81 0.75 0.73 0.73 0.71 0.67 0.65\n",
      " 0.65 0.82 0.7  0.68 0.73 0.74 0.73 0.72 0.83 0.73 0.78 0.71 0.73 0.65\n",
      " 0.66 0.77 0.74 0.63 0.71 0.82 0.62 0.63 0.71 0.87 0.8  0.77 0.69 0.8\n",
      " 0.73 0.78 0.81 0.76 0.81 0.65 0.65 0.67 0.69 0.76 0.76 0.58 0.69 0.67\n",
      " 0.77 0.73 0.72 0.76 0.77 0.68 0.74 0.69 0.68 0.69 0.76 0.81 0.7  0.72\n",
      " 0.75 0.7  0.72 0.9  0.76 0.84 0.72 0.75 0.74 0.7  0.8  0.81 0.73 0.71\n",
      " 0.78 0.68 0.73 0.73 0.68 0.7  0.82 0.6  0.74 0.8  0.74 0.67 0.72 0.59\n",
      " 0.7  0.68 0.73 0.76 0.68 0.72 0.66 0.75 0.66 0.71 0.7  0.7  0.71 0.84\n",
      " 0.66 0.79 0.79 0.68 0.72 0.69]\n",
      "sub\n",
      "[0.61 0.64 0.76 0.72 0.62 0.58 0.64 0.67 0.63 0.61 0.52 0.64 0.7  0.76\n",
      " 0.64 0.69 0.64 0.58 0.76 0.57 0.58 0.61 0.64 0.63 0.67 0.68 0.71 0.56\n",
      " 0.67 0.64 0.61 0.69 0.65 0.63 0.71 0.73 0.64 0.72 0.69 0.7  0.68 0.6\n",
      " 0.74 0.72 0.65 0.61 0.61 0.56 0.58 0.63 0.59 0.59 0.69 0.55 0.65 0.74\n",
      " 0.56 0.65 0.69 0.6  0.57 0.55 0.79 0.76 0.75 0.71 0.74 0.7  0.66 0.63\n",
      " 0.72 0.69 0.65 0.69 0.68 0.57 0.7  0.7  0.63 0.57 0.56 0.65 0.66 0.64\n",
      " 0.77 0.64 0.72 0.7  0.5  0.68 0.68 0.74 0.62 0.64 0.61 0.71 0.63 0.6\n",
      " 0.58 0.65 0.59 0.69 0.61 0.68 0.65 0.62 0.69 0.65 0.58 0.78 0.65 0.58\n",
      " 0.49 0.56 0.66 0.7  0.62 0.64 0.6  0.6  0.68 0.6  0.7  0.63 0.65 0.63\n",
      " 0.63 0.54 0.69 0.75 0.7  0.52 0.71 0.59 0.68 0.66 0.62 0.61 0.73 0.7\n",
      " 0.66 0.59 0.65 0.8  0.72 0.55 0.6  0.62 0.73 0.59 0.6  0.57 0.66 0.58\n",
      " 0.68 0.55 0.71 0.59 0.72 0.63 0.56 0.63 0.73 0.62 0.59 0.61 0.67 0.71\n",
      " 0.63 0.67 0.71 0.62 0.64 0.63 0.67 0.6  0.64 0.67 0.75 0.64 0.67 0.61\n",
      " 0.61 0.56 0.66 0.53 0.64 0.68 0.68 0.74 0.63 0.6  0.72 0.67 0.51 0.62\n",
      " 0.68 0.63 0.76 0.67 0.66 0.77 0.6  0.72 0.56 0.63 0.63 0.62 0.68 0.64\n",
      " 0.54 0.69 0.62 0.6  0.53 0.71 0.64 0.6  0.67 0.68 0.67 0.61 0.71 0.64\n",
      " 0.56 0.71 0.65 0.65 0.64 0.67 0.55 0.64 0.67 0.72 0.7  0.72 0.61 0.73\n",
      " 0.68 0.66 0.62 0.63 0.66 0.62 0.62 0.63 0.74 0.62 0.79 0.66 0.58 0.73\n",
      " 0.64 0.67 0.59 0.57 0.67 0.73 0.66 0.65 0.68 0.64 0.71 0.58 0.69 0.66\n",
      " 0.66 0.68 0.61 0.77 0.63 0.55 0.61 0.57 0.69 0.71 0.59 0.64 0.73 0.6\n",
      " 0.64 0.59 0.7  0.68 0.78 0.63 0.75 0.74 0.72 0.66 0.67 0.77 0.64 0.67\n",
      " 0.66 0.62 0.7  0.67 0.71 0.66]\n",
      "non\n",
      "[0.68 0.77 0.88 0.76 0.72 0.9  0.7  0.73 0.62 0.65 0.64 0.66 0.83 0.73\n",
      " 0.74 0.71 0.67 0.82 0.74 0.79 0.64 0.72 0.81 0.8  0.63 0.8  0.8  0.73\n",
      " 0.79 0.8  0.74 0.73 0.7  0.82 0.81 0.78 0.76 0.7  0.76 0.72 0.78 0.57\n",
      " 0.81 0.71 0.75 0.74 0.63 0.75 0.77 0.83 0.71 0.59 0.72 0.72 0.82 0.82\n",
      " 0.7  0.68 0.67 0.72 0.69 0.62 0.74 0.81 0.71 0.68 0.77 0.74 0.73 0.77\n",
      " 0.64 0.82 0.8  0.66 0.83 0.73 0.81 0.75 0.74 0.76 0.74 0.71 0.66 0.65\n",
      " 0.8  0.81 0.76 0.77 0.72 0.79 0.82 0.82 0.73 0.76 0.72 0.71 0.79 0.68\n",
      " 0.84 0.75 0.71 0.75 0.71 0.73 0.77 0.68 0.76 0.76 0.7  0.82 0.74 0.71\n",
      " 0.59 0.67 0.7  0.75 0.75 0.71 0.76 0.7  0.83 0.75 0.8  0.77 0.79 0.68\n",
      " 0.76 0.75 0.77 0.78 0.79 0.64 0.74 0.68 0.71 0.66 0.8  0.65 0.8  0.83\n",
      " 0.8  0.78 0.8  0.86 0.74 0.73 0.6  0.78 0.84 0.81 0.75 0.69 0.67 0.71\n",
      " 0.68 0.69 0.73 0.73 0.69 0.77 0.67 0.8  0.82 0.75 0.77 0.73 0.68 0.9\n",
      " 0.64 0.72 0.8  0.89 0.68 0.75 0.72 0.68 0.8  0.74 0.81 0.79 0.77 0.75\n",
      " 0.65 0.72 0.7  0.81 0.75 0.84 0.73 0.86 0.75 0.73 0.69 0.65 0.78 0.74\n",
      " 0.79 0.75 0.79 0.67 0.7  0.79 0.7  0.82 0.79 0.74 0.72 0.62 0.78 0.77\n",
      " 0.63 0.77 0.76 0.8  0.62 0.74 0.85 0.7  0.88 0.8  0.83 0.68 0.84 0.69\n",
      " 0.67 0.74 0.78 0.77 0.81 0.67 0.63 0.83 0.84 0.71 0.78 0.71 0.76 0.7\n",
      " 0.8  0.73 0.73 0.78 0.7  0.76 0.77 0.73 0.84 0.69 0.84 0.78 0.76 0.85\n",
      " 0.84 0.74 0.7  0.69 0.77 0.78 0.77 0.87 0.84 0.79 0.78 0.69 0.77 0.77\n",
      " 0.8  0.83 0.74 0.72 0.81 0.64 0.83 0.8  0.68 0.79 0.65 0.68 0.8  0.79\n",
      " 0.8  0.82 0.78 0.85 0.83 0.77 0.74 0.86 0.83 0.78 0.79 0.88 0.71 0.71\n",
      " 0.72 0.8  0.9  0.79 0.81 0.8 ]\n",
      "noc\n",
      "[0.48 0.4  0.47 0.53 0.41 0.3  0.43 0.36 0.39 0.35 0.57 0.5  0.38 0.36\n",
      " 0.53 0.46 0.33 0.46 0.41 0.45 0.44 0.41 0.44 0.4  0.34 0.47 0.33 0.48\n",
      " 0.46 0.41 0.42 0.42 0.38 0.5  0.35 0.38 0.37 0.42 0.45 0.37 0.49 0.37\n",
      " 0.39 0.51 0.52 0.41 0.47 0.52 0.26 0.44 0.3  0.5  0.4  0.36 0.44 0.38\n",
      " 0.38 0.43 0.4  0.31 0.49 0.29 0.49 0.42 0.36 0.45 0.48 0.47 0.53 0.41\n",
      " 0.47 0.38 0.39 0.33 0.29 0.42 0.46 0.54 0.46 0.45 0.48 0.49 0.43 0.53\n",
      " 0.39 0.57 0.51 0.39 0.62 0.42 0.45 0.25 0.37 0.42 0.49 0.4  0.38 0.47\n",
      " 0.49 0.39 0.43 0.43 0.35 0.48 0.47 0.58 0.4  0.41 0.51 0.45 0.45 0.53\n",
      " 0.3  0.28 0.34 0.34 0.39 0.44 0.51 0.4  0.52 0.53 0.39 0.38 0.38 0.34\n",
      " 0.34 0.49 0.43 0.56 0.41 0.49 0.55 0.35 0.41 0.4  0.39 0.42 0.38 0.52\n",
      " 0.42 0.35 0.37 0.38 0.42 0.45 0.35 0.53 0.32 0.37 0.41 0.42 0.39 0.47\n",
      " 0.42 0.43 0.49 0.44 0.49 0.39 0.45 0.44 0.32 0.55 0.48 0.39 0.32 0.4\n",
      " 0.4  0.5  0.37 0.43 0.34 0.56 0.55 0.47 0.45 0.34 0.41 0.43 0.37 0.52\n",
      " 0.51 0.37 0.4  0.47 0.37 0.42 0.51 0.37 0.51 0.51 0.44 0.5  0.37 0.43\n",
      " 0.3  0.38 0.42 0.57 0.41 0.38 0.55 0.5  0.34 0.46 0.54 0.39 0.38 0.36\n",
      " 0.52 0.34 0.51 0.45 0.49 0.32 0.41 0.49 0.41 0.47 0.45 0.31 0.42 0.38\n",
      " 0.45 0.31 0.38 0.33 0.55 0.42 0.37 0.4  0.45 0.41 0.38 0.36 0.47 0.56\n",
      " 0.53 0.42 0.45 0.49 0.5  0.32 0.44 0.48 0.35 0.48 0.45 0.42 0.47 0.47\n",
      " 0.42 0.53 0.42 0.53 0.36 0.38 0.34 0.52 0.34 0.54 0.44 0.45 0.43 0.47\n",
      " 0.41 0.45 0.29 0.47 0.45 0.43 0.46 0.47 0.5  0.39 0.5  0.41 0.28 0.53\n",
      " 0.29 0.39 0.44 0.54 0.37 0.45 0.41 0.45 0.53 0.37 0.44 0.41 0.34 0.48\n",
      " 0.35 0.47 0.42 0.44 0.42 0.42]\n"
     ]
    }
   ],
   "source": [
    "'''collect exps'''\n",
    "project_name = 'CGQA'\n",
    "modes = ['continual', 'sys', 'pro', 'sub', 'non', 'noc']    # , 'forgetting'\n",
    "\n",
    "'''resnet18'''\n",
    "exps = ['naive-cls-lr0_003', 'er-cls-lr0_003', 'gem-cls-lr0_01-p32-m0_3', 'lwf-cls-lr0_005-a1-t1', 'ewc-cls-lr0_005-lambda0_1', 'naive-tsk-lr0_008', 'er-tsk-lr0_0008', 'gem-tsk-lr0_001-p32-m0_3', 'lwf-tsk-lr0_01-a1-t1', 'ewc-tsk-lr0_005-lambda2']\n",
    "exps_dis = ['Finetune', 'ER', 'GEM', 'LwF', 'EWC', 'Finetune*', 'ER*', 'GEM*', 'LwF*', 'EWC*']\n",
    "\n",
    "'''vit'''\n",
    "# exps = ['HT-MT-vit-naive-tsk_False-lr0_0001', 'ht-vit-naive-cls-lr0_0001', 'ht-vit-er-cls-lr0_0001', 'ht-vit-gem-cls-lr5e-05', 'ht-vit-lwf-cls-lr0_0001', 'ht-vit-ewc-cls-lr0_0001',\n",
    "#         'HT-MT-vit-naive-tsk_True-lr0_0001', 'ht-vit-naive-tsk-lr0_0001', 'ht-vit-er-tsk-lr0_0001', 'ht-vit-gem-tsk-lr1e-05', 'ht-vit-lwf-tsk-lr0_0001', 'ht-vit-ewc-tsk-lr0_0001']\n",
    "# exps_dis = ['MT', 'Finetune', 'ER', 'GEM', 'LwF', 'EWC',\n",
    "#             'MT*', 'Finetune*', 'ER*', 'GEM*', 'LwF*', 'EWC*']\n",
    "\n",
    "'''concept'''\n",
    "# exps = [\n",
    "#         f'concept-concept-tsk_True-lr0_01-w1',\n",
    "#         f'concept-concept-tsk_False-lr0_001-w0_5',\n",
    "#     ]\n",
    "# exps_dis = ['Task-IL', 'Class-IL']\n",
    "\n",
    "'''Multi Task'''\n",
    "# exps = [\n",
    "#     f'MT-naive-tsk_{return_task_id}-lr{learning_rate}'\n",
    "#     for return_task_id in [True, False]\n",
    "#     for learning_rate in ['0_0001', '0_0005', '0_001', '0_005', '0_01', '0_05', '0_1']\n",
    "#     ]\n",
    "# exps_dis = [\n",
    "#     f\"{'Task' if return_task_id else 'Class'} lr{learning_rate}\"\n",
    "#     for return_task_id in [True, False]\n",
    "#     for learning_rate in ['0.0001', '0.0005', '0.001', '0.005', '0.01', '0.05', '0.1']\n",
    "# ]\n",
    "\n",
    "'''Random model'''\n",
    "# exps = [\n",
    "#     'random-naive-tsk_False'\n",
    "#     ]\n",
    "# exps_dis = [\n",
    "#     'random'\n",
    "# ]\n",
    "\n",
    "'''agem'''\n",
    "# exps = [\n",
    "#     *[f'HT-agem-tsk_{return_task_id}-lr{lr}'\n",
    "#       for return_task_id in [False, True]\n",
    "#       for lr in [\n",
    "#           (lambda x: str(x).replace('.', '_')) (x)\n",
    "#           for x in np.around(np.logspace(-4, -1, num=8), decimals=4).tolist()\n",
    "#       ]],\n",
    "#     ]\n",
    "# exps_dis = [\n",
    "#     *[f\"agem {'Task' if return_task_id else 'Class'} lr {lr}\"\n",
    "#       for return_task_id in [False, True] for lr in np.around(np.logspace(-4, -1, num=8), decimals=4).tolist()],\n",
    "# ]\n",
    "\n",
    "collected_data = []\n",
    "for exp_idx, exp_name in enumerate(exps):\n",
    "    print(exp_name)\n",
    "    for mode in modes:\n",
    "        print(mode)\n",
    "        if mode == 'forgetting':\n",
    "            accs = get_accs(exp_name, mode, project_name=project_name, index='' if 'MT' not in exp_name else '', name='StreamForgetting/eval_phase/test_stream')\n",
    "            print(accs)\n",
    "        else:\n",
    "            accs = get_accs(exp_name, mode, project_name=project_name, index='' if 'MT' not in exp_name else '', test_n_way=10 if 'MT' in exp_name else None)\n",
    "        print(accs)\n",
    "        collected_data.append(pd.DataFrame({'Method': exps_dis[exp_idx], 'Phase': mode, 'Accuracy': accs}))\n",
    "\n",
    "collected_data = pd.concat(collected_data, ignore_index=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{llllll}\n",
      "{Phase} & {continual} & {sys} & {pro} & {non} & {noc} \\\\\n",
      "{Method} & {} & {} & {} & {} & {} \\\\\n",
      "Task lr1e-05 & 0.00 & 71.13 $\\pm$ 1.60 & 72.87 $\\pm$ 1.61 & 0.00 $\\pm$ nan & 0.00 $\\pm$ nan \\\\\n",
      "Task lr1e-4 & 0.00 & 71.27 $\\pm$ 1.66 & 74.55 $\\pm$ 1.64 & 0.00 $\\pm$ nan & 0.00 $\\pm$ nan \\\\\n",
      "Task lr1e-3 & 0.00 & 70.87 $\\pm$ 1.71 & 74.08 $\\pm$ 1.75 & 0.00 $\\pm$ nan & 0.00 $\\pm$ nan \\\\\n",
      "Task lr1e-2 & 0.00 & 66.75 $\\pm$ 1.54 & 70.23 $\\pm$ 1.71 & 0.00 $\\pm$ nan & 0.00 $\\pm$ nan \\\\\n",
      "Class lr1e-05 & 0.00 & 73.20 $\\pm$ 1.67 & 75.92 $\\pm$ 1.63 & 0.00 $\\pm$ nan & 0.00 $\\pm$ nan \\\\\n",
      "Class lr1e-4 & 0.00 & 76.93 $\\pm$ 1.49 & 79.40 $\\pm$ 1.68 & 0.00 $\\pm$ nan & 0.00 $\\pm$ nan \\\\\n",
      "Class lr1e-3 & 0.00 & 81.78 $\\pm$ 1.45 & 84.62 $\\pm$ 1.40 & 0.00 $\\pm$ nan & 0.00 $\\pm$ nan \\\\\n",
      "Class lr1e-2 & 0.00 & 77.60 $\\pm$ 1.56 & 81.28 $\\pm$ 1.57 & 0.00 $\\pm$ nan & 0.00 $\\pm$ nan \\\\\n",
      "\\end{tabular}\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": "Phase         continual            sys            pro          non  \\\nMethod                                                               \nTask lr1e-05       0.00  71.13 +- 1.60  72.87 +- 1.61  0.00 +- nan   \nTask lr1e-4        0.00  71.27 +- 1.66  74.55 +- 1.64  0.00 +- nan   \nTask lr1e-3        0.00  70.87 +- 1.71  74.08 +- 1.75  0.00 +- nan   \nTask lr1e-2        0.00  66.75 +- 1.54  70.23 +- 1.71  0.00 +- nan   \nClass lr1e-05      0.00  73.20 +- 1.67  75.92 +- 1.63  0.00 +- nan   \nClass lr1e-4       0.00  76.93 +- 1.49  79.40 +- 1.68  0.00 +- nan   \nClass lr1e-3       0.00  81.78 +- 1.45  84.62 +- 1.40  0.00 +- nan   \nClass lr1e-2       0.00  77.60 +- 1.56  81.28 +- 1.57  0.00 +- nan   \n\nPhase                  noc  \nMethod                      \nTask lr1e-05   0.00 +- nan  \nTask lr1e-4    0.00 +- nan  \nTask lr1e-3    0.00 +- nan  \nTask lr1e-2    0.00 +- nan  \nClass lr1e-05  0.00 +- nan  \nClass lr1e-4   0.00 +- nan  \nClass lr1e-3   0.00 +- nan  \nClass lr1e-2   0.00 +- nan  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th>Phase</th>\n      <th>continual</th>\n      <th>sys</th>\n      <th>pro</th>\n      <th>non</th>\n      <th>noc</th>\n    </tr>\n    <tr>\n      <th>Method</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>Task lr1e-05</th>\n      <td>0.00</td>\n      <td>71.13 +- 1.60</td>\n      <td>72.87 +- 1.61</td>\n      <td>0.00 +- nan</td>\n      <td>0.00 +- nan</td>\n    </tr>\n    <tr>\n      <th>Task lr1e-4</th>\n      <td>0.00</td>\n      <td>71.27 +- 1.66</td>\n      <td>74.55 +- 1.64</td>\n      <td>0.00 +- nan</td>\n      <td>0.00 +- nan</td>\n    </tr>\n    <tr>\n      <th>Task lr1e-3</th>\n      <td>0.00</td>\n      <td>70.87 +- 1.71</td>\n      <td>74.08 +- 1.75</td>\n      <td>0.00 +- nan</td>\n      <td>0.00 +- nan</td>\n    </tr>\n    <tr>\n      <th>Task lr1e-2</th>\n      <td>0.00</td>\n      <td>66.75 +- 1.54</td>\n      <td>70.23 +- 1.71</td>\n      <td>0.00 +- nan</td>\n      <td>0.00 +- nan</td>\n    </tr>\n    <tr>\n      <th>Class lr1e-05</th>\n      <td>0.00</td>\n      <td>73.20 +- 1.67</td>\n      <td>75.92 +- 1.63</td>\n      <td>0.00 +- nan</td>\n      <td>0.00 +- nan</td>\n    </tr>\n    <tr>\n      <th>Class lr1e-4</th>\n      <td>0.00</td>\n      <td>76.93 +- 1.49</td>\n      <td>79.40 +- 1.68</td>\n      <td>0.00 +- nan</td>\n      <td>0.00 +- nan</td>\n    </tr>\n    <tr>\n      <th>Class lr1e-3</th>\n      <td>0.00</td>\n      <td>81.78 +- 1.45</td>\n      <td>84.62 +- 1.40</td>\n      <td>0.00 +- nan</td>\n      <td>0.00 +- nan</td>\n    </tr>\n    <tr>\n      <th>Class lr1e-2</th>\n      <td>0.00</td>\n      <td>77.60 +- 1.56</td>\n      <td>81.28 +- 1.57</td>\n      <td>0.00 +- nan</td>\n      <td>0.00 +- nan</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''cal mean and ci95, then formulate table str-cell and to latex.'''\n",
    "# exp = 'Finetune'\n",
    "# mode = 'continual'\n",
    "# acc_list = collected_data[(collected_data['Method'] == exp) & (collected_data['Phase'] == mode)]['Accuracy']\n",
    "data = []\n",
    "for exp in exps_dis:\n",
    "    for mode in modes:\n",
    "        acc_list = collected_data[(collected_data['Method'] == exp) & (collected_data['Phase'] == mode)]['Accuracy']\n",
    "        mean = acc_list.mean()\n",
    "        ci95 = 1.96 * (acc_list.std()/np.sqrt(len(acc_list)))\n",
    "        acc_str = f'{mean*100:.2f} +- {ci95*100:.2f}' if mode != 'continual' else f'{mean*100:.2f}'\n",
    "        data.append(pd.DataFrame({'Method': exp, 'Phase': mode, 'mean': mean, 'ci95': ci95, 'str': acc_str}, index=[0]))\n",
    "data = pd.concat(data, ignore_index=True)\n",
    "# print(data)\n",
    "\n",
    "# to latex\n",
    "\n",
    "data = data.pivot(index='Method', columns='Phase', values='str')\n",
    "data = data[modes]\n",
    "data = data.reindex(exps_dis)\n",
    "print(data.style.to_latex().replace('+-', '$\\\\pm$'))\n",
    "data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\anaconda3\\envs\\avalanche\\lib\\site-packages\\ipykernel_launcher.py:14: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  \n",
      "C:\\Users\\ASUS\\anaconda3\\envs\\avalanche\\lib\\site-packages\\ipykernel_launcher.py:14: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{lllllllllllll}\n",
      "{Phase} & {continual} & {continual_score} & {sys} & {sys_score} & {pro} & {pro_score} & {sub} & {sub_score} & {non} & {non_score} & {noc} & {noc_score} \\\\\n",
      "{Method} & {} & {} & {} & {} & {} & {} & {} & {} & {} & {} & {} & {} \\\\\n",
      "Task lr0.0001 & 89.67 & +5.84 & 79.36 $\\pm$ 0.76 & -6.33 & 79.16 $\\pm$ 0.79 & -6.56 & 64.07 $\\pm$ 0.79 & -24.38 & 84.72 $\\pm$ 0.63 & +0.00 & 37.38 $\\pm$ 0.96 & -55.88 \\\\\n",
      "Task lr0.0005 & 0.00 & +nan & 0.00 $\\pm$ nan & +nan & 0.00 $\\pm$ nan & +nan & 0.00 $\\pm$ nan & +nan & 0.00 $\\pm$ nan & +nan & 0.00 $\\pm$ nan & +nan \\\\\n",
      "Task lr0.001 & 92.17 & +6.63 & 82.16 $\\pm$ 0.68 & -4.95 & 84.22 $\\pm$ 0.60 & -2.56 & 71.82 $\\pm$ 0.75 & -16.91 & 86.44 $\\pm$ 0.69 & +0.00 & 44.07 $\\pm$ 0.95 & -49.01 \\\\\n",
      "Task lr0.005 & 91.08 & +12.05 & 76.28 $\\pm$ 0.84 & -6.15 & 77.07 $\\pm$ 0.93 & -5.18 & 65.41 $\\pm$ 0.85 & -19.53 & 81.28 $\\pm$ 0.75 & +0.00 & 39.72 $\\pm$ 0.95 & -51.14 \\\\\n",
      "Task lr0.01 & 89.14 & +18.88 & 68.97 $\\pm$ 1.12 & -8.02 & 66.65 $\\pm$ 1.27 & -11.11 & 58.87 $\\pm$ 1.10 & -21.49 & 74.98 $\\pm$ 0.88 & +0.00 & 34.59 $\\pm$ 1.20 & -53.87 \\\\\n",
      "Task lr0.05 & 86.48 & +inf & 55.15 $\\pm$ 1.09 & +inf & 49.63 $\\pm$ 1.16 & +inf & 0.00 $\\pm$ nan & +nan & 0.00 $\\pm$ nan & +nan & 0.00 $\\pm$ nan & +nan \\\\\n",
      "Task lr0.1 & 80.34 & +454.20 & 13.43 $\\pm$ 0.75 & -7.34 & 12.90 $\\pm$ 0.62 & -11.01 & 12.27 $\\pm$ 0.64 & -15.36 & 14.50 $\\pm$ 0.99 & +0.00 & 11.02 $\\pm$ 0.39 & -23.98 \\\\\n",
      "Class lr0.0001 & 71.48 & -21.13 & 86.75 $\\pm$ 0.60 & -4.28 & 83.26 $\\pm$ 0.67 & -8.13 & 65.11 $\\pm$ 0.71 & -28.15 & 90.63 $\\pm$ 0.53 & +0.00 & 35.72 $\\pm$ 1.06 & -60.59 \\\\\n",
      "Class lr0.0005 & 83.30 & -11.78 & 92.09 $\\pm$ 0.47 & -2.47 & 92.62 $\\pm$ 0.43 & -1.91 & 73.26 $\\pm$ 0.69 & -22.41 & 94.42 $\\pm$ 0.37 & +0.00 & 42.87 $\\pm$ 0.89 & -54.60 \\\\\n",
      "Class lr0.001 & 82.39 & -11.31 & 89.36 $\\pm$ 0.59 & -3.81 & 86.74 $\\pm$ 0.55 & -6.63 & 72.38 $\\pm$ 0.67 & -22.08 & 92.90 $\\pm$ 0.40 & +0.00 & 42.80 $\\pm$ 0.76 & -53.92 \\\\\n",
      "Class lr0.005 & 83.61 & -8.68 & 88.14 $\\pm$ 0.62 & -3.73 & 85.94 $\\pm$ 0.65 & -6.13 & 69.67 $\\pm$ 0.75 & -23.90 & 91.55 $\\pm$ 0.51 & +0.00 & 40.04 $\\pm$ 0.99 & -56.27 \\\\\n",
      "Class lr0.01 & 78.24 & -8.10 & 80.58 $\\pm$ 0.92 & -5.35 & 75.60 $\\pm$ 1.23 & -11.20 & 63.06 $\\pm$ 1.08 & -25.93 & 85.14 $\\pm$ 0.65 & +0.00 & 31.08 $\\pm$ 1.33 & -63.49 \\\\\n",
      "Class lr0.05 & 78.68 & +72.90 & 42.36 $\\pm$ 2.63 & -6.91 & 25.14 $\\pm$ 1.87 & -44.76 & 26.38 $\\pm$ 1.65 & -42.02 & 45.51 $\\pm$ 2.79 & +0.00 & 12.55 $\\pm$ 0.61 & -72.41 \\\\\n",
      "Class lr0.1 & 0.00 & +nan & 0.00 $\\pm$ nan & +nan & 0.00 $\\pm$ nan & +nan & 0.00 $\\pm$ nan & +nan & 0.00 $\\pm$ nan & +nan & 0.00 $\\pm$ nan & +nan \\\\\n",
      "\\end{tabular}\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": "Phase         continual continual_score            sys sys_score  \\\nMethod                                                             \nTask lr0.0001     89.67           +5.84  79.36 +- 0.76     -6.33   \nTask lr0.0005      0.00            +nan    0.00 +- nan      +nan   \nTask lr0.001      92.17           +6.63  82.16 +- 0.68     -4.95   \nTask lr0.005      91.08          +12.05  76.28 +- 0.84     -6.15   \nTask lr0.01       89.14          +18.88  68.97 +- 1.12     -8.02   \n\nPhase                    pro pro_score            sub sub_score  \\\nMethod                                                            \nTask lr0.0001  79.16 +- 0.79     -6.56  64.07 +- 0.79    -24.38   \nTask lr0.0005    0.00 +- nan      +nan    0.00 +- nan      +nan   \nTask lr0.001   84.22 +- 0.60     -2.56  71.82 +- 0.75    -16.91   \nTask lr0.005   77.07 +- 0.93     -5.18  65.41 +- 0.85    -19.53   \nTask lr0.01    66.65 +- 1.27    -11.11  58.87 +- 1.10    -21.49   \n\nPhase                    non non_score            noc noc_score  \nMethod                                                           \nTask lr0.0001  84.72 +- 0.63     +0.00  37.38 +- 0.96    -55.88  \nTask lr0.0005    0.00 +- nan      +nan    0.00 +- nan      +nan  \nTask lr0.001   86.44 +- 0.69     +0.00  44.07 +- 0.95    -49.01  \nTask lr0.005   81.28 +- 0.75     +0.00  39.72 +- 0.95    -51.14  \nTask lr0.01    74.98 +- 0.88     +0.00  34.59 +- 1.20    -53.87  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th>Phase</th>\n      <th>continual</th>\n      <th>continual_score</th>\n      <th>sys</th>\n      <th>sys_score</th>\n      <th>pro</th>\n      <th>pro_score</th>\n      <th>sub</th>\n      <th>sub_score</th>\n      <th>non</th>\n      <th>non_score</th>\n      <th>noc</th>\n      <th>noc_score</th>\n    </tr>\n    <tr>\n      <th>Method</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>Task lr0.0001</th>\n      <td>89.67</td>\n      <td>+5.84</td>\n      <td>79.36 +- 0.76</td>\n      <td>-6.33</td>\n      <td>79.16 +- 0.79</td>\n      <td>-6.56</td>\n      <td>64.07 +- 0.79</td>\n      <td>-24.38</td>\n      <td>84.72 +- 0.63</td>\n      <td>+0.00</td>\n      <td>37.38 +- 0.96</td>\n      <td>-55.88</td>\n    </tr>\n    <tr>\n      <th>Task lr0.0005</th>\n      <td>0.00</td>\n      <td>+nan</td>\n      <td>0.00 +- nan</td>\n      <td>+nan</td>\n      <td>0.00 +- nan</td>\n      <td>+nan</td>\n      <td>0.00 +- nan</td>\n      <td>+nan</td>\n      <td>0.00 +- nan</td>\n      <td>+nan</td>\n      <td>0.00 +- nan</td>\n      <td>+nan</td>\n    </tr>\n    <tr>\n      <th>Task lr0.001</th>\n      <td>92.17</td>\n      <td>+6.63</td>\n      <td>82.16 +- 0.68</td>\n      <td>-4.95</td>\n      <td>84.22 +- 0.60</td>\n      <td>-2.56</td>\n      <td>71.82 +- 0.75</td>\n      <td>-16.91</td>\n      <td>86.44 +- 0.69</td>\n      <td>+0.00</td>\n      <td>44.07 +- 0.95</td>\n      <td>-49.01</td>\n    </tr>\n    <tr>\n      <th>Task lr0.005</th>\n      <td>91.08</td>\n      <td>+12.05</td>\n      <td>76.28 +- 0.84</td>\n      <td>-6.15</td>\n      <td>77.07 +- 0.93</td>\n      <td>-5.18</td>\n      <td>65.41 +- 0.85</td>\n      <td>-19.53</td>\n      <td>81.28 +- 0.75</td>\n      <td>+0.00</td>\n      <td>39.72 +- 0.95</td>\n      <td>-51.14</td>\n    </tr>\n    <tr>\n      <th>Task lr0.01</th>\n      <td>89.14</td>\n      <td>+18.88</td>\n      <td>68.97 +- 1.12</td>\n      <td>-8.02</td>\n      <td>66.65 +- 1.27</td>\n      <td>-11.11</td>\n      <td>58.87 +- 1.10</td>\n      <td>-21.49</td>\n      <td>74.98 +- 0.88</td>\n      <td>+0.00</td>\n      <td>34.59 +- 1.20</td>\n      <td>-53.87</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''cal mean and ci95 and relative score, then formulate table str-cell and to latex.'''\n",
    "data = []\n",
    "for exp in exps_dis:\n",
    "    mean_dict = {}\n",
    "    for mode in modes:\n",
    "        acc_list = collected_data[(collected_data['Method'] == exp) & (collected_data['Phase'] == mode)]['Accuracy']\n",
    "        mean = acc_list.mean()\n",
    "        mean_dict[mode] = mean\n",
    "        ci95 = 1.96 * (acc_list.std()/np.sqrt(len(acc_list)))\n",
    "        acc_str = f'{mean*100:.2f} +- {ci95*100:.2f}' if mode != 'continual' else f'{mean*100:.2f}'\n",
    "        data.append(pd.DataFrame({'Method': exp, 'Phase': mode, 'mean': mean, 'ci95': ci95, 'str': acc_str}, index=[0]))\n",
    "\n",
    "    for mode, mean in mean_dict.items():\n",
    "        score = (mean - mean_dict['non']) / mean_dict['non']\n",
    "        data.append(pd.DataFrame({'Method': exp, 'Phase': f'{mode}_score', 'str': f'{score*100:+.2f}'}, index=[0]))\n",
    "\n",
    "    # todo: try Harmonic mean\n",
    "\n",
    "data = pd.concat(data, ignore_index=True)\n",
    "# print(data)\n",
    "\n",
    "# to latex\n",
    "\n",
    "data = data.pivot(index='Method', values='str', columns='Phase')\n",
    "data = data[['continual', 'continual_score', 'sys', 'sys_score', 'pro', 'pro_score', 'sub', 'sub_score', 'non', 'non_score',  'noc', 'noc_score']]\n",
    "data = data.reindex(exps_dis)\n",
    "print(data.style.to_latex().replace('+-', '$\\\\pm$'))\n",
    "data.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{llllllllll}\n",
      "{Phase} & {continual} & {sys} & {pro} & {sub} & {Hn} & {non} & {noc} & {Hr} & {Ha} \\\\\n",
      "{Method} & {} & {} & {} & {} & {} & {} & {} & {} & {} \\\\\n",
      "Finetune & 8.38 & 64.73 $\\pm$ 0.78 & 65.43 $\\pm$ 0.73 & 61.26 $\\pm$ 0.67 & 63.75 & 68.54 $\\pm$ 0.80 & 40.32 $\\pm$ 0.72 & 50.77 & 57.84 \\\\\n",
      "ER & 19.78 & 71.38 $\\pm$ 0.75 & 70.11 $\\pm$ 0.64 & 64.32 $\\pm$ 0.69 & 68.46 & 77.27 $\\pm$ 0.67 & 40.98 $\\pm$ 0.72 & 53.56 & 61.60 \\\\\n",
      "GEM & 8.56 & 66.56 $\\pm$ 0.77 & 73.47 $\\pm$ 0.61 & 62.80 $\\pm$ 0.71 & 67.33 & 72.65 $\\pm$ 0.73 & 41.64 $\\pm$ 0.72 & 52.94 & 60.72 \\\\\n",
      "LwF & 9.11 & 71.22 $\\pm$ 0.74 & 73.28 $\\pm$ 0.61 & 68.74 $\\pm$ 0.66 & 71.03 & 76.56 $\\pm$ 0.66 & 48.69 $\\pm$ 0.77 & 59.52 & 65.93 \\\\\n",
      "EWC & 8.22 & 64.99 $\\pm$ 0.78 & 73.47 $\\pm$ 0.64 & 63.25 $\\pm$ 0.66 & 66.95 & 69.03 $\\pm$ 0.74 & 41.38 $\\pm$ 0.71 & 51.75 & 59.91 \\\\\n",
      "Finetune* & 72.46 & 70.32 $\\pm$ 0.73 & 72.62 $\\pm$ 0.63 & 66.33 $\\pm$ 0.69 & 69.66 & 75.32 $\\pm$ 0.70 & 43.26 $\\pm$ 0.73 & 54.95 & 62.92 \\\\\n",
      "ER* & 76.05 & 71.37 $\\pm$ 0.70 & 72.67 $\\pm$ 0.69 & 66.80 $\\pm$ 0.63 & 70.19 & 76.28 $\\pm$ 0.66 & 45.61 $\\pm$ 0.77 & 57.09 & 64.29 \\\\\n",
      "GEM* & 21.60 & 69.44 $\\pm$ 0.75 & 73.14 $\\pm$ 0.67 & 66.61 $\\pm$ 0.66 & 69.63 & 73.31 $\\pm$ 0.71 & 45.97 $\\pm$ 0.74 & 56.51 & 63.71 \\\\\n",
      "LwF* & 73.19 & 69.61 $\\pm$ 0.77 & 74.22 $\\pm$ 0.63 & 65.00 $\\pm$ 0.65 & 69.40 & 75.25 $\\pm$ 0.69 & 42.52 $\\pm$ 0.72 & 54.34 & 62.48 \\\\\n",
      "EWC* & 71.10 & 69.40 $\\pm$ 0.79 & 72.37 $\\pm$ 0.65 & 65.10 $\\pm$ 0.67 & 68.83 & 74.99 $\\pm$ 0.72 & 42.82 $\\pm$ 0.77 & 54.51 & 62.29 \\\\\n",
      "\\end{tabular}\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": "Phase     continual            sys            pro            sub     Hn  \\\nMethod                                                                    \nFinetune       8.38  64.73 +- 0.78  65.43 +- 0.73  61.26 +- 0.67  63.75   \nER            19.78  71.38 +- 0.75  70.11 +- 0.64  64.32 +- 0.69  68.46   \nGEM            8.56  66.56 +- 0.77  73.47 +- 0.61  62.80 +- 0.71  67.33   \nLwF            9.11  71.22 +- 0.74  73.28 +- 0.61  68.74 +- 0.66  71.03   \nEWC            8.22  64.99 +- 0.78  73.47 +- 0.64  63.25 +- 0.66  66.95   \nFinetune*     72.46  70.32 +- 0.73  72.62 +- 0.63  66.33 +- 0.69  69.66   \nER*           76.05  71.37 +- 0.70  72.67 +- 0.69  66.80 +- 0.63  70.19   \nGEM*          21.60  69.44 +- 0.75  73.14 +- 0.67  66.61 +- 0.66  69.63   \nLwF*          73.19  69.61 +- 0.77  74.22 +- 0.63  65.00 +- 0.65  69.40   \nEWC*          71.10  69.40 +- 0.79  72.37 +- 0.65  65.10 +- 0.67  68.83   \n\nPhase                non            noc     Hr     Ha  \nMethod                                                 \nFinetune   68.54 +- 0.80  40.32 +- 0.72  50.77  57.84  \nER         77.27 +- 0.67  40.98 +- 0.72  53.56  61.60  \nGEM        72.65 +- 0.73  41.64 +- 0.72  52.94  60.72  \nLwF        76.56 +- 0.66  48.69 +- 0.77  59.52  65.93  \nEWC        69.03 +- 0.74  41.38 +- 0.71  51.75  59.91  \nFinetune*  75.32 +- 0.70  43.26 +- 0.73  54.95  62.92  \nER*        76.28 +- 0.66  45.61 +- 0.77  57.09  64.29  \nGEM*       73.31 +- 0.71  45.97 +- 0.74  56.51  63.71  \nLwF*       75.25 +- 0.69  42.52 +- 0.72  54.34  62.48  \nEWC*       74.99 +- 0.72  42.82 +- 0.77  54.51  62.29  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th>Phase</th>\n      <th>continual</th>\n      <th>sys</th>\n      <th>pro</th>\n      <th>sub</th>\n      <th>Hn</th>\n      <th>non</th>\n      <th>noc</th>\n      <th>Hr</th>\n      <th>Ha</th>\n    </tr>\n    <tr>\n      <th>Method</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>Finetune</th>\n      <td>8.38</td>\n      <td>64.73 +- 0.78</td>\n      <td>65.43 +- 0.73</td>\n      <td>61.26 +- 0.67</td>\n      <td>63.75</td>\n      <td>68.54 +- 0.80</td>\n      <td>40.32 +- 0.72</td>\n      <td>50.77</td>\n      <td>57.84</td>\n    </tr>\n    <tr>\n      <th>ER</th>\n      <td>19.78</td>\n      <td>71.38 +- 0.75</td>\n      <td>70.11 +- 0.64</td>\n      <td>64.32 +- 0.69</td>\n      <td>68.46</td>\n      <td>77.27 +- 0.67</td>\n      <td>40.98 +- 0.72</td>\n      <td>53.56</td>\n      <td>61.60</td>\n    </tr>\n    <tr>\n      <th>GEM</th>\n      <td>8.56</td>\n      <td>66.56 +- 0.77</td>\n      <td>73.47 +- 0.61</td>\n      <td>62.80 +- 0.71</td>\n      <td>67.33</td>\n      <td>72.65 +- 0.73</td>\n      <td>41.64 +- 0.72</td>\n      <td>52.94</td>\n      <td>60.72</td>\n    </tr>\n    <tr>\n      <th>LwF</th>\n      <td>9.11</td>\n      <td>71.22 +- 0.74</td>\n      <td>73.28 +- 0.61</td>\n      <td>68.74 +- 0.66</td>\n      <td>71.03</td>\n      <td>76.56 +- 0.66</td>\n      <td>48.69 +- 0.77</td>\n      <td>59.52</td>\n      <td>65.93</td>\n    </tr>\n    <tr>\n      <th>EWC</th>\n      <td>8.22</td>\n      <td>64.99 +- 0.78</td>\n      <td>73.47 +- 0.64</td>\n      <td>63.25 +- 0.66</td>\n      <td>66.95</td>\n      <td>69.03 +- 0.74</td>\n      <td>41.38 +- 0.71</td>\n      <td>51.75</td>\n      <td>59.91</td>\n    </tr>\n    <tr>\n      <th>Finetune*</th>\n      <td>72.46</td>\n      <td>70.32 +- 0.73</td>\n      <td>72.62 +- 0.63</td>\n      <td>66.33 +- 0.69</td>\n      <td>69.66</td>\n      <td>75.32 +- 0.70</td>\n      <td>43.26 +- 0.73</td>\n      <td>54.95</td>\n      <td>62.92</td>\n    </tr>\n    <tr>\n      <th>ER*</th>\n      <td>76.05</td>\n      <td>71.37 +- 0.70</td>\n      <td>72.67 +- 0.69</td>\n      <td>66.80 +- 0.63</td>\n      <td>70.19</td>\n      <td>76.28 +- 0.66</td>\n      <td>45.61 +- 0.77</td>\n      <td>57.09</td>\n      <td>64.29</td>\n    </tr>\n    <tr>\n      <th>GEM*</th>\n      <td>21.60</td>\n      <td>69.44 +- 0.75</td>\n      <td>73.14 +- 0.67</td>\n      <td>66.61 +- 0.66</td>\n      <td>69.63</td>\n      <td>73.31 +- 0.71</td>\n      <td>45.97 +- 0.74</td>\n      <td>56.51</td>\n      <td>63.71</td>\n    </tr>\n    <tr>\n      <th>LwF*</th>\n      <td>73.19</td>\n      <td>69.61 +- 0.77</td>\n      <td>74.22 +- 0.63</td>\n      <td>65.00 +- 0.65</td>\n      <td>69.40</td>\n      <td>75.25 +- 0.69</td>\n      <td>42.52 +- 0.72</td>\n      <td>54.34</td>\n      <td>62.48</td>\n    </tr>\n    <tr>\n      <th>EWC*</th>\n      <td>71.10</td>\n      <td>69.40 +- 0.79</td>\n      <td>72.37 +- 0.65</td>\n      <td>65.10 +- 0.67</td>\n      <td>68.83</td>\n      <td>74.99 +- 0.72</td>\n      <td>42.82 +- 0.77</td>\n      <td>54.51</td>\n      <td>62.29</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''todo: cal Harmonic mean (among sys, pro, sub) and Harmonic mean (between non, noc)'''\n",
    "data = []\n",
    "for exp in exps_dis:\n",
    "    mean_dict = {}\n",
    "    for mode in modes:\n",
    "        acc_list = collected_data[(collected_data['Method'] == exp) & (collected_data['Phase'] == mode)]['Accuracy']\n",
    "        mean = acc_list.mean()\n",
    "        mean_dict[mode] = mean\n",
    "        ci95 = 1.96 * (acc_list.std()/np.sqrt(len(acc_list)))\n",
    "        acc_str = f'{mean*100:.2f} +- {ci95*100:.2f}' if mode not in ['continual', 'forgetting'] else f'{mean*100:.2f}'\n",
    "        data.append(pd.DataFrame({'Method': exp, 'Phase': mode, 'mean': mean, 'ci95': ci95, 'str': acc_str}, index=[0]))\n",
    "\n",
    "    # 3x1*x2*x3/(x1*x2+x1*x3+x2*x3)\n",
    "    hm_nov = 3 * mean_dict['sys'] * mean_dict['pro'] * mean_dict['sub'] / (mean_dict['sys'] * mean_dict['pro'] + mean_dict['sys'] * mean_dict['sub'] + mean_dict['pro'] * mean_dict['sub'])\n",
    "    data.append(pd.DataFrame({'Method': exp, 'Phase': f'Hn', 'str': f'{hm_nov*100:.2f}'}, index=[0]))\n",
    "\n",
    "    # 2x1*x2/(x1+x2)\n",
    "    hm_ref = 2 * mean_dict['non'] * mean_dict['noc'] / (mean_dict['non'] + mean_dict['noc'])\n",
    "    data.append(pd.DataFrame({'Method': exp, 'Phase': f'Hr', 'str': f'{hm_ref*100:.2f}'}, index=[0]))\n",
    "\n",
    "    hm_all = 5 / (1/mean_dict['sys'] + 1/mean_dict['pro'] + 1/mean_dict['sub'] + 1/mean_dict['non'] + 1/mean_dict['noc'])\n",
    "    data.append(pd.DataFrame({'Method': exp, 'Phase': f'Ha', 'str': f'{hm_all*100:.2f}'}, index=[0]))\n",
    "\n",
    "data = pd.concat(data, ignore_index=True)\n",
    "# print(data)\n",
    "\n",
    "# to latex\n",
    "\n",
    "data = data.pivot(index='Method', values='str', columns='Phase')\n",
    "# data = data[['continual', 'forgetting', 'sys', 'pro', 'sub', 'Hn', 'non',  'noc', 'Hr', 'Ha']]\n",
    "data = data[['continual', 'sys', 'pro', 'sub', 'Hn', 'non',  'noc', 'Hr', 'Ha']]\n",
    "data = data.reindex(exps_dis)\n",
    "print(data.style.to_latex().replace('+-', '$\\\\pm$'))\n",
    "data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## COBJ"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HT-vit-3tasks-MT-naive-tsk_True-lr5e-05\n",
      "continual\n",
      "sys\n",
      "pro\n",
      "non\n",
      "noc\n",
      "HT-vit-3tasks-MT-naive-tsk_False-lr5e-05\n",
      "continual\n",
      "sys\n",
      "pro\n",
      "non\n",
      "noc\n",
      "HT-vit-3tasks-naive-tsk_True-lr5e-05\n",
      "continual\n",
      "sys\n",
      "pro\n",
      "non\n",
      "noc\n",
      "HT-vit-3tasks-naive-tsk_False-lr5e-05\n",
      "continual\n",
      "sys\n",
      "pro\n",
      "non\n",
      "noc\n",
      "HT-vit-3tasks-er-tsk_True-lr5e-05\n",
      "continual\n",
      "sys\n",
      "pro\n",
      "non\n",
      "noc\n",
      "HT-vit-3tasks-er-tsk_False-lr5e-05\n",
      "continual\n",
      "sys\n",
      "pro\n",
      "non\n",
      "noc\n",
      "HT-vit-3tasks-gem-tsk_True-lr5e-05-p32-m0_3\n",
      "continual\n",
      "sys\n",
      "pro\n",
      "non\n",
      "noc\n",
      "HT-vit-3tasks-gem-tsk_False-lr0_0001-p32-m0_3\n",
      "continual\n",
      "sys\n",
      "pro\n",
      "non\n",
      "noc\n",
      "HT-vit-3tasks-lwf-tsk_True-lr5e-05-a1-t1\n",
      "continual\n",
      "sys\n",
      "pro\n",
      "non\n",
      "noc\n",
      "HT-vit-3tasks-lwf-tsk_False-lr0_0001-a1-t1\n",
      "continual\n",
      "sys\n",
      "pro\n",
      "non\n",
      "noc\n",
      "HT-vit-3tasks-ewc-tsk_True-lr0_0001-lambda2\n",
      "continual\n",
      "sys\n",
      "pro\n",
      "non\n",
      "noc\n",
      "HT-vit-3tasks-ewc-tsk_False-lr0_0001-lambda2\n",
      "continual\n",
      "sys\n",
      "pro\n",
      "non\n",
      "noc\n"
     ]
    }
   ],
   "source": [
    "'''collect exps'''\n",
    "project_name = 'COBJ'\n",
    "modes = ['continual', 'sys', 'pro', 'non', 'noc']\n",
    "index = '-last'        # -last\n",
    "\n",
    "'''resnet18'''\n",
    "# exps = [\n",
    "#         # # 3 tasks 10-way\n",
    "#         # 'HT-MT-3tasks-naive-tsk_True-lr0_001', 'HT-MT-3tasks-naive-tsk_False-lr0_001',\n",
    "#         # 'HT-naive-tsk_True-lr0_001', 'HT-naive-tsk_False-lr0_001',\n",
    "#         # 'HT-er-tsk_True-lr0_01', 'HT-er-tsk_False-lr0_01',\n",
    "#         # 'HT-gem-tsk_True-lr0_001-p16-m0_3', 'HT-gem-tsk_False-lr0_001-p256-m0_00139',\n",
    "#         # 'HT-lwf-tsk_True-lr0_001-a1-t2', 'HT-lwf-tsk_False-lr0_001-a1-t1_52',\n",
    "#         # 'HT-ewc-tsk_True-lr0_01-lambda100', 'HT-ewc-tsk_False-lr0_00053-lambda10',\n",
    "#\n",
    "#         # # 10 tasks 3-way\n",
    "#         # 'HT-MT-naive-tsk_True-lr0_00231', 'HT-MT-naive-tsk_False-lr0_00123',\n",
    "#         # 'HT-10tasks-naive-tsk_True-lr1e-05', 'HT-10tasks-naive-tsk_False-lr0_001',\n",
    "#         # 'HT-10tasks-er-tsk_True-lr0_001', 'HT-10tasks-er-tsk_False-lr0_001',\n",
    "#         # 'HT-10tasks-gem-tsk_True-lr0_001-p16-m0_3', 'HT-10tasks-gem-tsk_False-lr0_001-p256-m0_00139',\n",
    "#         # 'HT-10tasks-lwf-tsk_True-lr1e-05-a1-t2', 'HT-10tasks-lwf-tsk_False-lr0_001-a1-t2',\n",
    "#         # 'HT-10tasks-ewc-tsk_True-lr1e-05-lambda100', 'HT-10tasks-ewc-tsk_False-lr0_01-lambda10',\n",
    "#\n",
    "#         # 5 tasks 6-way\n",
    "#         'HT-MT-5tasks-naive-tsk_True-lr0_001', 'HT-MT-5tasks-naive-tsk_False-lr0_001',\n",
    "#         'HT-5tasks-naive-tsk_True-lr0_01', 'HT-5tasks-naive-tsk_False-lr0_001',\n",
    "#         'HT-5tasks-er-tsk_True-lr0_001', 'HT-5tasks-er-tsk_False-lr0_001',\n",
    "#         'HT-5tasks-gem-tsk_True-lr0_01-p16-m0_3', 'HT-5tasks-gem-tsk_False-lr0_01-p256-m0_00139',\n",
    "#         'HT-5tasks-lwf-tsk_True-lr0_001-a1-t2', 'HT-5tasks-lwf-tsk_False-lr0_001-a1-t2',\n",
    "#         'HT-5tasks-ewc-tsk_True-lr0_001-lambda100', 'HT-5tasks-ewc-tsk_False-lr0_001-lambda10',\n",
    "# ]\n",
    "# exps_dis = ['MultiTask*', 'MultiTask', 'Finetune*', 'Finetune', 'ER*', 'ER', 'GEM*', 'GEM', 'LwF*', 'LwF', 'EWC*', 'EWC']\n",
    "\n",
    "'''vit'''\n",
    "exps = [\n",
    "        # 3 tasks 10-way\n",
    "        'HT-vit-3tasks-MT-naive-tsk_True-lr5e-05', 'HT-vit-3tasks-MT-naive-tsk_False-lr5e-05', # (sk, sk)\n",
    "        'HT-vit-3tasks-naive-tsk_True-lr5e-05', 'HT-vit-3tasks-naive-tsk_False-lr5e-05',    # (gc, sty)\n",
    "        'HT-vit-3tasks-er-tsk_True-lr5e-05', 'HT-vit-3tasks-er-tsk_False-lr5e-05',\n",
    "        'HT-vit-3tasks-gem-tsk_True-lr5e-05-p32-m0_3', 'HT-vit-3tasks-gem-tsk_False-lr0_0001-p32-m0_3',  # (gc, sty)\n",
    "        'HT-vit-3tasks-lwf-tsk_True-lr5e-05-a1-t1', 'HT-vit-3tasks-lwf-tsk_False-lr0_0001-a1-t1',\n",
    "        'HT-vit-3tasks-ewc-tsk_True-lr0_0001-lambda2', 'HT-vit-3tasks-ewc-tsk_False-lr0_0001-lambda2',   # (gc, sty)\n",
    "]\n",
    "exps_dis = ['MultiTask*', 'MultiTask', 'Finetune*', 'Finetune', 'ER*', 'ER', 'GEM*', 'GEM', 'LwF*', 'LwF', 'EWC*', 'EWC']\n",
    "\n",
    "\n",
    "'''Multi Task'''\n",
    "# exps = [\n",
    "#     f'HT-MT-naive-tsk_{return_task_id}-lr{learning_rate}'\n",
    "#     for return_task_id in [True, False]\n",
    "#     for learning_rate in [\n",
    "#         (lambda x: str(x).replace('.', '_')) (x)\n",
    "#         for x in np.around(np.logspace(-4, -1, num=12), decimals=5).tolist()]\n",
    "#     ]\n",
    "# exps_dis = [\n",
    "#     f\"{'Task' if return_task_id else 'Class'} lr{learning_rate}\"\n",
    "#     for return_task_id in [True, False]\n",
    "#     for learning_rate in np.around(np.logspace(-4, -1, num=12), decimals=5).tolist()\n",
    "# ]\n",
    "\n",
    "collected_data = []\n",
    "for exp_idx, exp_name in enumerate(exps):\n",
    "    print(exp_name)\n",
    "    for mode in modes:\n",
    "        print(mode)\n",
    "        accs = get_accs(exp_name, mode, project_name=project_name, index=index if 'MT' not in exp_name else '',\n",
    "                        test_n_way=10)\n",
    "        # print(accs)\n",
    "        collected_data.append(pd.DataFrame({'Method': exps_dis[exp_idx], 'Phase': mode, 'Accuracy': accs}))\n",
    "\n",
    "collected_data = pd.concat(collected_data, ignore_index=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{lllllllll}\n",
      "{Phase} & {continual} & {sys} & {pro} & {Hn} & {non} & {noc} & {Hr} & {Ha} \\\\\n",
      "{Method} & {} & {} & {} & {} & {} & {} & {} & {} \\\\\n",
      "MultiTask* & 46.40 & 41.78 $\\pm$ 0.67 & 35.95 $\\pm$ 0.72 & 38.64 & 53.19 $\\pm$ 0.84 & 35.23 $\\pm$ 0.68 & 42.39 & 40.43 \\\\\n",
      "MultiTask & 28.23 & 41.90 $\\pm$ 0.63 & 36.58 $\\pm$ 0.72 & 39.06 & 49.88 $\\pm$ 0.82 & 35.69 $\\pm$ 0.69 & 41.61 & 40.29 \\\\\n",
      "Finetune* & 41.97 & 37.53 $\\pm$ 0.66 & 33.49 $\\pm$ 0.71 & 35.39 & 41.45 $\\pm$ 0.77 & 34.94 $\\pm$ 0.67 & 37.92 & 36.61 \\\\\n",
      "Finetune & 13.93 & 37.26 $\\pm$ 0.59 & 33.08 $\\pm$ 0.71 & 35.05 & 41.76 $\\pm$ 0.74 & 33.63 $\\pm$ 0.67 & 37.25 & 36.12 \\\\\n",
      "ER* & 41.10 & 38.23 $\\pm$ 0.64 & 33.48 $\\pm$ 0.69 & 35.70 & 44.95 $\\pm$ 0.78 & 34.46 $\\pm$ 0.63 & 39.01 & 37.28 \\\\\n",
      "ER & 18.10 & 35.03 $\\pm$ 0.63 & 30.76 $\\pm$ 0.66 & 32.75 & 43.62 $\\pm$ 0.78 & 33.25 $\\pm$ 0.68 & 37.73 & 35.07 \\\\\n",
      "GEM* & 26.70 & 36.36 $\\pm$ 0.62 & 33.00 $\\pm$ 0.71 & 34.60 & 39.34 $\\pm$ 0.69 & 32.18 $\\pm$ 0.61 & 35.40 & 35.00 \\\\\n",
      "GEM & 13.43 & 34.29 $\\pm$ 0.59 & 31.56 $\\pm$ 0.67 & 32.87 & 38.96 $\\pm$ 0.68 & 31.50 $\\pm$ 0.66 & 34.84 & 33.82 \\\\\n",
      "LwF* & 43.90 & 40.30 $\\pm$ 0.64 & 34.70 $\\pm$ 0.75 & 37.29 & 44.83 $\\pm$ 0.79 & 35.50 $\\pm$ 0.71 & 39.62 & 38.42 \\\\\n",
      "LwF & 15.07 & 39.52 $\\pm$ 0.61 & 35.18 $\\pm$ 0.73 & 37.22 & 44.81 $\\pm$ 0.80 & 35.23 $\\pm$ 0.65 & 39.45 & 38.30 \\\\\n",
      "EWC* & 40.07 & 37.65 $\\pm$ 0.62 & 33.22 $\\pm$ 0.69 & 35.30 & 41.95 $\\pm$ 0.72 & 33.72 $\\pm$ 0.66 & 37.39 & 36.31 \\\\\n",
      "EWC & 14.67 & 36.36 $\\pm$ 0.62 & 32.12 $\\pm$ 0.66 & 34.11 & 40.30 $\\pm$ 0.70 & 33.17 $\\pm$ 0.68 & 36.39 & 35.21 \\\\\n",
      "\\end{tabular}\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": "Phase      continual            sys            pro     Hn            non  \\\nMethod                                                                     \nMultiTask*     46.40  41.78 +- 0.67  35.95 +- 0.72  38.64  53.19 +- 0.84   \nMultiTask      28.23  41.90 +- 0.63  36.58 +- 0.72  39.06  49.88 +- 0.82   \nFinetune*      41.97  37.53 +- 0.66  33.49 +- 0.71  35.39  41.45 +- 0.77   \nFinetune       13.93  37.26 +- 0.59  33.08 +- 0.71  35.05  41.76 +- 0.74   \nER*            41.10  38.23 +- 0.64  33.48 +- 0.69  35.70  44.95 +- 0.78   \nER             18.10  35.03 +- 0.63  30.76 +- 0.66  32.75  43.62 +- 0.78   \nGEM*           26.70  36.36 +- 0.62  33.00 +- 0.71  34.60  39.34 +- 0.69   \nGEM            13.43  34.29 +- 0.59  31.56 +- 0.67  32.87  38.96 +- 0.68   \nLwF*           43.90  40.30 +- 0.64  34.70 +- 0.75  37.29  44.83 +- 0.79   \nLwF            15.07  39.52 +- 0.61  35.18 +- 0.73  37.22  44.81 +- 0.80   \nEWC*           40.07  37.65 +- 0.62  33.22 +- 0.69  35.30  41.95 +- 0.72   \nEWC            14.67  36.36 +- 0.62  32.12 +- 0.66  34.11  40.30 +- 0.70   \n\nPhase                 noc     Hr     Ha  \nMethod                                   \nMultiTask*  35.23 +- 0.68  42.39  40.43  \nMultiTask   35.69 +- 0.69  41.61  40.29  \nFinetune*   34.94 +- 0.67  37.92  36.61  \nFinetune    33.63 +- 0.67  37.25  36.12  \nER*         34.46 +- 0.63  39.01  37.28  \nER          33.25 +- 0.68  37.73  35.07  \nGEM*        32.18 +- 0.61  35.40  35.00  \nGEM         31.50 +- 0.66  34.84  33.82  \nLwF*        35.50 +- 0.71  39.62  38.42  \nLwF         35.23 +- 0.65  39.45  38.30  \nEWC*        33.72 +- 0.66  37.39  36.31  \nEWC         33.17 +- 0.68  36.39  35.21  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th>Phase</th>\n      <th>continual</th>\n      <th>sys</th>\n      <th>pro</th>\n      <th>Hn</th>\n      <th>non</th>\n      <th>noc</th>\n      <th>Hr</th>\n      <th>Ha</th>\n    </tr>\n    <tr>\n      <th>Method</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>MultiTask*</th>\n      <td>46.40</td>\n      <td>41.78 +- 0.67</td>\n      <td>35.95 +- 0.72</td>\n      <td>38.64</td>\n      <td>53.19 +- 0.84</td>\n      <td>35.23 +- 0.68</td>\n      <td>42.39</td>\n      <td>40.43</td>\n    </tr>\n    <tr>\n      <th>MultiTask</th>\n      <td>28.23</td>\n      <td>41.90 +- 0.63</td>\n      <td>36.58 +- 0.72</td>\n      <td>39.06</td>\n      <td>49.88 +- 0.82</td>\n      <td>35.69 +- 0.69</td>\n      <td>41.61</td>\n      <td>40.29</td>\n    </tr>\n    <tr>\n      <th>Finetune*</th>\n      <td>41.97</td>\n      <td>37.53 +- 0.66</td>\n      <td>33.49 +- 0.71</td>\n      <td>35.39</td>\n      <td>41.45 +- 0.77</td>\n      <td>34.94 +- 0.67</td>\n      <td>37.92</td>\n      <td>36.61</td>\n    </tr>\n    <tr>\n      <th>Finetune</th>\n      <td>13.93</td>\n      <td>37.26 +- 0.59</td>\n      <td>33.08 +- 0.71</td>\n      <td>35.05</td>\n      <td>41.76 +- 0.74</td>\n      <td>33.63 +- 0.67</td>\n      <td>37.25</td>\n      <td>36.12</td>\n    </tr>\n    <tr>\n      <th>ER*</th>\n      <td>41.10</td>\n      <td>38.23 +- 0.64</td>\n      <td>33.48 +- 0.69</td>\n      <td>35.70</td>\n      <td>44.95 +- 0.78</td>\n      <td>34.46 +- 0.63</td>\n      <td>39.01</td>\n      <td>37.28</td>\n    </tr>\n    <tr>\n      <th>ER</th>\n      <td>18.10</td>\n      <td>35.03 +- 0.63</td>\n      <td>30.76 +- 0.66</td>\n      <td>32.75</td>\n      <td>43.62 +- 0.78</td>\n      <td>33.25 +- 0.68</td>\n      <td>37.73</td>\n      <td>35.07</td>\n    </tr>\n    <tr>\n      <th>GEM*</th>\n      <td>26.70</td>\n      <td>36.36 +- 0.62</td>\n      <td>33.00 +- 0.71</td>\n      <td>34.60</td>\n      <td>39.34 +- 0.69</td>\n      <td>32.18 +- 0.61</td>\n      <td>35.40</td>\n      <td>35.00</td>\n    </tr>\n    <tr>\n      <th>GEM</th>\n      <td>13.43</td>\n      <td>34.29 +- 0.59</td>\n      <td>31.56 +- 0.67</td>\n      <td>32.87</td>\n      <td>38.96 +- 0.68</td>\n      <td>31.50 +- 0.66</td>\n      <td>34.84</td>\n      <td>33.82</td>\n    </tr>\n    <tr>\n      <th>LwF*</th>\n      <td>43.90</td>\n      <td>40.30 +- 0.64</td>\n      <td>34.70 +- 0.75</td>\n      <td>37.29</td>\n      <td>44.83 +- 0.79</td>\n      <td>35.50 +- 0.71</td>\n      <td>39.62</td>\n      <td>38.42</td>\n    </tr>\n    <tr>\n      <th>LwF</th>\n      <td>15.07</td>\n      <td>39.52 +- 0.61</td>\n      <td>35.18 +- 0.73</td>\n      <td>37.22</td>\n      <td>44.81 +- 0.80</td>\n      <td>35.23 +- 0.65</td>\n      <td>39.45</td>\n      <td>38.30</td>\n    </tr>\n    <tr>\n      <th>EWC*</th>\n      <td>40.07</td>\n      <td>37.65 +- 0.62</td>\n      <td>33.22 +- 0.69</td>\n      <td>35.30</td>\n      <td>41.95 +- 0.72</td>\n      <td>33.72 +- 0.66</td>\n      <td>37.39</td>\n      <td>36.31</td>\n    </tr>\n    <tr>\n      <th>EWC</th>\n      <td>14.67</td>\n      <td>36.36 +- 0.62</td>\n      <td>32.12 +- 0.66</td>\n      <td>34.11</td>\n      <td>40.30 +- 0.70</td>\n      <td>33.17 +- 0.68</td>\n      <td>36.39</td>\n      <td>35.21</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''For COBJ, no sub cal Harmonic mean (among sys, pro) and Harmonic mean (between non, noc)'''\n",
    "data = []\n",
    "for exp in exps_dis:\n",
    "    mean_dict = {}\n",
    "    for mode in modes:\n",
    "        acc_list = collected_data[(collected_data['Method'] == exp) & (collected_data['Phase'] == mode)]['Accuracy']\n",
    "        mean = acc_list.mean()\n",
    "        mean_dict[mode] = mean\n",
    "        ci95 = 1.96 * (acc_list.std()/np.sqrt(len(acc_list)))\n",
    "        acc_str = f'{mean*100:.2f} +- {ci95*100:.2f}' if mode != 'continual' else f'{mean*100:.2f}'\n",
    "        data.append(pd.DataFrame({'Method': exp, 'Phase': mode, 'mean': mean, 'ci95': ci95, 'str': acc_str}, index=[0]))\n",
    "\n",
    "    # 3x1*x2*x3/(x1*x2+x1*x3+x2*x3)\n",
    "    # hm_nov = 3 * mean_dict['sys'] * mean_dict['pro'] * mean_dict['sub'] / (mean_dict['sys'] * mean_dict['pro'] + mean_dict['sys'] * mean_dict['sub'] + mean_dict['pro'] * mean_dict['sub'])\n",
    "    hm_nov = 2 * mean_dict['sys'] * mean_dict['pro'] / (mean_dict['sys'] + mean_dict['pro'])\n",
    "    data.append(pd.DataFrame({'Method': exp, 'Phase': f'Hn', 'str': f'{hm_nov*100:.2f}'}, index=[0]))\n",
    "\n",
    "    # 2x1*x2/(x1+x2)\n",
    "    hm_ref = 2 * mean_dict['non'] * mean_dict['noc'] / (mean_dict['non'] + mean_dict['noc'])\n",
    "    data.append(pd.DataFrame({'Method': exp, 'Phase': f'Hr', 'str': f'{hm_ref*100:.2f}'}, index=[0]))\n",
    "\n",
    "    # hm_all = 5 / (1/mean_dict['sys'] + 1/mean_dict['pro'] + 1/mean_dict['sub'] + 1/mean_dict['non'] + 1/mean_dict['noc'])\n",
    "    hm_all = 4 / (1/mean_dict['sys'] + 1/mean_dict['pro'] + 1/mean_dict['non'] + 1/mean_dict['noc'])\n",
    "    data.append(pd.DataFrame({'Method': exp, 'Phase': f'Ha', 'str': f'{hm_all*100:.2f}'}, index=[0]))\n",
    "\n",
    "data = pd.concat(data, ignore_index=True)\n",
    "# print(data)\n",
    "\n",
    "# to latex\n",
    "\n",
    "data = data.pivot(index='Method', values='str', columns='Phase')\n",
    "# data = data[['continual', 'sys', 'pro', 'sub', 'Hn', 'non',  'noc', 'Hr', 'Ha']]\n",
    "data = data[['continual', 'sys', 'pro', 'Hn', 'non',  'noc', 'Hr', 'Ha']]\n",
    "data = data.reindex(exps_dis)\n",
    "print(data.style.to_latex().replace('+-', '$\\\\pm$'))\n",
    "data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hm_nov: 17.40, hm_ref: 15.50, hm_all: 16.59\n"
     ]
    }
   ],
   "source": [
    "# cal Harmonic mean\n",
    "# for RPSnet\n",
    "# mean_dict = {\n",
    "#     'sys': 59.80,\n",
    "#     'pro': 60.26,\n",
    "#     'sub': 59.75,\n",
    "#     'non': 64.22,\n",
    "#     'noc': 45.09,\n",
    "# }\n",
    "mean_dict = {\n",
    "    'sys': 15.03,\n",
    "    'pro': 22.41,\n",
    "    'sub': 16.32,\n",
    "    'non': 15.51,\n",
    "    'noc': 15.49,\n",
    "}\n",
    "# for MNTDP\n",
    "# mean_dict = {\n",
    "#     'sys': 43.26,\n",
    "#     'pro': 44.46,\n",
    "#     'sub': 43.31,\n",
    "#     'non': 48.27,\n",
    "#     'noc': 30.42,\n",
    "# }\n",
    "# for LMC\n",
    "# mean_dict = {\n",
    "#     'sys': 43.96,\n",
    "#     'pro': 49.38,\n",
    "#     'sub': 44.20,\n",
    "#     'non': 48.18,\n",
    "#     'noc': 28.96,\n",
    "# }\n",
    "\n",
    "# 3x1*x2*x3/(x1*x2+x1*x3+x2*x3)\n",
    "hm_nov = 3 * mean_dict['sys'] * mean_dict['pro'] * mean_dict['sub'] / (mean_dict['sys'] * mean_dict['pro'] + mean_dict['sys'] * mean_dict['sub'] + mean_dict['pro'] * mean_dict['sub'])\n",
    "\n",
    "# 2x1*x2/(x1+x2)\n",
    "hm_ref = 2 * mean_dict['non'] * mean_dict['noc'] / (mean_dict['non'] + mean_dict['noc'])\n",
    "\n",
    "hm_all = 5 / (1/mean_dict['sys'] + 1/mean_dict['pro'] + 1/mean_dict['sub'] + 1/mean_dict['non'] + 1/mean_dict['noc'])\n",
    "\n",
    "print(f'hm_nov: {hm_nov:.2f}, hm_ref: {hm_ref:.2f}, hm_all: {hm_all:.2f}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 1500x300 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# draw Harmonic mean understanding\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x = np.arange(10)+ 1\n",
    "y1 = np.arange(10) + 1\n",
    "y2 = 10 - np.arange(10)\n",
    "H = 2/(1/y1+1/y2)\n",
    "\n",
    "collected_data = []\n",
    "collected_data.append(pd.DataFrame({\n",
    "    'Case': x,\n",
    "    'Accuracy': y1,\n",
    "    'Task': 1,\n",
    "}))\n",
    "collected_data.append(pd.DataFrame({\n",
    "    'Case': x,\n",
    "    'Accuracy': y2,\n",
    "    'Task': 2,\n",
    "}))\n",
    "collected_data.append(pd.DataFrame({\n",
    "    'Case': x,\n",
    "    'Accuracy': H,\n",
    "    'Task': 'H',\n",
    "}))\n",
    "\n",
    "collected_data = pd.concat(collected_data, ignore_index=True)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(15, 3))\n",
    "# sns.set_palette('Paired')\n",
    "matplotlib.rcParams.update({'font.size': 18})\n",
    "\n",
    "ax = sns.lineplot(x=\"Case\", y=\"Accuracy\", data=collected_data, hue='Task')\n",
    "# plt.legend(loc = 'upper left', bbox_to_anchor = (1, 1))\n",
    "# ax = px.line(collected_data, x=\"Train size\", y=\"Accuracy\", hover_data=['Accuracy', 'Method'], color='Method')\n",
    "# ax.show()\n",
    "\n",
    "# plt.ylim([5, 6.5])\n",
    "\n",
    "fig = ax.get_figure()\n",
    "fig.savefig(os.path.join('D:', 'Downloads', 'fig.png'), dpi = 400, bbox_inches='tight')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# SCIFAR results"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HT-MT-naive-tsk_False-lr0_001-seed1\n",
      "continual\n",
      "HT-MT-naive-tsk_False-lr0_001-seed2\n",
      "continual\n",
      "HT-MT-naive-tsk_False-lr0_001-seed3\n",
      "continual\n",
      "HT-MT-naive-tsk_False-lr0_001-seed4\n",
      "continual\n",
      "HT-MT-naive-tsk_False-lr0_001-seed5\n",
      "continual\n",
      "HT-MT-naive-tsk_False-lr0_001-seed6\n",
      "continual\n",
      "HT-MT-naive-tsk_False-lr0_001-seed7\n",
      "continual\n",
      "HT-MT-naive-tsk_False-lr0_001-seed8\n",
      "continual\n",
      "HT-MT-naive-tsk_True-lr0_001-seed1\n",
      "continual\n",
      "HT-MT-naive-tsk_True-lr0_001-seed2\n",
      "continual\n",
      "HT-MT-naive-tsk_True-lr0_001-seed3\n",
      "continual\n",
      "HT-MT-naive-tsk_True-lr0_001-seed4\n",
      "continual\n",
      "HT-MT-naive-tsk_True-lr0_001-seed5\n",
      "continual\n",
      "HT-MT-naive-tsk_True-lr0_001-seed6\n",
      "continual\n",
      "HT-MT-naive-tsk_True-lr0_001-seed7\n",
      "continual\n",
      "HT-MT-naive-tsk_True-lr0_001-seed8\n",
      "continual\n",
      "HT-naive-tsk_False-lr0_003-seed1\n",
      "continual\n",
      "HT-naive-tsk_False-lr0_003-seed2\n",
      "continual\n",
      "HT-naive-tsk_False-lr0_003-seed3\n",
      "continual\n",
      "HT-naive-tsk_False-lr0_003-seed4\n",
      "continual\n",
      "HT-naive-tsk_False-lr0_003-seed5\n",
      "continual\n",
      "HT-naive-tsk_False-lr0_003-seed6\n",
      "continual\n",
      "HT-naive-tsk_False-lr0_003-seed7\n",
      "continual\n",
      "HT-naive-tsk_False-lr0_003-seed8\n",
      "continual\n",
      "HT-naive-tsk_True-lr0_01-seed1\n",
      "continual\n",
      "HT-naive-tsk_True-lr0_01-seed2\n",
      "continual\n",
      "HT-naive-tsk_True-lr0_01-seed3\n",
      "continual\n",
      "HT-naive-tsk_True-lr0_01-seed4\n",
      "continual\n",
      "HT-naive-tsk_True-lr0_01-seed5\n",
      "continual\n",
      "HT-naive-tsk_True-lr0_01-seed6\n",
      "continual\n",
      "HT-naive-tsk_True-lr0_01-seed7\n",
      "continual\n",
      "HT-naive-tsk_True-lr0_01-seed8\n",
      "continual\n",
      "HT-er-tsk_False-lr0_0007-seed1\n",
      "continual\n",
      "HT-er-tsk_False-lr0_0007-seed2\n",
      "continual\n",
      "HT-er-tsk_False-lr0_0007-seed3\n",
      "continual\n",
      "HT-er-tsk_False-lr0_0007-seed4\n",
      "continual\n",
      "HT-er-tsk_False-lr0_0007-seed5\n",
      "continual\n",
      "HT-er-tsk_False-lr0_0007-seed6\n",
      "continual\n",
      "HT-er-tsk_False-lr0_0007-seed7\n",
      "continual\n",
      "HT-er-tsk_False-lr0_0007-seed8\n",
      "continual\n",
      "HT-er-tsk_True-lr0_0007-seed1\n",
      "continual\n",
      "HT-er-tsk_True-lr0_0007-seed2\n",
      "continual\n",
      "HT-er-tsk_True-lr0_0007-seed3\n",
      "continual\n",
      "HT-er-tsk_True-lr0_0007-seed4\n",
      "continual\n",
      "HT-er-tsk_True-lr0_0007-seed5\n",
      "continual\n",
      "HT-er-tsk_True-lr0_0007-seed6\n",
      "continual\n",
      "HT-er-tsk_True-lr0_0007-seed7\n",
      "continual\n",
      "HT-er-tsk_True-lr0_0007-seed8\n",
      "continual\n",
      "HT-gem-tsk_False-lr0_001-p256-m0_1-seed1\n",
      "continual\n",
      "HT-gem-tsk_False-lr0_001-p256-m0_1-seed2\n",
      "continual\n",
      "HT-gem-tsk_False-lr0_001-p256-m0_1-seed3\n",
      "continual\n",
      "HT-gem-tsk_False-lr0_001-p256-m0_1-seed4\n",
      "continual\n",
      "HT-gem-tsk_False-lr0_001-p256-m0_1-seed5\n",
      "continual\n",
      "HT-gem-tsk_False-lr0_001-p256-m0_1-seed6\n",
      "continual\n",
      "HT-gem-tsk_False-lr0_001-p256-m0_1-seed7\n",
      "continual\n",
      "HT-gem-tsk_False-lr0_001-p256-m0_1-seed8\n",
      "continual\n",
      "HT-gem-tsk_True-lr0_001-p256-m0_7-seed1\n",
      "continual\n",
      "HT-gem-tsk_True-lr0_001-p256-m0_7-seed2\n",
      "continual\n",
      "HT-gem-tsk_True-lr0_001-p256-m0_7-seed3\n",
      "continual\n",
      "HT-gem-tsk_True-lr0_001-p256-m0_7-seed4\n",
      "continual\n",
      "HT-gem-tsk_True-lr0_001-p256-m0_7-seed5\n",
      "continual\n",
      "HT-gem-tsk_True-lr0_001-p256-m0_7-seed6\n",
      "continual\n",
      "HT-gem-tsk_True-lr0_001-p256-m0_7-seed7\n",
      "continual\n",
      "HT-gem-tsk_True-lr0_001-p256-m0_7-seed8\n",
      "continual\n",
      "HT-lwf-tsk_False-lr0_001-a1-t3_32-seed1\n",
      "continual\n",
      "HT-lwf-tsk_False-lr0_001-a1-t3_32-seed2\n",
      "continual\n",
      "HT-lwf-tsk_False-lr0_001-a1-t3_32-seed3\n",
      "continual\n",
      "HT-lwf-tsk_False-lr0_001-a1-t3_32-seed4\n",
      "continual\n",
      "HT-lwf-tsk_False-lr0_001-a1-t3_32-seed5\n",
      "continual\n",
      "HT-lwf-tsk_False-lr0_001-a1-t3_32-seed6\n",
      "continual\n",
      "HT-lwf-tsk_False-lr0_001-a1-t3_32-seed7\n",
      "continual\n",
      "HT-lwf-tsk_False-lr0_001-a1-t3_32-seed8\n",
      "continual\n",
      "HT-lwf-tsk_True-lr0_001-a1-t2-seed1\n",
      "continual\n",
      "HT-lwf-tsk_True-lr0_001-a1-t2-seed2\n",
      "continual\n",
      "HT-lwf-tsk_True-lr0_001-a1-t2-seed3\n",
      "continual\n",
      "HT-lwf-tsk_True-lr0_001-a1-t2-seed4\n",
      "continual\n",
      "HT-lwf-tsk_True-lr0_001-a1-t2-seed5\n",
      "continual\n",
      "HT-lwf-tsk_True-lr0_001-a1-t2-seed6\n",
      "continual\n",
      "HT-lwf-tsk_True-lr0_001-a1-t2-seed7\n",
      "continual\n",
      "HT-lwf-tsk_True-lr0_001-a1-t2-seed8\n",
      "continual\n",
      "HT-ewc-tsk_False-lr0_001-lambda0_22-seed1\n",
      "continual\n",
      "HT-ewc-tsk_False-lr0_001-lambda0_22-seed2\n",
      "continual\n",
      "HT-ewc-tsk_False-lr0_001-lambda0_22-seed3\n",
      "continual\n",
      "HT-ewc-tsk_False-lr0_001-lambda0_22-seed4\n",
      "continual\n",
      "HT-ewc-tsk_False-lr0_001-lambda0_22-seed5\n",
      "continual\n",
      "HT-ewc-tsk_False-lr0_001-lambda0_22-seed6\n",
      "continual\n",
      "HT-ewc-tsk_False-lr0_001-lambda0_22-seed7\n",
      "continual\n",
      "HT-ewc-tsk_False-lr0_001-lambda0_22-seed8\n",
      "continual\n",
      "HT-ewc-tsk_True-lr0_01-lambda1-seed1\n",
      "continual\n",
      "HT-ewc-tsk_True-lr0_01-lambda1-seed2\n",
      "continual\n",
      "HT-ewc-tsk_True-lr0_01-lambda1-seed3\n",
      "continual\n",
      "HT-ewc-tsk_True-lr0_01-lambda1-seed4\n",
      "continual\n",
      "HT-ewc-tsk_True-lr0_01-lambda1-seed5\n",
      "continual\n",
      "HT-ewc-tsk_True-lr0_01-lambda1-seed6\n",
      "continual\n",
      "HT-ewc-tsk_True-lr0_01-lambda1-seed7\n",
      "continual\n",
      "HT-ewc-tsk_True-lr0_01-lambda1-seed8\n",
      "continual\n"
     ]
    }
   ],
   "source": [
    "'''collect exps'''\n",
    "project_name = 'SCIFAR100'\n",
    "modes = ['continual']\n",
    "\n",
    "'''scifar100 resnet18'''\n",
    "# exps = [\n",
    "#     *[f'HT-naive-tsk_{return_task_id}-lr{learning_rate}'\n",
    "#       for return_task_id in [True, False] for learning_rate in ['1e-05', '0_0001', '0_001', '0_01']],\n",
    "#     *[f'HT-er-tsk_{return_task_id}-lr{learning_rate}'\n",
    "#       for return_task_id in [True, False] for learning_rate in ['1e-05', '0_0001', '0_001', '0_01']],\n",
    "#     *[f'HT-gem-tsk_{return_task_id}-lr{learning_rate}-p256-m0_5'\n",
    "#       for return_task_id in [True, False] for learning_rate in ['1e-05', '0_0001', '0_001', '0_01']],\n",
    "#     *[f'HT-lwf-tsk_{return_task_id}-lr{learning_rate}-a1-t2'\n",
    "#       for return_task_id in [True, False] for learning_rate in ['1e-05', '0_0001', '0_001', '0_01']],\n",
    "#     ]\n",
    "# exps_dis = [\n",
    "#     *[f\"naive {'Task' if return_task_id else 'Class'} lr{learning_rate}\"\n",
    "#       for return_task_id in [True, False]\n",
    "#       for learning_rate in ['1e-05', '1e-4', '1e-3', '1e-2']],\n",
    "#     *[f\"er {'Task' if return_task_id else 'Class'} lr{learning_rate}\"\n",
    "#       for return_task_id in [True, False]\n",
    "#       for learning_rate in ['1e-05', '1e-4', '1e-3', '1e-2']],\n",
    "#     *[f\"gem {'Task' if return_task_id else 'Class'} lr{learning_rate}\"\n",
    "#       for return_task_id in [True, False]\n",
    "#       for learning_rate in ['1e-05', '1e-4', '1e-3', '1e-2']],\n",
    "#     *[f\"lwf {'Task' if return_task_id else 'Class'} lr{learning_rate}\"\n",
    "#       for return_task_id in [True, False]\n",
    "#       for learning_rate in ['1e-05', '1e-4', '1e-3', '1e-2']],\n",
    "# ]\n",
    "# exps = [\n",
    "#     *[f'HT-naive-tsk_{return_task_id}-lr{learning_rate}'\n",
    "#       for return_task_id in [True] for learning_rate in ['0_025', '0_05', '0_075', '0_1']],\n",
    "#     *[f'HT-naive-tsk_{return_task_id}-lr{learning_rate}'\n",
    "#       for return_task_id in [False] for learning_rate in ['0_0003', '0_0007', '0_003', '0_007']],\n",
    "#     *[f'HT-er-tsk_{return_task_id}-lr{learning_rate}'\n",
    "#       for return_task_id in [True, False] for learning_rate in ['0_0003', '0_0007', '0_003', '0_007']],\n",
    "#     *[f'HT-gem-tsk_{return_task_id}-lr0_001-p256-m{m}'\n",
    "#       for return_task_id in [True, False] for m in ['0_1', '0_3', '0_7', '1_0']],\n",
    "#     *[f'HT-lwf-tsk_{return_task_id}-lr0_001-a{a}-t2'\n",
    "#       for return_task_id in [True, False] for a in ['0_01', '0_1', '10', '100']],\n",
    "#     ]\n",
    "# exps_dis = [\n",
    "#     *[f\"naive {'Task' if return_task_id else 'Class'} lr{learning_rate}\"\n",
    "#       for return_task_id in [True] for learning_rate in ['0.025', '0.05', '0.075', '0.1']],\n",
    "#     *[f\"naive {'Task' if return_task_id else 'Class'} lr{learning_rate}\"\n",
    "#       for return_task_id in [False] for learning_rate in ['3e-4', '7e-4', '3e-3', '7e-3']],\n",
    "#     *[f\"er {'Task' if return_task_id else 'Class'} lr{learning_rate}\"\n",
    "#       for return_task_id in [True, False] for learning_rate in ['3e-4', '7e-4', '3e-3', '7e-3']],\n",
    "#     *[f\"gem {'Task' if return_task_id else 'Class'} lr1e-3 m{m}\"\n",
    "#       for return_task_id in [True, False] for m in ['0.1', '0.3', '0.7', '1.0']],\n",
    "#     *[f\"lwf {'Task' if return_task_id else 'Class'} lr0_001 a{a}\"\n",
    "#       for return_task_id in [True, False] for a in ['0.01', '0.1', '10', '100']],\n",
    "# ]\n",
    "# exps = [\n",
    "#     *[f'HT-ewc-tsk_{return_task_id}-lr{learning_rate}-lambda1'\n",
    "#       for return_task_id in [True, False] for learning_rate in ['0_0001', '0_001', '0_01', '0_1']],\n",
    "#     ]\n",
    "# exps_dis = [\n",
    "#     *[f\"ewc {'Task' if return_task_id else 'Class'} lr{learning_rate}\"\n",
    "#       for return_task_id in [True, False] for learning_rate in ['0.0001', '0.001', '0.01', '0.1']],\n",
    "# ]\n",
    "# exps = [\n",
    "#     *[f'HT-ewc-tsk_{return_task_id}-lr0_01-lambda{ewc_lambda}'\n",
    "#       for return_task_id in [True] for ewc_lambda in ['0_01', '0_1', '10', '100']],\n",
    "#     *[f'HT-ewc-tsk_{return_task_id}-lr0_001-lambda{ewc_lambda}'\n",
    "#       for return_task_id in [False] for ewc_lambda in ['0_01', '0_1', '10', '100']],\n",
    "#     ]\n",
    "# exps_dis = [\n",
    "#     *[f\"ewc {'Task' if return_task_id else 'Class'} lr0_01 lambda {ewc_lambda}\"\n",
    "#       for return_task_id in [True] for ewc_lambda in ['0.01', '0.1', '10', '100']],\n",
    "#     *[f\"ewc {'Task' if return_task_id else 'Class'} lr0_001 lambda {ewc_lambda}\"\n",
    "#       for return_task_id in [False] for ewc_lambda in ['0.01', '0.1', '10', '100']],\n",
    "# ]\n",
    "# exps = [\n",
    "#     *[f'HT-MT-naive-tsk_{return_task_id}-lr{learning_rate}'\n",
    "#       for return_task_id in [True, False] for learning_rate in ['0_0001', '0_001', '0_01', '0_1']],\n",
    "#     ]\n",
    "# exps_dis = [\n",
    "#     *[f\"MT {'Task' if return_task_id else 'Class'} lr{learning_rate}\"\n",
    "#       for return_task_id in [True, False] for learning_rate in ['0.0001', '0.001', '0.01', '0.1']],\n",
    "# ]\n",
    "# exps = [\n",
    "#     *[f'HT-lwf-tsk_{return_task_id}-lr0_001-a1-t{t}'\n",
    "#       for return_task_id in [True, False] for t in ['0_1', '1', '10', '100']],\n",
    "#     ]\n",
    "# exps_dis = [\n",
    "#     *[f\"lwf {'Task' if return_task_id else 'Class'} lr0_001 a1 t{t}\"\n",
    "#       for return_task_id in [True, False] for t in ['0.1', '1', '10', '100']],\n",
    "# ]\n",
    "# exps = [\n",
    "#     *[f'HT-ewc-tsk_{return_task_id}-lr0_001-lambda{ewc_lambda}'\n",
    "#       for return_task_id in [False]\n",
    "#       for ewc_lambda in [\n",
    "#           (lambda x: str(x).replace('.', '_')) (x)\n",
    "#           for x in np.around(np.logspace(-1, 1, num=24), decimals=2).tolist()\n",
    "#       ]],\n",
    "#     ]\n",
    "# exps_dis = [\n",
    "#     *[f\"ewc {'Task' if return_task_id else 'Class'} lr0_001 lambda {ewc_lambda}\"\n",
    "#       for return_task_id in [False] for ewc_lambda in np.around(np.logspace(-1, 1, num=24), decimals=2).tolist()],\n",
    "# ]\n",
    "# exps = [\n",
    "#     *[f'HT-lwf-tsk_{return_task_id}-lr0_001-a1-t{t}'\n",
    "#       for return_task_id in [False]\n",
    "#       for t in [\n",
    "#           (lambda x: str(x).replace('.', '_')) (x)\n",
    "#           for x in np.around(np.logspace(0, 1, num=24), decimals=2).tolist()\n",
    "#       ]],\n",
    "#     ]\n",
    "# exps_dis = [\n",
    "#     *[f\"ewc {'Task' if return_task_id else 'Class'} lr0_001 a1 t{t}\"\n",
    "#       for return_task_id in [False] for t in np.around(np.logspace(0, 1, num=24), decimals=2).tolist()],\n",
    "# ]\n",
    "# exps = [\n",
    "#     *[f'HT-ewc-tsk_{return_task_id}-lr{lr}-lambda{t}'\n",
    "#       for return_task_id in [False]\n",
    "#       for t in ['0_22', '1']\n",
    "#       for lr in [\n",
    "#           (lambda x: str(x).replace('.', '_')) (x)\n",
    "#           for x in np.around(np.logspace(-4, -2, num=24), decimals=5).tolist()\n",
    "#       ]],\n",
    "#     ]\n",
    "# exps_dis = [\n",
    "#     *[f\"ewc {'Task' if return_task_id else 'Class'} lr{lr} lambda{t}\"\n",
    "#       for return_task_id in [False]\n",
    "#       for t in [0.22, 1]\n",
    "#       for lr in np.around(np.logspace(-4, -2, num=24), decimals=5).tolist()],\n",
    "# ]\n",
    "# exps = [\n",
    "#     *[f'HT-lwf-tsk_{return_task_id}-lr{lr}-a{a}-t{t}'\n",
    "#       for return_task_id in [False]\n",
    "#       for t in ['3_32']\n",
    "#       for a in [\n",
    "#           (lambda x: str(x).replace('.', '_')) (x)\n",
    "#           for x in np.around(np.logspace(-2, 1, num=8), decimals=3).tolist()\n",
    "#       ]\n",
    "#       for lr in [\n",
    "#           (lambda x: str(x).replace('.', '_')) (x)\n",
    "#           for x in np.around(np.logspace(-4, -2, num=8), decimals=5).tolist()\n",
    "#       ]],\n",
    "#     ]\n",
    "# exps_dis = [\n",
    "#     *[f\"lwf {'Task' if return_task_id else 'Class'} lr{lr} a{a} t{t}\"\n",
    "#       for return_task_id in [False]\n",
    "#       for t in [3.32]\n",
    "#       for a in np.around(np.logspace(-2, 1, num=8), decimals=3).tolist()\n",
    "#       for lr in np.around(np.logspace(-4, -2, num=8), decimals=5).tolist()],\n",
    "# ]\n",
    "\n",
    "# show overall results\n",
    "exps = [\n",
    "    *[f'HT-MT-naive-tsk_{return_task_id}-lr0_001-seed{seed}'\n",
    "      for return_task_id in [False, True] for seed in [1, 2, 3, 4, 5, 6, 7, 8]],\n",
    "    *[f'HT-naive-tsk_{return_task_id}-lr{\"0_01\" if return_task_id else \"0_003\"}-seed{seed}'\n",
    "      for return_task_id in [False, True] for seed in [1, 2, 3, 4, 5, 6, 7, 8]],\n",
    "    *[f'HT-er-tsk_{return_task_id}-lr0_0007-seed{seed}'\n",
    "      for return_task_id in [False, True] for seed in [1, 2, 3, 4, 5, 6, 7, 8]],\n",
    "    *[f'HT-gem-tsk_{return_task_id}-lr0_001-p256-m{\"0_7\" if return_task_id else \"0_1\"}-seed{seed}'\n",
    "      for return_task_id in [False, True] for seed in [1, 2, 3, 4, 5, 6, 7, 8]],\n",
    "    *[f'HT-lwf-tsk_{return_task_id}-lr0_001-a1-t{\"2\" if return_task_id else \"3_32\"}-seed{seed}'\n",
    "      for return_task_id in [False, True] for seed in [1, 2, 3, 4, 5, 6, 7, 8]],\n",
    "    *[f'HT-ewc-tsk_{return_task_id}-{\"lr0_01-lambda1\" if return_task_id else \"lr0_001-lambda0_22\"}-seed{seed}'\n",
    "      for return_task_id in [False, True] for seed in [1, 2, 3, 4, 5, 6, 7, 8]],\n",
    "    ]\n",
    "exps_dis = [\n",
    "    *[f\"MultiTask{'*' if return_task_id else ''}\"\n",
    "      for return_task_id in [False, True] for seed in [1, 2, 3, 4, 5, 6, 7, 8]],\n",
    "    *[f\"Finetune{'*' if return_task_id else ''}\"\n",
    "      for return_task_id in [False, True] for seed in [1, 2, 3, 4, 5, 6, 7, 8]],\n",
    "    *[f\"ER{'*' if return_task_id else ''}\"\n",
    "      for return_task_id in [False, True] for seed in [1, 2, 3, 4, 5, 6, 7, 8]],\n",
    "    *[f\"GEM{'*' if return_task_id else ''}\"\n",
    "      for return_task_id in [False, True] for seed in [1, 2, 3, 4, 5, 6, 7, 8]],\n",
    "    *[f\"LwF{'*' if return_task_id else ''}\"\n",
    "      for return_task_id in [False, True] for seed in [1, 2, 3, 4, 5, 6, 7, 8]],\n",
    "    *[f\"EWC{'*' if return_task_id else ''}\"\n",
    "      for return_task_id in [False, True] for seed in [1, 2, 3, 4, 5, 6, 7, 8]],\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "collected_data = []\n",
    "for exp_idx, exp_name in enumerate(exps):\n",
    "    print(exp_name)\n",
    "    for mode in modes:\n",
    "        print(mode)\n",
    "        accs = get_accs(exp_name, mode, project_name=project_name, index='-9' if 'MT' not in exp_name else '')\n",
    "        # print(accs)\n",
    "        collected_data.append(pd.DataFrame({'Method': exps_dis[exp_idx], 'Phase': mode, 'Accuracy': accs}))\n",
    "\n",
    "collected_data = pd.concat(collected_data, ignore_index=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{ll}\n",
      "{Phase} & {continual} \\\\\n",
      "{Method} & {} \\\\\n",
      "ER & 12.86 $\\pm$ 0.13 \\\\\n",
      "ER* & 61.23 $\\pm$ 0.54 \\\\\n",
      "EWC & 7.41 $\\pm$ 0.35 \\\\\n",
      "EWC* & 46.12 $\\pm$ 1.23 \\\\\n",
      "Finetune & 7.37 $\\pm$ 0.27 \\\\\n",
      "Finetune* & 46.77 $\\pm$ 1.29 \\\\\n",
      "GEM & 13.43 $\\pm$ 0.21 \\\\\n",
      "GEM* & 60.14 $\\pm$ 0.41 \\\\\n",
      "LwF & 8.07 $\\pm$ 0.26 \\\\\n",
      "LwF* & 70.41 $\\pm$ 0.74 \\\\\n",
      "MultiTask & 53.42 $\\pm$ 0.30 \\\\\n",
      "MultiTask* & 79.87 $\\pm$ 0.25 \\\\\n",
      "\\end{tabular}\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": "Phase           continual\nMethod                   \nER          12.86 +- 0.13\nER*         61.23 +- 0.54\nEWC          7.41 +- 0.35\nEWC*        46.12 +- 1.23\nFinetune     7.37 +- 0.27\nFinetune*   46.77 +- 1.29\nGEM         13.43 +- 0.21\nGEM*        60.14 +- 0.41\nLwF          8.07 +- 0.26\nLwF*        70.41 +- 0.74\nMultiTask   53.42 +- 0.30\nMultiTask*  79.87 +- 0.25",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th>Phase</th>\n      <th>continual</th>\n    </tr>\n    <tr>\n      <th>Method</th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>ER</th>\n      <td>12.86 +- 0.13</td>\n    </tr>\n    <tr>\n      <th>ER*</th>\n      <td>61.23 +- 0.54</td>\n    </tr>\n    <tr>\n      <th>EWC</th>\n      <td>7.41 +- 0.35</td>\n    </tr>\n    <tr>\n      <th>EWC*</th>\n      <td>46.12 +- 1.23</td>\n    </tr>\n    <tr>\n      <th>Finetune</th>\n      <td>7.37 +- 0.27</td>\n    </tr>\n    <tr>\n      <th>Finetune*</th>\n      <td>46.77 +- 1.29</td>\n    </tr>\n    <tr>\n      <th>GEM</th>\n      <td>13.43 +- 0.21</td>\n    </tr>\n    <tr>\n      <th>GEM*</th>\n      <td>60.14 +- 0.41</td>\n    </tr>\n    <tr>\n      <th>LwF</th>\n      <td>8.07 +- 0.26</td>\n    </tr>\n    <tr>\n      <th>LwF*</th>\n      <td>70.41 +- 0.74</td>\n    </tr>\n    <tr>\n      <th>MultiTask</th>\n      <td>53.42 +- 0.30</td>\n    </tr>\n    <tr>\n      <th>MultiTask*</th>\n      <td>79.87 +- 0.25</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "'''cal mean and ci95, then formulate table str-cell and to latex.'''\n",
    "# exp = 'Finetune'\n",
    "# mode = 'continual'\n",
    "# acc_list = collected_data[(collected_data['Method'] == exp) & (collected_data['Phase'] == mode)]['Accuracy']\n",
    "data = []\n",
    "for exp in sorted(set(exps_dis)):\n",
    "    for mode in modes:\n",
    "        acc_list = collected_data[(collected_data['Method'] == exp) & (collected_data['Phase'] == mode)]['Accuracy']\n",
    "        mean = acc_list.mean()\n",
    "        ci95 = 1.96 * (acc_list.std()/np.sqrt(len(acc_list)))\n",
    "        acc_str = f'{mean*100:.2f} +- {ci95*100:.2f}'\n",
    "        data.append(pd.DataFrame({'Method': exp, 'Phase': mode, 'mean': mean, 'ci95': ci95, 'str': acc_str}, index=[0]))\n",
    "data = pd.concat(data, ignore_index=True)\n",
    "# print(data)\n",
    "\n",
    "# to latex\n",
    "\n",
    "data = data.pivot(index='Method', columns='Phase', values='str')\n",
    "data = data[modes]\n",
    "data = data.reindex(sorted(set(exps_dis)))\n",
    "print(data.style.to_latex().replace('+-', '$\\\\pm$'))\n",
    "data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# CPIN"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55.37 &  & 27.60 & 36.83\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Finetune & 9.22 & 48.97 $\\pm$ 0.88 & 42.06 $\\pm$ 0.91 & 52.33 $\\pm$ 0.96 & 15.56 $\\pm$ 0.43 \\\\\n",
    "# ER & 28.82 & 59.16 $\\pm$ 0.89 & 45.94 $\\pm$ 1.05 & 60.11 $\\pm$ 1.01 & 14.55 $\\pm$ 0.42 \\\\\n",
    "# GEM & 9.19 & 51.13 $\\pm$ 1.08 & 46.14 $\\pm$ 1.04 & 55.16 $\\pm$ 1.05 & 13.23 $\\pm$ 0.38 \\\\\n",
    "# LwF & 9.21 & \\textbf{64.45 $\\pm$ 0.65} & \\textbf{57.87 $\\pm$ 0.67} & \\textbf{65.73 $\\pm$ 0.74} & 17.97 $\\pm$ 0.51 \\\\\n",
    "# EWC & 9.37 & 53.92 $\\pm$ 0.75 & 50.36 $\\pm$ 0.73 & 56.64 $\\pm$ 0.83 & 17.11 $\\pm$ 0.46 \\\\\n",
    "# RPSnet & \\textbf{45.88} & 55.33 $\\pm$ 0.81 & 48.21 $\\pm$ 0.63 & 58.21 $\\pm$ 0.95 & \\textbf{26.32 $\\pm$ 0.33} \\\\\n",
    "# \\midrule\n",
    "# Finetune* & 75.37 & 61.57 $\\pm$ 0.68 & \\textbf{58.47 $\\pm$ 0.66} & \\textbf{64.30 $\\pm$ 0.77} & \\textbf{18.32 $\\pm$ 0.45} \\\\\n",
    "# ER* & \\textbf{86.46} & \\textbf{62.22 $\\pm$ 0.85} & 50.20 $\\pm$ 0.94 & 63.23 $\\pm$ 0.96 & 15.74 $\\pm$ 0.42 \\\\\n",
    "# GEM* & 19.68 & 57.26 $\\pm$ 0.71 & 49.65 $\\pm$ 0.71 & 58.33 $\\pm$ 0.75 & 16.30 $\\pm$ 0.43 \\\\\n",
    "# LwF* & 73.24 & 59.99 $\\pm$ 0.72 & 51.05 $\\pm$ 0.75 & 60.44 $\\pm$ 0.84 & 16.21 $\\pm$ 0.42 \\\\\n",
    "# EWC* & 76.15 & 57.83 $\\pm$ 0.65 & 53.11 $\\pm$ 0.69 & 59.00 $\\pm$ 0.77 & 18.01 $\\pm$ 0.53 \\\\\n",
    "\n",
    "'''calculate Hn'''\n",
    "\n",
    "sys = 57.83\n",
    "pro = 53.11\n",
    "non = 59.00\n",
    "noc = 18.01\n",
    "Hn = 2/(1/sys + 1/pro)\n",
    "Hr = 2/(1/non + 1/noc)\n",
    "Ha = 4/(1/sys + 1/pro + 1/non + 1/noc)\n",
    "print (f'{Hn:.2f} &  & {Hr:.2f} & {Ha:.2f}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}